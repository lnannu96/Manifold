{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hypersurface_tf import *\n",
    "from generate_h import *\n",
    "from complexNN import *\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "z0, z1, z2, z3, z4 = sp.symbols('z0, z1, z2, z3, z4')\n",
    "Z = [z0,z1,z2,z3,z4]\n",
    "f = z0**5 + z1**5 + z2**5 + z3**5 + z4**5 + 0.5*z0*z1*z2*z3*z4\n",
    "np.random.seed(123)\n",
    "HS = Hypersurface(Z, f, 10000)\n",
    "np.random.seed(124)\n",
    "HS_test = Hypersurface(Z, f, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f705f18f598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f705f0f10d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7040648a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f704069ed90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7040694598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f70405b6b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f70405c9d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7040694bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7040694c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7040694b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f70406947b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f70404fb2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7040514378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7040499268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7040514158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f70405141e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f70404ba730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f70404baae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f70403fdbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f70403b16a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f70403b09d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7040371f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f70403ad8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f70403629d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7040362f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f70402f6268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f704030da60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f70402c98c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f70402c9510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7040286b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7040286840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7040261158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7040261950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7040214598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f70402617b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f704018c158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "train_set = generate_dataset(HS)\n",
    "test_set = generate_dataset(HS_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = train_set.shuffle(500000).batch(1000)\n",
    "test_set = test_set.shuffle(50000).batch(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KahlerPotential(tf.keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(KahlerPotential, self).__init__()\n",
    "        self.layer_1 = ComplexDense(5, 15, activation=tf.square, trainable=False)\n",
    "        self.layer_trans = LinearTrans(15, 15)\n",
    "        self.layer_2 = ComplexDense(15, 70, activation=tf.square, trainable=True)\n",
    "\n",
    "        #self.layer_4 = ComplexDense(50, 10, activation=tf.square)\n",
    "        #self.layer_3 = ComplexDense(10, 15, activation=tf.square)\n",
    "        self.g = ComplexG(70)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.layer_1(inputs)\n",
    "        x = self.layer_trans(x)\n",
    "        x = self.layer_2(x)\n",
    "\n",
    "        #x = self.layer_4(x)\n",
    "        x = self.g(x)\n",
    "        x = tf.linalg.diag_part(tf.matmul(x, x, adjoint_b=True))\n",
    "        x = tf.math.log(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KahlerPotential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def volume_form(x, Omega_Omegabar, mass, restriction):\n",
    "\n",
    "    kahler_metric = complex_hessian(tf.math.real(model(x)), x)\n",
    "    volume_form = tf.linalg.det(tf.matmul(restriction, tf.matmul(kahler_metric, restriction, adjoint_b=True)))\n",
    "    weights = mass / tf.reduce_sum(mass)\n",
    "    factor = tf.reduce_sum(weights * volume_form / Omega_Omegabar)\n",
    "    #factor = tf.constant(35.1774, dtype=tf.complex64)\n",
    "    return volume_form / factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009374412298202515\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.009544750452041626\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009487475752830505\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009657870531082153\n",
      "step 0: loss = 0.0093\n",
      "test_loss: 0.009367501139640808\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009568567872047424\n",
      "step 0: loss = 0.0095\n",
      "test_loss: 0.009541394710540772\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.009673133492469788\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.009566249251365662\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.00975422739982605\n",
      "step 0: loss = 0.0097\n",
      "test_loss: 0.009610427021980285\n",
      "step 0: loss = 0.0096\n",
      "test_loss: 0.009673517346382141\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.009682464599609374\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009643289446830749\n",
      "step 0: loss = 0.0094\n",
      "test_loss: 0.009698024392127991\n",
      "step 0: loss = 0.0096\n",
      "test_loss: 0.009433353543281555\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009540339708328247\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009621804356575012\n",
      "step 0: loss = 0.0094\n",
      "test_loss: 0.009539515376091004\n",
      "step 0: loss = 0.0093\n",
      "test_loss: 0.009733592271804809\n",
      "step 0: loss = 0.0093\n",
      "test_loss: 0.009666740894317627\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.00947123885154724\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009482781291007996\n",
      "step 0: loss = 0.0095\n",
      "test_loss: 0.009777600169181824\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.009400673508644104\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.009525747895240784\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009612540006637573\n",
      "step 0: loss = 0.0093\n",
      "test_loss: 0.009643783569335937\n",
      "step 0: loss = 0.0095\n",
      "test_loss: 0.009837620258331299\n",
      "step 0: loss = 0.0096\n",
      "test_loss: 0.009665629863739013\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009525433778762818\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.00965460479259491\n",
      "step 0: loss = 0.0095\n",
      "test_loss: 0.00980452001094818\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009558499455451966\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.009545866250991821\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.009723874926567077\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009593272805213928\n",
      "step 0: loss = 0.0093\n",
      "test_loss: 0.009773521423339844\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.009308428764343261\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.00977781593799591\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.00944861114025116\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009644501209259033\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009702666401863099\n",
      "step 0: loss = 0.0093\n",
      "test_loss: 0.009476476907730102\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009650976657867431\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009640621542930604\n",
      "step 0: loss = 0.0098\n",
      "test_loss: 0.009805934429168701\n",
      "step 0: loss = 0.0095\n",
      "test_loss: 0.009687253832817077\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.009657372832298278\n",
      "step 0: loss = 0.0093\n",
      "test_loss: 0.009671204090118408\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009545729756355285\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009530832767486572\n",
      "step 0: loss = 0.0093\n",
      "test_loss: 0.009641005992889404\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009463253021240235\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.00962513566017151\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009618878364562988\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009377613067626953\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.009599846601486207\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.00960943877696991\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009925640821456909\n",
      "step 0: loss = 0.0095\n",
      "test_loss: 0.009508671760559083\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009586563110351562\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009485653042793274\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009509096145629883\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.00967374324798584\n",
      "step 0: loss = 0.0093\n",
      "test_loss: 0.009536173939704896\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009814938306808471\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.00959575891494751\n",
      "step 0: loss = 0.0095\n",
      "test_loss: 0.00958859920501709\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.009528728723526001\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009590069055557251\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009578661918640137\n",
      "step 0: loss = 0.0093\n",
      "test_loss: 0.009671298265457153\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.009582785367965698\n",
      "step 0: loss = 0.0093\n",
      "test_loss: 0.009497556686401367\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009497832655906677\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009482273459434509\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.009568341374397278\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009407598972320557\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.0096674245595932\n",
      "step 0: loss = 0.0093\n",
      "test_loss: 0.009706905484199524\n",
      "step 0: loss = 0.0093\n",
      "test_loss: 0.009496493339538574\n",
      "step 0: loss = 0.0093\n",
      "test_loss: 0.009821017384529113\n",
      "step 0: loss = 0.0095\n",
      "test_loss: 0.00972649097442627\n",
      "step 0: loss = 0.0094\n",
      "test_loss: 0.009700350761413574\n",
      "step 0: loss = 0.0094\n",
      "test_loss: 0.00937299370765686\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009442031383514404\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.00939692258834839\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009420816898345947\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009324867725372315\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.00998642086982727\n",
      "step 0: loss = 0.0098\n",
      "test_loss: 0.009607951641082763\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.009538614749908447\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.009496366381645202\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.00963090717792511\n",
      "step 0: loss = 0.0098\n",
      "test_loss: 0.009353778958320617\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009619526267051696\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.009814235568046569\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009548832774162292\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009497168660163879\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.009415539503097535\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009672127366065979\n",
      "step 0: loss = 0.0093\n",
      "test_loss: 0.009661227464675903\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009600810408592224\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009673407077789306\n",
      "step 0: loss = 0.0096\n",
      "test_loss: 0.009614588022232055\n",
      "step 0: loss = 0.0094\n",
      "test_loss: 0.009565683007240295\n",
      "step 0: loss = 0.0093\n",
      "test_loss: 0.009477928280830383\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009835480451583863\n",
      "step 0: loss = 0.0093\n",
      "test_loss: 0.009312955141067504\n",
      "step 0: loss = 0.0084\n",
      "test_loss: 0.009614882469177246\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009446266889572143\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009576000571250916\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009353213906288148\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009589442610740661\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009533117413520814\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.009627979993820191\n",
      "step 0: loss = 0.0094\n",
      "test_loss: 0.009713532328605652\n",
      "step 0: loss = 0.0095\n",
      "test_loss: 0.009484680891036988\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009570757746696473\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009722415804862976\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009425505399703979\n",
      "step 0: loss = 0.0093\n",
      "test_loss: 0.009543529748916625\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.009517096281051636\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009418272376060487\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.00950279951095581\n",
      "step 0: loss = 0.0093\n",
      "test_loss: 0.009340410828590393\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009696712493896484\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.009675425291061402\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.009407309889793396\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009285336136817932\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009525370001792908\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009657190442085265\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.00961550533771515\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.00960696280002594\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009565529823303222\n",
      "step 0: loss = 0.0094\n",
      "test_loss: 0.009723724126815795\n",
      "step 0: loss = 0.0094\n",
      "test_loss: 0.009482215642929077\n",
      "step 0: loss = 0.0095\n",
      "test_loss: 0.009639701247215271\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.009687217473983765\n",
      "step 0: loss = 0.0094\n",
      "test_loss: 0.009385330080986022\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009577329754829406\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009913069605827331\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009464172720909119\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.00948991060256958\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.009360345602035523\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.009613905549049378\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009450990557670593\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009405237436294556\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.00973504364490509\n",
      "step 0: loss = 0.0097\n",
      "test_loss: 0.009656909108161926\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009622237682342528\n",
      "step 0: loss = 0.0096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 0.009483100175857543\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.00948877513408661\n",
      "step 0: loss = 0.0095\n",
      "test_loss: 0.00948485791683197\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009407698512077331\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009586189389228821\n",
      "step 0: loss = 0.0093\n",
      "test_loss: 0.009729294776916505\n",
      "step 0: loss = 0.0098\n",
      "test_loss: 0.00948222041130066\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009574536681175232\n",
      "step 0: loss = 0.0093\n",
      "test_loss: 0.009628851413726807\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009490644335746765\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009498852491378783\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009553110599517823\n",
      "step 0: loss = 0.0093\n",
      "test_loss: 0.009540852904319764\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009486294984817505\n",
      "step 0: loss = 0.0094\n",
      "test_loss: 0.009510684013366699\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009452998638153076\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009489944577217102\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009571993350982666\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.00945563018321991\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009397192001342774\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009343591928482055\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.00953326404094696\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.009486550688743592\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009387578964233399\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.009526236653327943\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009539129734039307\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.00957348346710205\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009508866071701049\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009714250564575195\n",
      "step 0: loss = 0.0095\n",
      "test_loss: 0.009529041647911072\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.00939694344997406\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.009450231194496155\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009628827571868897\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.00947571635246277\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009675712585449218\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009574288725852967\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009588924646377563\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009448615908622742\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009661938548088073\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009715452194213867\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009625887274742126\n",
      "step 0: loss = 0.0095\n",
      "test_loss: 0.009516879916191101\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009436164498329163\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009265394806861877\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009623458981513977\n",
      "step 0: loss = 0.0094\n",
      "test_loss: 0.009559149742126466\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009654874205589295\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009511238932609557\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.00939444601535797\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009337287545204163\n",
      "step 0: loss = 0.0093\n",
      "test_loss: 0.00968062162399292\n",
      "step 0: loss = 0.0096\n",
      "test_loss: 0.009501214623451232\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009406629800796509\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009639970660209655\n",
      "step 0: loss = 0.0095\n",
      "test_loss: 0.00947894811630249\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.009318510293960571\n",
      "step 0: loss = 0.0093\n",
      "test_loss: 0.009440851807594299\n",
      "step 0: loss = 0.0093\n",
      "test_loss: 0.009548621773719788\n",
      "step 0: loss = 0.0093\n",
      "test_loss: 0.009372820854187012\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009482935070991516\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009677956104278565\n",
      "step 0: loss = 0.0099\n",
      "test_loss: 0.00947923719882965\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.00951756775379181\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.00964756965637207\n",
      "step 0: loss = 0.0094\n",
      "test_loss: 0.009397205114364624\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009571256637573243\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009446879625320434\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009365347027778626\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.00959466516971588\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.0093748277425766\n",
      "step 0: loss = 0.0093\n",
      "test_loss: 0.009544678330421448\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.00953218162059784\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009428978562355042\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.009543062448501586\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.009486533403396606\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009520691633224488\n",
      "step 0: loss = 0.0094\n",
      "test_loss: 0.009570995569229126\n",
      "step 0: loss = 0.0096\n",
      "test_loss: 0.009635180830955506\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.009407107830047607\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.009678437113761902\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009344674944877625\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009766508936882018\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.009444355368614196\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009418721795082092\n",
      "step 0: loss = 0.0093\n",
      "test_loss: 0.009542658925056458\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009583515524864196\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009637519121170043\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009653493762016296\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.00966687023639679\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009688940644264222\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.009484307765960693\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009578867554664612\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009471354484558105\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.009541574716567993\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.009524757862091065\n",
      "step 0: loss = 0.0093\n",
      "test_loss: 0.009512646198272705\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009309337735176086\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.00949911892414093\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009434490799903869\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009617648124694823\n",
      "step 0: loss = 0.0094\n",
      "test_loss: 0.00943638801574707\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009247105717658997\n",
      "step 0: loss = 0.0082\n",
      "test_loss: 0.009412893652915954\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009415808916091919\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.00962222158908844\n",
      "step 0: loss = 0.0095\n",
      "test_loss: 0.009484615921974183\n",
      "step 0: loss = 0.0094\n",
      "test_loss: 0.009323121905326843\n",
      "step 0: loss = 0.0085\n",
      "test_loss: 0.009557484388351441\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009535638093948364\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009475950598716736\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009283860921859741\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009401259422302246\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009471203684806823\n",
      "step 0: loss = 0.0093\n",
      "test_loss: 0.009479830265045166\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009384964704513549\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009454777240753174\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.00927587330341339\n",
      "step 0: loss = 0.0094\n",
      "test_loss: 0.009480994939804078\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009625744819641114\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009432008862495423\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.009465349316596984\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.00938198745250702\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009517740607261658\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009539637565612793\n",
      "step 0: loss = 0.0095\n",
      "test_loss: 0.009434740543365478\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.00957456350326538\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.009630568027496338\n",
      "step 0: loss = 0.0094\n",
      "test_loss: 0.009498764276504517\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009573900699615478\n",
      "step 0: loss = 0.0085\n",
      "test_loss: 0.009476048946380615\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009521519541740417\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009431961178779601\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.009454230070114136\n",
      "step 0: loss = 0.0096\n",
      "test_loss: 0.009554571509361266\n",
      "step 0: loss = 0.0093\n",
      "test_loss: 0.009436684250831604\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009667837619781494\n",
      "step 0: loss = 0.0093\n",
      "test_loss: 0.009371072053909302\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009255630373954772\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.00946661651134491\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009477225542068481\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009569305777549744\n",
      "step 0: loss = 0.0096\n",
      "test_loss: 0.009504883289337159\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009522436857223511\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009403104782104493\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009502514004707337\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009522889256477356\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.009497987031936645\n",
      "step 0: loss = 0.0093\n",
      "test_loss: 0.009527709484100342\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009770394563674926\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009438731074333191\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009382880330085754\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009283356070518493\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009384058713912964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: loss = 0.0092\n",
      "test_loss: 0.00936903953552246\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009333707094192505\n",
      "step 0: loss = 0.0085\n",
      "test_loss: 0.009378741979598999\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009603625535964966\n",
      "step 0: loss = 0.0093\n",
      "test_loss: 0.009594237208366394\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009654552936553954\n",
      "step 0: loss = 0.0095\n",
      "test_loss: 0.009554157257080078\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.009487879872322082\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009575966596603393\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.009478806853294372\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009394279718399047\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.00948985755443573\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.00936271607875824\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009688311219215394\n",
      "step 0: loss = 0.0096\n",
      "test_loss: 0.009423916935920715\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009611356258392333\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009379985332489014\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009409704208374024\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.009428144097328187\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009516677856445312\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009439653158187867\n",
      "step 0: loss = 0.0095\n",
      "test_loss: 0.009660085439682007\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009226398468017578\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009583320617675781\n",
      "step 0: loss = 0.0095\n",
      "test_loss: 0.009340922832489013\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.00951870322227478\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009299539923667909\n",
      "step 0: loss = 0.0093\n",
      "test_loss: 0.009431401491165161\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009509403705596924\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009384307265281677\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009386802315711975\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009223353862762452\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009411441087722778\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009320991635322571\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009538662433624268\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009493289589881897\n",
      "step 0: loss = 0.0093\n",
      "test_loss: 0.009507514238357544\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.00961961567401886\n",
      "step 0: loss = 0.0093\n",
      "test_loss: 0.009353815317153931\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.009744306206703185\n",
      "step 0: loss = 0.0095\n",
      "test_loss: 0.009435726404190064\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.009153493642807008\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009438607096672057\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009759717583656312\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009365730285644532\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009369629621505737\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009409778118133545\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009406647086143494\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.009279016852378846\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.009500415325164794\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.009696238040924073\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009566596746444701\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009531947374343873\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.009389828443527221\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009441083669662476\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.009653279185295105\n",
      "step 0: loss = 0.0098\n",
      "test_loss: 0.00947838008403778\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009429877996444702\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009333176016807556\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009360154867172241\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009264888763427735\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009497769474983216\n",
      "step 0: loss = 0.0093\n",
      "test_loss: 0.009432238340377808\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009530616402626037\n",
      "step 0: loss = 0.0098\n",
      "test_loss: 0.00962462067604065\n",
      "step 0: loss = 0.0094\n",
      "test_loss: 0.009432085156440735\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009403207898139953\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009383383989334106\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009396898746490478\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009464996457099915\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.009611506462097168\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009678736329078674\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009447638988494872\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009492490887641907\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009297991991043092\n",
      "step 0: loss = 0.0084\n",
      "test_loss: 0.009236890077590942\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009194047451019286\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009311751127243043\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009241156578063965\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.00929502010345459\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009536765217781067\n",
      "step 0: loss = 0.0085\n",
      "test_loss: 0.009697142839431763\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.009439858198165894\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009296209812164306\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009521539211273194\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009340728521347047\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.009586416482925415\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009585580229759217\n",
      "step 0: loss = 0.0096\n",
      "test_loss: 0.009546128511428833\n",
      "step 0: loss = 0.0093\n",
      "test_loss: 0.00956561803817749\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009432802796363831\n",
      "step 0: loss = 0.0095\n",
      "test_loss: 0.009471064209938049\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009393550157546997\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009744671583175659\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.009323928356170654\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.009572258591651917\n",
      "step 0: loss = 0.0094\n",
      "test_loss: 0.009422314763069152\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009362857341766357\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009497166275978089\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.009563199281692504\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009276698231697083\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.00949911653995514\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009535003900527955\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009388580322265624\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009158743023872375\n",
      "step 0: loss = 0.0083\n",
      "test_loss: 0.009368023872375487\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009542508125305176\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009663886427879333\n",
      "step 0: loss = 0.0093\n",
      "test_loss: 0.009310453534126281\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009651873707771302\n",
      "step 0: loss = 0.0093\n",
      "test_loss: 0.009304622411727905\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009321413040161132\n",
      "step 0: loss = 0.0093\n",
      "test_loss: 0.009336791634559631\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.0093861323595047\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009686448574066163\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.00951633632183075\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009402546882629394\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009523556232452393\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009271365404129029\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009263142347335815\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009305344223976135\n",
      "step 0: loss = 0.0093\n",
      "test_loss: 0.00933348000049591\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009483326077461243\n",
      "step 0: loss = 0.0094\n",
      "test_loss: 0.009547598361968994\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009512078166007996\n",
      "step 0: loss = 0.0093\n",
      "test_loss: 0.009493114948272706\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009450936913490296\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009455158710479736\n",
      "step 0: loss = 0.0093\n",
      "test_loss: 0.009391587972640992\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009475600123405457\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.00944836437702179\n",
      "step 0: loss = 0.0085\n",
      "test_loss: 0.009409328699111938\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009432558417320252\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009354202747344971\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009394066333770752\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009637528657913208\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009144346117973328\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009536284804344177\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009413864612579346\n",
      "step 0: loss = 0.0093\n",
      "test_loss: 0.009330679774284362\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.00959930121898651\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009416513442993164\n",
      "step 0: loss = 0.0094\n",
      "test_loss: 0.00947130560874939\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009300788640975952\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.00930010437965393\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009391981959342956\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.009522691369056702\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.009255634546279907\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009293788075447083\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.00930159628391266\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009194777011871338\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009384269118309021\n",
      "step 0: loss = 0.0091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 0.009462733268737793\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.009521654844284057\n",
      "step 0: loss = 0.0095\n",
      "test_loss: 0.00929696261882782\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009307093024253844\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009666650891304016\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.00932434320449829\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.00923300266265869\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.0093774151802063\n",
      "step 0: loss = 0.0097\n",
      "test_loss: 0.009757463932037353\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009337984919548036\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009407848715782166\n",
      "step 0: loss = 0.0084\n",
      "test_loss: 0.009476780295372009\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.00930970549583435\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.00939538598060608\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.00922407627105713\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009552507400512696\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009350388646125793\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.00941051721572876\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.00938973307609558\n",
      "step 0: loss = 0.0095\n",
      "test_loss: 0.009319283366203308\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009320340752601623\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.009569287896156312\n",
      "step 0: loss = 0.0093\n",
      "test_loss: 0.009374062418937684\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009470008015632629\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.009502373337745666\n",
      "step 0: loss = 0.0096\n",
      "test_loss: 0.009180110692977906\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009299362897872926\n",
      "step 0: loss = 0.0093\n",
      "test_loss: 0.009381906390190124\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.009632459282875061\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.00964021384716034\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009259664416313172\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.00943390727043152\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009413620829582215\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009229447841644287\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009363237619400024\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.009455693364143371\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.00935111403465271\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009491778016090392\n",
      "step 0: loss = 0.0096\n",
      "test_loss: 0.009301410317420959\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009282546043395996\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009535646438598633\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.009345799684524536\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009294421672821044\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009187076091766357\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009305568933486939\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.009179013967514037\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.009296514987945557\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.009318859577178954\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009375742077827454\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009491339921951295\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.009318283796310424\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009472017884254455\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009374841451644897\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009181461334228515\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.009239129424095154\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.009249178171157836\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009628071784973144\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.00930361807346344\n",
      "step 0: loss = 0.0085\n",
      "test_loss: 0.009378492832183838\n",
      "step 0: loss = 0.0094\n",
      "test_loss: 0.009273326396942139\n",
      "step 0: loss = 0.0084\n",
      "test_loss: 0.009524117708206176\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009363731145858764\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009483345150947571\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009377300143241882\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009595432877540588\n",
      "step 0: loss = 0.0096\n",
      "test_loss: 0.00927760124206543\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.00917385995388031\n",
      "step 0: loss = 0.0094\n",
      "test_loss: 0.009215609431266784\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009413784742355347\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009412264823913575\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009301941394805908\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.0095614355802536\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009418650269508362\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.009276706576347351\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.009453924894332886\n",
      "step 0: loss = 0.0093\n",
      "test_loss: 0.009332286715507508\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009394619464874267\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009344013333320618\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009311538934707642\n",
      "step 0: loss = 0.0085\n",
      "test_loss: 0.009612982273101806\n",
      "step 0: loss = 0.0096\n",
      "test_loss: 0.00939638614654541\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009278417229652405\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009283912777900695\n",
      "step 0: loss = 0.0085\n",
      "test_loss: 0.009241541028022765\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009315043091773986\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009430918097496032\n",
      "step 0: loss = 0.0093\n",
      "test_loss: 0.009399487972259521\n",
      "step 0: loss = 0.0093\n",
      "test_loss: 0.009342719316482544\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.00946137249469757\n",
      "step 0: loss = 0.0083\n",
      "test_loss: 0.009399349689483643\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009395562410354615\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009306489825248718\n",
      "step 0: loss = 0.0084\n",
      "test_loss: 0.009065183997154235\n",
      "step 0: loss = 0.0085\n",
      "test_loss: 0.00921476423740387\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009472883939743042\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.00930531620979309\n",
      "step 0: loss = 0.0093\n",
      "test_loss: 0.009300455451011658\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009331530928611755\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009405644536018372\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.00945237398147583\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009479562640190125\n",
      "step 0: loss = 0.0084\n",
      "test_loss: 0.009418392777442932\n",
      "step 0: loss = 0.0093\n",
      "test_loss: 0.009245203137397765\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009575651288032531\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009229625463485719\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009368777871131898\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.00931032419204712\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009399625062942505\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009277195930480956\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.00938509464263916\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009405869245529174\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.009322692155838013\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.00943284809589386\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.009525971412658691\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.00937698245048523\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009541247487068177\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.00955702006816864\n",
      "step 0: loss = 0.0094\n",
      "test_loss: 0.009377813339233399\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009169211983680725\n",
      "step 0: loss = 0.0093\n",
      "test_loss: 0.009429191946983337\n",
      "step 0: loss = 0.0094\n",
      "test_loss: 0.009235056042671204\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009387056827545166\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009636874794960022\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.00943299114704132\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009438152313232423\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009228875041007995\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009263108372688294\n",
      "step 0: loss = 0.0085\n",
      "test_loss: 0.009152557849884033\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.009471654891967773\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.00925872564315796\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.009455448389053345\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.009250757694244384\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009100226759910584\n",
      "step 0: loss = 0.0083\n",
      "test_loss: 0.009229629039764405\n",
      "step 0: loss = 0.0084\n",
      "test_loss: 0.009537647366523743\n",
      "step 0: loss = 0.0093\n",
      "test_loss: 0.00932951033115387\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009519477486610412\n",
      "step 0: loss = 0.0095\n",
      "test_loss: 0.009282256364822388\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.00919474720954895\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.009388608336448669\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.009182130098342895\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009309790730476379\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.00930979311466217\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009289431571960449\n",
      "step 0: loss = 0.0093\n",
      "test_loss: 0.00940716803073883\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009513317346572877\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.00942682385444641\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009241669178009034\n",
      "step 0: loss = 0.0085\n",
      "test_loss: 0.009360666275024415\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009241097569465638\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009341443777084351\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.00946592390537262\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.00936329185962677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009594451785087586\n",
      "step 0: loss = 0.0093\n",
      "test_loss: 0.009500896334648132\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009165288805961609\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009307503700256348\n",
      "step 0: loss = 0.0085\n",
      "test_loss: 0.009300997853279114\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009229044318199158\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009398529529571533\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009196272492408753\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.009397608041763306\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009383633732795715\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009420974254608154\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009312160015106201\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009200961589813232\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009324052929878235\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009254478216171265\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009348561763763428\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009349888563156128\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009564566612243652\n",
      "step 0: loss = 0.0095\n",
      "test_loss: 0.009232861399650573\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.009361820816993714\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009326249957084656\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009241379499435425\n",
      "step 0: loss = 0.0085\n",
      "test_loss: 0.009410998225212098\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009243214130401611\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009348436594009399\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.0094150048494339\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009215674996376037\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.009278790354728698\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009390117526054382\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009320806264877319\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.009283972978591919\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009076364040374756\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009291032552719116\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.00940434992313385\n",
      "step 0: loss = 0.0093\n",
      "test_loss: 0.009348630905151367\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009270012378692627\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009264591932296753\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009305608272552491\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009283143877983093\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.009214104413986205\n",
      "step 0: loss = 0.0085\n",
      "test_loss: 0.009317736625671386\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009145427346229553\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.00920593023300171\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.009336963295936584\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.009464091658592223\n",
      "step 0: loss = 0.0096\n",
      "test_loss: 0.009497770071029664\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009048041701316834\n",
      "step 0: loss = 0.0081\n",
      "test_loss: 0.009466437697410584\n",
      "step 0: loss = 0.0093\n",
      "test_loss: 0.009102510809898377\n",
      "step 0: loss = 0.0084\n",
      "test_loss: 0.009367318153381347\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.009302282333374023\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009318482279777527\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009243908524513244\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009185317158699035\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.00942300796508789\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.009283491969108581\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009279860258102417\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009313165545463561\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009332877993583679\n",
      "step 0: loss = 0.0085\n",
      "test_loss: 0.009368336796760558\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009364509582519531\n",
      "step 0: loss = 0.0093\n",
      "test_loss: 0.009338952898979187\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009573069810867309\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.00922636330127716\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009346664547920228\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009326096773147583\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.009401845335960389\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009326406121253968\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.009635788202285767\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009275722503662109\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009382538199424743\n",
      "step 0: loss = 0.0093\n",
      "test_loss: 0.009476414322853089\n",
      "step 0: loss = 0.0095\n",
      "test_loss: 0.009236130118370055\n",
      "step 0: loss = 0.0093\n",
      "test_loss: 0.009474348425865173\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009542704224586487\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009537329077720641\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009303501844406127\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.009372850060462951\n",
      "step 0: loss = 0.0085\n",
      "test_loss: 0.009312202334403991\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009336664080619813\n",
      "step 0: loss = 0.0094\n",
      "test_loss: 0.009431406259536743\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009463590383529664\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009309472441673279\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009316278100013732\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.009538987874984741\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009333404898643493\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.00945173978805542\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009261645078659057\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.009236318469047546\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009434944987297057\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.00920068085193634\n",
      "step 0: loss = 0.0085\n",
      "test_loss: 0.009456600546836853\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009285101294517517\n",
      "step 0: loss = 0.0093\n",
      "test_loss: 0.009092295169830322\n",
      "step 0: loss = 0.0085\n",
      "test_loss: 0.009319841861724854\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009272217750549316\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.009247371554374694\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.00944625735282898\n",
      "step 0: loss = 0.0098\n",
      "test_loss: 0.00915786623954773\n",
      "step 0: loss = 0.0085\n",
      "test_loss: 0.00928000569343567\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.009224791526794434\n",
      "step 0: loss = 0.0082\n",
      "test_loss: 0.00928676724433899\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.00930425763130188\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.00945623278617859\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009406059980392456\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009494064450263977\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009234870672225953\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009584492444992066\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.009100801348686217\n",
      "step 0: loss = 0.0085\n",
      "test_loss: 0.009310567378997802\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.00926901638507843\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009313737750053406\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009516921639442445\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009021025896072388\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.009225598573684692\n",
      "step 0: loss = 0.0085\n",
      "test_loss: 0.009281530380249023\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009272953867912293\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009317282438278198\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.00923277735710144\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009479062557220459\n",
      "step 0: loss = 0.0093\n",
      "test_loss: 0.009289448857307434\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009295865297317504\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.009207122921943665\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009360387921333313\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.00935263216495514\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.009200056791305542\n",
      "step 0: loss = 0.0084\n",
      "test_loss: 0.009377048015594483\n",
      "step 0: loss = 0.0082\n",
      "test_loss: 0.009708110690116883\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009347673058509826\n",
      "step 0: loss = 0.0084\n",
      "test_loss: 0.009259120225906373\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.009306739568710327\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009464007616043092\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009204736948013305\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.00920417845249176\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.00918899655342102\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009297925233840942\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009423114657402039\n",
      "step 0: loss = 0.0093\n",
      "test_loss: 0.009474791288375854\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.009222963452339172\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009424905180931092\n",
      "step 0: loss = 0.0093\n",
      "test_loss: 0.009156856536865234\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.00924017608165741\n",
      "step 0: loss = 0.0084\n",
      "test_loss: 0.009310120940208435\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009242878556251525\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009467115998268128\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.00915645956993103\n",
      "step 0: loss = 0.0085\n",
      "test_loss: 0.009296692609786987\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.009147565364837646\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.009308802485466004\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.00948431432247162\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009072396159172057\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009050028920173646\n",
      "step 0: loss = 0.0090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 0.009256112575531005\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009180520176887513\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.009357097148895264\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.00951068639755249\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009380071759223937\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009204322695732117\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009256626963615417\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009222795963287353\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009090502262115479\n",
      "step 0: loss = 0.0083\n",
      "test_loss: 0.009674981832504273\n",
      "step 0: loss = 0.0094\n",
      "test_loss: 0.009190874695777894\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009459108710289002\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009270002841949463\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009254018664360047\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009424105882644654\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009345014095306397\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.00915932834148407\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.009224420785903931\n",
      "step 0: loss = 0.0085\n",
      "test_loss: 0.009327542781829835\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009298140406608582\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.00907063364982605\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009343592524528503\n",
      "step 0: loss = 0.0083\n",
      "test_loss: 0.009093797802925109\n",
      "step 0: loss = 0.0085\n",
      "test_loss: 0.009392995834350586\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.009188138246536255\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.00926247239112854\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.009121953248977662\n",
      "step 0: loss = 0.0085\n",
      "test_loss: 0.009110774993896485\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.00927862286567688\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009271673560142517\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009430654048919677\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009182978868484497\n",
      "step 0: loss = 0.0085\n",
      "test_loss: 0.009461682438850403\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009141775369644166\n",
      "step 0: loss = 0.0084\n",
      "test_loss: 0.009269328117370605\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009262458682060241\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009381814002990723\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009089959263801574\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009049292802810669\n",
      "step 0: loss = 0.0085\n",
      "test_loss: 0.009354510903358459\n",
      "step 0: loss = 0.0085\n",
      "test_loss: 0.009198018908500671\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009274309277534485\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009637584090232848\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.00930642604827881\n",
      "step 0: loss = 0.0093\n",
      "test_loss: 0.009267665147781372\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009198347330093384\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009401803612709045\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009246506094932556\n",
      "step 0: loss = 0.0082\n",
      "test_loss: 0.009376057386398316\n",
      "step 0: loss = 0.0085\n",
      "test_loss: 0.009198764562606812\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009389398097991943\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.00913723886013031\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.00932761311531067\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009346542954444885\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009407981038093568\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009392998218536376\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009144482016563416\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.00942046582698822\n",
      "step 0: loss = 0.0095\n",
      "test_loss: 0.009237799644470215\n",
      "step 0: loss = 0.0085\n",
      "test_loss: 0.009350676536560059\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.00924208402633667\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009555105566978455\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009266072511672973\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.00925695538520813\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.009420158267021179\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.00899478316307068\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.009214581251144409\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009195637106895447\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009352260828018188\n",
      "step 0: loss = 0.0085\n",
      "test_loss: 0.009298444390296937\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009140005707740784\n",
      "step 0: loss = 0.0084\n",
      "test_loss: 0.009123107194900512\n",
      "step 0: loss = 0.0093\n",
      "test_loss: 0.009057905077934265\n",
      "step 0: loss = 0.0082\n",
      "test_loss: 0.009463757276535034\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.009382919669151306\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.009213777184486389\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009263535737991333\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009393203258514404\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009284204840660094\n",
      "step 0: loss = 0.0095\n",
      "test_loss: 0.009351808428764343\n",
      "step 0: loss = 0.0083\n",
      "test_loss: 0.009363868832588195\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.00929439663887024\n",
      "step 0: loss = 0.0081\n",
      "test_loss: 0.009277758598327636\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009271057844161988\n",
      "step 0: loss = 0.0085\n",
      "test_loss: 0.009323702454566955\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.009153519868850707\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009334239363670348\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.009409590363502503\n",
      "step 0: loss = 0.0083\n",
      "test_loss: 0.009218193292617798\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.00929369568824768\n",
      "step 0: loss = 0.0093\n",
      "test_loss: 0.009255462884902954\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.00925946056842804\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009201231002807618\n",
      "step 0: loss = 0.0084\n",
      "test_loss: 0.00947913110256195\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009301213026046752\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009311950802803039\n",
      "step 0: loss = 0.0085\n",
      "test_loss: 0.00907975971698761\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009335834383964538\n",
      "step 0: loss = 0.0085\n",
      "test_loss: 0.0093386971950531\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009440150260925293\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009191012382507325\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.009098619222640991\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009321116209030152\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009384858012199402\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009401783347129822\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009173755645751952\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009204986095428467\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.00926558554172516\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.009034659266471862\n",
      "step 0: loss = 0.0085\n",
      "test_loss: 0.009358432888984681\n",
      "step 0: loss = 0.0094\n",
      "test_loss: 0.009416741132736207\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009307595491409302\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009268465638160705\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.009252825975418091\n",
      "step 0: loss = 0.0095\n",
      "test_loss: 0.009328662157058716\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.00932940661907196\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009125953912734986\n",
      "step 0: loss = 0.0085\n",
      "test_loss: 0.009228140115737915\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009054800271987915\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.00924980640411377\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.009384610056877137\n",
      "step 0: loss = 0.0093\n",
      "test_loss: 0.00913368821144104\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009172972440719605\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009459766745567321\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009190972447395324\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009403782486915589\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009161495566368104\n",
      "step 0: loss = 0.0083\n",
      "test_loss: 0.009235616326332092\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009150750041007995\n",
      "step 0: loss = 0.0084\n",
      "test_loss: 0.009259571433067321\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009374833703041076\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.00925901234149933\n",
      "step 0: loss = 0.0081\n",
      "test_loss: 0.009279467463493347\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.009049018621444702\n",
      "step 0: loss = 0.0084\n",
      "test_loss: 0.009073100090026855\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009108107089996337\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009336265325546265\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.009356693029403687\n",
      "step 0: loss = 0.0085\n",
      "test_loss: 0.009451594948768616\n",
      "step 0: loss = 0.0085\n",
      "test_loss: 0.009345414638519288\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009280413985252381\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009485158920288086\n",
      "step 0: loss = 0.0085\n",
      "test_loss: 0.009256801605224609\n",
      "step 0: loss = 0.0085\n",
      "test_loss: 0.009036270380020141\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.0091515052318573\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.009438032507896424\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009108360409736633\n",
      "step 0: loss = 0.0085\n",
      "test_loss: 0.00914262056350708\n",
      "step 0: loss = 0.0085\n",
      "test_loss: 0.009165496826171874\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009300689101219178\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009243128895759583\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009387726187705994\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009418315291404723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009468648433685303\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.00941927671432495\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009218792915344238\n",
      "step 0: loss = 0.0084\n",
      "test_loss: 0.009240161180496215\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009197253584861755\n",
      "step 0: loss = 0.0081\n",
      "test_loss: 0.009325242042541504\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009442656636238098\n",
      "step 0: loss = 0.0094\n",
      "test_loss: 0.009169229865074157\n",
      "step 0: loss = 0.0083\n",
      "test_loss: 0.009320728182792663\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009339910745620728\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.00926833152770996\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009241839051246642\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009051544666290283\n",
      "step 0: loss = 0.0082\n",
      "test_loss: 0.009132473468780518\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.008982700705528259\n",
      "step 0: loss = 0.0084\n",
      "test_loss: 0.009107506871223449\n",
      "step 0: loss = 0.0082\n",
      "test_loss: 0.0093228942155838\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.0093459814786911\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.009327337145805359\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.00903207778930664\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.009124535918235779\n",
      "step 0: loss = 0.0085\n",
      "test_loss: 0.009154232740402222\n",
      "step 0: loss = 0.0083\n",
      "test_loss: 0.00944214940071106\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009145224690437317\n",
      "step 0: loss = 0.0082\n",
      "test_loss: 0.009294978976249695\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009201130867004394\n",
      "step 0: loss = 0.0080\n",
      "test_loss: 0.009283851385116577\n",
      "step 0: loss = 0.0085\n",
      "test_loss: 0.00924289584159851\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009090526700019837\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009374192357063294\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009254726767539977\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.00917972207069397\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009330312609672547\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009308775663375854\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009338635802268982\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.00904353141784668\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009015388488769531\n",
      "step 0: loss = 0.0083\n",
      "test_loss: 0.009059922099113464\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009179751873016357\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.00914018154144287\n",
      "step 0: loss = 0.0085\n",
      "test_loss: 0.009209972620010377\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009193564057350159\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.009387142658233642\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009251779317855835\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009312361478805542\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.00911202907562256\n",
      "step 0: loss = 0.0084\n",
      "test_loss: 0.009111018776893616\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009294567704200745\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009274541735649108\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009017616510391235\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.009187605381011963\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009177772402763367\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.00927608847618103\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009177926778793335\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009272802472114563\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.00912916898727417\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009252567291259766\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.009248785972595215\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009049112796783448\n",
      "step 0: loss = 0.0084\n",
      "test_loss: 0.00907032608985901\n",
      "step 0: loss = 0.0083\n",
      "test_loss: 0.009372204542160034\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009059774875640868\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009258705377578735\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.00939311385154724\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009311718940734863\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.009197526574134827\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.00923633635044098\n",
      "step 0: loss = 0.0085\n",
      "test_loss: 0.009298096895217895\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.009242303967475891\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009029943943023682\n",
      "step 0: loss = 0.0085\n",
      "test_loss: 0.009111499786376953\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.00922670066356659\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009272069931030273\n",
      "step 0: loss = 0.0093\n",
      "test_loss: 0.009204603433609009\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009011293053627014\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.00907128632068634\n",
      "step 0: loss = 0.0085\n",
      "test_loss: 0.009167667627334595\n",
      "step 0: loss = 0.0085\n",
      "test_loss: 0.00910605013370514\n",
      "step 0: loss = 0.0085\n",
      "test_loss: 0.009055723547935485\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009174056649208069\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009299079179763794\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009177350401878358\n",
      "step 0: loss = 0.0083\n",
      "test_loss: 0.009392049312591553\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.00907138228416443\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.009245150089263916\n",
      "step 0: loss = 0.0078\n",
      "test_loss: 0.009227230548858642\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.00918997347354889\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009268583059310912\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.00900664746761322\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009359689950942994\n",
      "step 0: loss = 0.0096\n",
      "test_loss: 0.009156785607337951\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009447477459907531\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009195655584335327\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009338811039924622\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009175443053245545\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.009096301794052124\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009254356026649474\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.009251922369003296\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.00913674533367157\n",
      "step 0: loss = 0.0084\n",
      "test_loss: 0.00923054814338684\n",
      "step 0: loss = 0.0085\n",
      "test_loss: 0.00917337715625763\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009260724186897279\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.00910511314868927\n",
      "step 0: loss = 0.0085\n",
      "test_loss: 0.009234144687652587\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009320318102836608\n",
      "step 0: loss = 0.0095\n",
      "test_loss: 0.009105342626571655\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.009276814460754394\n",
      "step 0: loss = 0.0084\n",
      "test_loss: 0.009288617372512818\n",
      "step 0: loss = 0.0093\n",
      "test_loss: 0.00905868411064148\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009183154106140137\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009245155453681946\n",
      "step 0: loss = 0.0083\n",
      "test_loss: 0.009401253461837768\n",
      "step 0: loss = 0.0092\n",
      "test_loss: 0.009410708546638488\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009145476818084718\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.009049860835075378\n",
      "step 0: loss = 0.0081\n",
      "test_loss: 0.009138551354408265\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009029067158699035\n",
      "step 0: loss = 0.0085\n",
      "test_loss: 0.009138739705085754\n",
      "step 0: loss = 0.0084\n",
      "test_loss: 0.009109970331192017\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009248388409614563\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009304907321929932\n",
      "step 0: loss = 0.0093\n",
      "test_loss: 0.009315805435180664\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009295923709869385\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.009151011109352111\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009207580089569092\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.00921927034854889\n",
      "step 0: loss = 0.0084\n",
      "test_loss: 0.009089177250862121\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009077024459838868\n",
      "step 0: loss = 0.0083\n",
      "test_loss: 0.009247133731842041\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009152742624282837\n",
      "step 0: loss = 0.0085\n",
      "test_loss: 0.00910434365272522\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009175552129745483\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009198895692825317\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009105151891708374\n",
      "step 0: loss = 0.0081\n",
      "test_loss: 0.009234391450881958\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009146983027458191\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.00908144772052765\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.009105702638626098\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009173688888549804\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009113696217536927\n",
      "step 0: loss = 0.0085\n",
      "test_loss: 0.009465464949607849\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.009064963459968567\n",
      "step 0: loss = 0.0084\n",
      "test_loss: 0.009082787036895752\n",
      "step 0: loss = 0.0085\n",
      "test_loss: 0.00943255066871643\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009408959150314332\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009183213114738464\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.008959431052207947\n",
      "step 0: loss = 0.0083\n",
      "test_loss: 0.00915118932723999\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009117293953895569\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.00937398910522461\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009116936922073365\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.008965746760368347\n",
      "step 0: loss = 0.0089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 0.009319987297058106\n",
      "step 0: loss = 0.0084\n",
      "test_loss: 0.009001184701919556\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.009543954133987427\n",
      "step 0: loss = 0.0091\n",
      "test_loss: 0.009354785680770875\n",
      "step 0: loss = 0.0086\n",
      "test_loss: 0.00927104651927948\n",
      "step 0: loss = 0.0083\n",
      "test_loss: 0.009270716309547424\n",
      "step 0: loss = 0.0093\n",
      "test_loss: 0.008968934416770935\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009141297936439514\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009231497645378112\n",
      "step 0: loss = 0.0090\n",
      "test_loss: 0.009041218161582947\n",
      "step 0: loss = 0.0087\n",
      "test_loss: 0.009231114983558655\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009111796021461487\n",
      "step 0: loss = 0.0083\n",
      "test_loss: 0.009208092093467712\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009208779335021972\n",
      "step 0: loss = 0.0083\n",
      "test_loss: 0.009365463852882385\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009135335683822632\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.00906499981880188\n",
      "step 0: loss = 0.0085\n",
      "test_loss: 0.009108151793479919\n",
      "step 0: loss = 0.0085\n",
      "test_loss: 0.009054843187332153\n",
      "step 0: loss = 0.0088\n",
      "test_loss: 0.009467164278030396\n",
      "step 0: loss = 0.0089\n",
      "test_loss: 0.009152735471725465\n",
      "step 0: loss = 0.0090\n"
     ]
    }
   ],
   "source": [
    "#optimizer = tf.keras.optimizers.SGD(learning_rate=1e-1)\n",
    "learning_rate = 1\n",
    "epochs = 4000\n",
    "\n",
    "alpha = 0.001\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "eta = 1e-8\n",
    "\n",
    "m = [0]*len(model.trainable_weights)\n",
    "v = [0]*len(model.trainable_weights)\n",
    "t = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for step, (points, Omega_Omegabar, mass, restriction) in enumerate(train_set):\n",
    "        with tf.GradientTape() as tape:\n",
    "            \n",
    "            omega = volume_form(points, Omega_Omegabar, mass, restriction)\n",
    "            loss = weighted_MAPE(Omega_Omegabar, omega, mass)  \n",
    "            grads = tape.gradient(loss, model.trainable_weights)\n",
    "            #print(grads)\n",
    "\n",
    "        t = t + 1\n",
    "        i = 0\n",
    "        for weight, grad in zip(model.trainable_weights, grads):\n",
    "            \n",
    "            m[i] = beta1 * m[i] + (1 - beta1) * grad\n",
    "            v[i] = beta2 * v[i] + (1 - beta2) * tf.multiply(grad, tf.math.conj(grad))\n",
    "  \n",
    "            alpha_t = alpha * np.sqrt(1 - beta2**t) / (1 - beta1**t)\n",
    "            theta = alpha_t * m[i] / (tf.sqrt(v[i]) + eta)\n",
    "            #print(theta)\n",
    "            \n",
    "            i = i + 1\n",
    "            \n",
    "            with tf.device('/cpu:0'):            \n",
    "                weight_cpu = tf.Variable(weight)\n",
    "                weight_cpu.assign_sub(theta)\n",
    "                #weight_cpu.assign_sub(learning_rate*grad)\n",
    "                weight.assign(weight_cpu)\n",
    "                #print(weight)\n",
    "\n",
    "    \n",
    "        if step % 500 == 0:\n",
    "            print(\"step %d: loss = %.4f\" % (step, loss))\n",
    "    \n",
    "    test_loss = 0\n",
    "    test_loss_old = 100\n",
    "    \n",
    "    for step, (points, Omega_Omegabar, mass, restriction) in enumerate(test_set):\n",
    "        omega = volume_form(points, Omega_Omegabar, mass, restriction)\n",
    "        test_loss += weighted_MAPE(Omega_Omegabar, omega, mass)\n",
    "   \n",
    "    test_loss = tf.math.real(test_loss).numpy()/(step+1)\n",
    "    print(\"test_loss:\", test_loss)\n",
    "    \n",
    "    # This part doesn't work right now\n",
    "    if test_loss > test_loss_old:\n",
    "        break\n",
    "    test_loss_old = test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some debugging tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def volume_form(x, Omega_Omegabar, mass, restriction):\n",
    "\n",
    "    kahler_metric = complex_hessian(tf.math.real(model(x)), x)\n",
    "    volume_form = tf.linalg.det(tf.matmul(restriction, tf.matmul(kahler_metric, restriction, adjoint_b=True)))\n",
    "    weights = mass / tf.reduce_sum(mass)\n",
    "    factor = tf.reduce_sum(weights * volume_form / Omega_Omegabar)\n",
    "    #factor = tf.constant(4.380538, dtype=tf.complex64)\n",
    "    return  volume_form/factor\n",
    "    #return factor\n",
    "for step, (points, Omega_Omegabar, mass, restriction) in enumerate(dataset):\n",
    "    omega = volume_form(points, Omega_Omegabar, mass, restriction)\n",
    "    \n",
    "    weights = mass / tf.reduce_sum(mass)\n",
    "    print('omega', omega)\n",
    "    print('OO',Omega_Omegabar)\n",
    "    print(tf.cast(tf.abs(Omega_Omegabar -  omega), dtype=tf.complex64) / Omega_Omegabar)\n",
    "   # print(mass/tf.reduce_sum(mass))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(5, 15) dtype=complex64, numpy=\n",
      "array([[ 2.2768168e+00+1.6352530e+00j,  5.8658237e+00-4.2718673e-01j,\n",
      "         5.3873414e-01-1.7244281e-01j, -3.9534414e-01+7.2521999e-02j,\n",
      "        -1.8923687e-02+1.1802919e-02j, -3.8347977e-01-2.2627316e-01j,\n",
      "        -7.6662725e-01+8.0790210e-01j,  2.6793966e-01+6.7697294e-02j,\n",
      "        -4.4106621e-01-1.5242220e-01j, -1.6198709e+00-5.7326740e-01j,\n",
      "         2.8095651e+00+8.1460869e-01j, -1.1343197e+00-2.5883400e+00j,\n",
      "        -4.6649609e+00+1.1073344e+00j,  1.7060937e-01-2.2714563e-02j,\n",
      "         8.5827917e-01-7.9530555e-01j],\n",
      "       [ 1.9180773e+00+4.4432992e-01j, -1.7982172e+00+1.0105304e+00j,\n",
      "        -6.0589733e+00-8.5629034e-01j,  2.9838247e+00-3.7910095e-01j,\n",
      "         2.8432586e+00+9.5829725e-01j, -4.8352519e-01-4.6606249e-01j,\n",
      "        -1.7036586e+00-9.6777737e-01j, -3.2158126e-03+1.6477600e-03j,\n",
      "         1.7850419e+00+6.1775154e-01j,  4.3784446e-01-8.8729739e-02j,\n",
      "         2.8238079e-01+1.9228293e-01j, -2.9208440e-01+1.6115788e-01j,\n",
      "        -2.7304552e+00+1.0982864e+00j, -1.3929390e+00+3.4327587e-01j,\n",
      "        -3.5853200e+00-1.3666987e-01j],\n",
      "       [-1.4581397e+00+1.9110115e+00j, -1.7909920e-01+3.6252013e-01j,\n",
      "        -1.1931094e+00+3.0919951e-01j,  7.2101420e-01-1.2520562e+00j,\n",
      "        -5.5755311e-01-2.0128660e-02j,  3.1300583e+00-2.7365923e-01j,\n",
      "         4.9949604e-01-5.2422490e+00j, -1.7338538e-01+2.4592508e-02j,\n",
      "         1.9228923e+00-6.3471007e-01j,  3.0070311e-01+5.4850805e-01j,\n",
      "         1.8207195e+00-1.2229290e+00j, -4.6268797e+00-1.4987248e+00j,\n",
      "         1.1876729e+00+3.0295810e-01j, -7.5662631e-01+1.2398404e-01j,\n",
      "         3.7306695e+00+1.6974617e+00j],\n",
      "       [ 1.5903249e-01-2.4123290e-01j,  4.2902447e-02+8.1711315e-02j,\n",
      "        -3.3539832e-01+3.9116329e-01j, -2.4176884e+00+1.5029418e-02j,\n",
      "        -4.5817409e+00+8.9052248e-01j, -4.5735011e+00+3.7343332e-01j,\n",
      "         2.3762546e-02-4.5621830e-01j, -1.9385403e-01+8.9566067e-02j,\n",
      "         4.0899768e+00-1.5305756e-02j, -2.5236410e-01+9.8926350e-02j,\n",
      "        -2.1313027e-01-2.0266116e-02j,  2.7595261e-02+4.9135927e-03j,\n",
      "         4.4409629e-02+1.6594049e-01j, -5.0119634e+00+7.1290761e-02j,\n",
      "        -3.5859435e-03+2.7132857e-01j],\n",
      "       [ 2.8225083e+00-1.9472898e+00j, -1.9300312e-01+1.2453795e-01j,\n",
      "        -7.0934814e-01-1.3123615e-01j, -3.2763820e+00+1.2321837e+00j,\n",
      "         1.0924923e+00-7.1551239e-01j,  6.0893261e-01+3.7527627e-01j,\n",
      "         1.0850860e+00-8.6598074e-01j,  4.2221852e-02+8.6574353e-02j,\n",
      "        -2.4428689e+00-5.7877851e-01j,  5.8101020e+00-3.2419884e-01j,\n",
      "         4.1471138e+00+9.2794955e-02j,  1.2292573e+00+7.8488910e-01j,\n",
      "        -1.2955481e-01-9.0879291e-01j, -2.4329317e+00-7.5963151e-01j,\n",
      "         8.8978034e-01+5.8874261e-01j]], dtype=complex64)>\n",
      "<tf.Variable 'Variable:0' shape=(15, 15) dtype=complex64, numpy=\n",
      "array([[ 4.48537874e+00-5.30717336e-02j,  3.97880413e-02-8.86093974e-02j,\n",
      "        -5.31146526e-02+2.45492142e-02j,  8.67075175e-02-1.01624019e-01j,\n",
      "        -1.85670733e-01-6.91963136e-02j, -1.48487300e-01-6.05651662e-02j,\n",
      "         1.20190650e-01-1.69870649e-02j, -5.21362424e-02+1.12177646e-02j,\n",
      "        -9.27942619e-03-8.68658349e-02j,  1.54285967e-01-1.91998314e-02j,\n",
      "         5.47923846e-03+3.80867757e-02j,  3.08177918e-01+2.94221759e-01j,\n",
      "        -1.75426140e-01+1.19707145e-01j, -1.02277972e-01-1.52171299e-01j,\n",
      "        -1.69872731e-01-1.57393202e-01j],\n",
      "       [-5.20558469e-02+1.35510653e-01j,  4.76845026e+00-1.03557847e-01j,\n",
      "        -3.10402393e-01-2.63872325e-01j, -3.83945145e-02-8.85578468e-02j,\n",
      "        -6.10975474e-02-5.13151139e-02j,  3.30757163e-02-6.16653822e-02j,\n",
      "        -6.88735908e-03+3.06749791e-01j,  5.53110726e-02+3.24442722e-02j,\n",
      "         5.61661236e-02-9.50987190e-02j,  3.09128780e-02-1.52821139e-01j,\n",
      "        -5.20106591e-02-6.28411546e-02j,  7.88327605e-02+3.26288608e-03j,\n",
      "         2.27459460e-01+5.63018508e-02j,  8.88381824e-02-2.21358798e-02j,\n",
      "        -2.27249622e-01-5.98294660e-02j],\n",
      "       [-8.53944421e-02-1.59421653e-01j, -4.13616151e-01+3.89929742e-01j,\n",
      "         4.94551754e+00+4.43636924e-02j,  1.50215358e-01+4.44892310e-02j,\n",
      "        -1.41069680e-01+1.37919381e-01j,  4.76978607e-02+3.27756219e-02j,\n",
      "         3.87099892e-01-3.87359917e-01j,  2.56092846e-03+2.12033968e-02j,\n",
      "        -3.92926149e-02-7.29754344e-02j, -1.58888400e-01+7.86960423e-02j,\n",
      "        -1.02544896e-01+3.43110152e-02j, -1.23513997e-01+1.75399467e-01j,\n",
      "         8.80063027e-02+7.74727017e-02j, -4.35849488e-01-7.28005618e-02j,\n",
      "         2.04764962e-01+3.01671684e-01j],\n",
      "       [ 1.15745574e-01+1.57252774e-01j, -1.34361401e-01+5.97085431e-03j,\n",
      "        -1.57307282e-01-2.38582008e-02j,  4.51907349e+00+9.55896154e-02j,\n",
      "        -2.30961278e-01-1.77684844e-01j, -4.40963022e-02+1.08772136e-01j,\n",
      "         1.72351792e-01-3.85349840e-01j,  1.27901202e-02-2.35390174e-03j,\n",
      "         4.82990772e-01-4.60580401e-02j, -7.62113333e-02+4.24391218e-02j,\n",
      "         4.44995686e-02+1.28467217e-01j,  4.94761541e-02-1.91863418e-01j,\n",
      "        -6.11173883e-02+1.82351992e-01j, -1.56536192e-01-3.34206402e-01j,\n",
      "        -2.64782637e-01+9.08263773e-02j],\n",
      "       [-3.49517226e-01+4.60335091e-02j,  1.31304011e-01+1.01816170e-02j,\n",
      "        -3.40697199e-01-1.68635607e-01j, -5.71821369e-02+1.61503091e-01j,\n",
      "         4.63839626e+00+1.15938382e-02j,  6.46486878e-02-9.14678499e-02j,\n",
      "        -3.66927534e-02+1.30854309e-01j,  3.99125144e-02-9.48923826e-03j,\n",
      "         1.12672292e-01-1.14765264e-01j, -1.40039295e-01-4.60513979e-02j,\n",
      "         1.43637145e-02-1.35932982e-01j, -1.13129027e-01-5.67088183e-03j,\n",
      "        -5.44985309e-02-1.37464151e-01j,  2.69388884e-01-2.30145697e-02j,\n",
      "        -5.95672801e-02+1.25840053e-01j],\n",
      "       [-4.16479334e-02+4.21791933e-02j,  2.05890760e-02+2.97590904e-02j,\n",
      "         1.83426924e-02+1.32056922e-01j, -1.38517901e-01+1.10309437e-01j,\n",
      "         6.92991614e-02+5.72190918e-02j,  4.63341808e+00-9.73349288e-02j,\n",
      "         2.43234590e-01+1.41646847e-01j,  7.56485062e-03-4.06649448e-02j,\n",
      "         2.42830552e-02-5.73686250e-02j, -8.83598477e-02-5.31893317e-03j,\n",
      "        -1.07734889e-01+3.14638652e-02j, -1.21181324e-01-1.96566097e-02j,\n",
      "        -4.81016561e-02+2.78094318e-04j,  1.52766019e-01+1.74600426e-02j,\n",
      "        -3.01439971e-01-3.85408252e-02j],\n",
      "       [ 3.00037742e-01-6.98629916e-02j, -4.49373275e-02-1.61733106e-01j,\n",
      "         2.56243914e-01+6.09023511e-01j,  1.36562020e-01+2.60854691e-01j,\n",
      "        -2.74651754e-03-1.08257674e-01j,  1.03100389e-01-2.59782612e-01j,\n",
      "         4.71440935e+00+3.64487455e-03j,  3.80301401e-02-3.25144790e-02j,\n",
      "         9.35852975e-02+5.02375886e-02j,  2.00922623e-01-1.79778919e-01j,\n",
      "        -1.75713956e-01-1.36633992e-01j, -2.38222659e-01-1.92321986e-01j,\n",
      "        -1.23847418e-01-2.11267114e-01j,  3.34210247e-01-1.15863480e-01j,\n",
      "         2.37649888e-01-4.49349672e-01j],\n",
      "       [-1.66156478e-02-7.40435254e-03j,  4.84454557e-02-4.18997854e-02j,\n",
      "        -2.20417902e-02-1.80865303e-02j,  2.84999199e-02+1.54592039e-03j,\n",
      "         1.95790012e-03+9.60797537e-03j, -1.78892480e-03+2.34980136e-02j,\n",
      "         5.60312420e-02+6.79078884e-03j,  1.02035797e+00-1.41587004e-03j,\n",
      "         5.71653470e-02+1.35919440e-03j, -4.11586696e-03+1.70466807e-02j,\n",
      "        -1.22735696e-02+5.34778112e-04j,  2.58589257e-03-8.94873589e-03j,\n",
      "         3.30339633e-02-1.97864827e-02j, -9.78538394e-03+3.22149433e-02j,\n",
      "        -3.68325524e-02+5.54051772e-02j],\n",
      "       [-2.33105617e-04+1.45302713e-01j, -1.40367433e-01-7.84626901e-02j,\n",
      "        -2.98394740e-01+2.03740776e-01j,  4.79328364e-01+1.82237089e-01j,\n",
      "         8.17002635e-03+2.04623297e-01j,  3.38333040e-01+8.51498097e-02j,\n",
      "         1.94009811e-01-3.49350661e-01j,  1.14320517e-01-2.17770189e-02j,\n",
      "         4.51932383e+00+1.78256646e-01j,  1.14981748e-01+1.48429707e-01j,\n",
      "         5.64091764e-02-1.75696075e-01j, -3.98885965e-01-1.18323140e-01j,\n",
      "         3.33802626e-02-4.43213582e-02j,  6.17789589e-02+1.38245270e-01j,\n",
      "         2.13064000e-01+1.79522529e-01j],\n",
      "       [-7.09886604e-04-1.26861371e-02j, -2.62292355e-01+2.72542089e-01j,\n",
      "        -5.57135046e-02+1.39568806e-01j, -4.03439905e-03+4.15739752e-02j,\n",
      "        -2.80338436e-01+3.21313441e-02j, -1.63199157e-01-8.84823427e-02j,\n",
      "         2.22131297e-01+8.47790092e-02j, -4.71316651e-02-2.76802052e-02j,\n",
      "        -2.04155609e-01-9.45463106e-02j,  4.82785606e+00-9.14236009e-02j,\n",
      "        -6.13764375e-02-3.94683145e-02j,  3.66575152e-01-3.63307834e-01j,\n",
      "         1.35576446e-02+3.57938886e-01j, -2.09227741e-01-5.84533885e-02j,\n",
      "         9.33421105e-02-2.38120303e-01j],\n",
      "       [-4.62606281e-01-2.92114038e-02j, -1.05735824e-01-3.64217255e-03j,\n",
      "        -2.66819566e-01-5.29242568e-02j,  8.31092298e-02+5.24944253e-03j,\n",
      "        -1.63037237e-02+1.26137689e-01j,  5.59939817e-03-1.25258237e-01j,\n",
      "         1.06825724e-01+2.39404127e-01j, -1.50322216e-02-3.76014039e-03j,\n",
      "         7.54148513e-02+2.03093141e-02j,  2.86383659e-01-2.77430087e-01j,\n",
      "         4.48912239e+00-4.42797877e-03j,  4.68228877e-01+1.18662231e-02j,\n",
      "        -1.73990220e-01+1.60518482e-01j, -2.17481717e-01-1.80866912e-01j,\n",
      "        -9.43631157e-02-3.28368872e-01j],\n",
      "       [ 1.39473125e-01-3.72130185e-01j,  5.04988022e-02-1.66483656e-01j,\n",
      "        -4.57128733e-02-1.31300867e-01j,  2.80909566e-03+1.68979898e-01j,\n",
      "         8.12469050e-02+2.91961357e-02j, -4.57474701e-02-3.06981169e-02j,\n",
      "         3.91981453e-02-1.05741225e-01j,  2.10939124e-02-2.54678586e-03j,\n",
      "         6.12901598e-02+2.84539282e-01j,  1.91698879e-01+2.75423944e-01j,\n",
      "        -5.37234508e-02+4.94065508e-02j,  4.68861437e+00-6.97277561e-02j,\n",
      "         1.88950926e-01-3.14086676e-01j, -2.56929696e-02-9.12510138e-03j,\n",
      "        -7.24915490e-02+2.37303257e-01j],\n",
      "       [-5.19249380e-01-8.25218931e-02j,  4.53692585e-01-6.55159131e-02j,\n",
      "         1.70892984e-01-2.90004879e-01j,  1.45027399e-01-1.58730075e-01j,\n",
      "        -1.79294348e-01+1.14760011e-01j, -3.70380431e-02-7.66561106e-02j,\n",
      "        -1.95335552e-01+1.75296322e-01j,  9.01950300e-02+2.10906975e-02j,\n",
      "         9.28890929e-02+1.45167699e-02j,  3.77069503e-01-2.98564043e-02j,\n",
      "         7.31683522e-02+8.59315693e-02j,  1.53700531e-01+3.36694151e-01j,\n",
      "         4.57327032e+00+1.19299643e-01j, -1.22024298e-01+1.21267781e-01j,\n",
      "         8.43100473e-02-1.91762120e-01j],\n",
      "       [-5.13286963e-02+1.19752854e-01j, -4.29707654e-02-1.45511866e-01j,\n",
      "        -4.25873190e-01+3.33982483e-02j, -5.83635420e-02+2.12984264e-01j,\n",
      "         3.29427958e-01-1.09983608e-01j,  7.29476437e-02-1.16220005e-02j,\n",
      "         3.54535788e-01+2.12048709e-01j, -2.34260764e-02-6.92817569e-02j,\n",
      "         1.44652680e-01-1.01296343e-01j, -1.73711032e-01+5.02539426e-02j,\n",
      "        -2.53031403e-02-4.64386865e-02j,  6.10113889e-02+8.44140574e-02j,\n",
      "        -7.20824208e-03-5.15585840e-02j,  4.63824749e+00-2.41747703e-02j,\n",
      "         1.78209066e-01-2.16658525e-02j],\n",
      "       [-9.55702737e-02+2.83877194e-01j, -2.95165628e-01-2.67776310e-01j,\n",
      "         3.08804125e-01-5.71958303e-01j, -2.65512139e-01-2.44255111e-01j,\n",
      "        -2.29474574e-01+4.49274331e-02j, -2.87705183e-01-1.03206253e-02j,\n",
      "         1.73533857e-01+5.55275679e-01j, -7.68620819e-02-1.07333794e-01j,\n",
      "         2.17798963e-01-2.64715731e-01j,  1.73822373e-01+1.34617284e-01j,\n",
      "        -1.17425717e-01+1.60592675e-01j,  1.03107719e-02-3.18308085e-01j,\n",
      "         2.97945172e-01+3.53375167e-01j,  6.63591251e-02+9.30496529e-02j,\n",
      "         4.46437311e+00+1.52958790e-02j]], dtype=complex64)>\n"
     ]
    }
   ],
   "source": [
    "for weight in model.trainable_weights:\n",
    "    print(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
