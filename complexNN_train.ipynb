{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hypersurface_tf import *\n",
    "from generate_h import *\n",
    "from complexNN import *\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "z0, z1, z2, z3, z4 = sp.symbols('z0, z1, z2, z3, z4')\n",
    "Z = [z0,z1,z2,z3,z4]\n",
    "f = z0**5 + z1**5 + z2**5 + z3**5 + z4**5 + 0.5*z0*z1*z2*z3*z4\n",
    "HS = Hypersurface(Z, f, 100000)\n",
    "HS_test = Hypersurface(Z, f, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7eff9015dea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7eff9b95c0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7eff9b95cf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7eff9b95cbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7eff9b8c1ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7eff901278c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7eff90127ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7eff900b0d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7eff3c6df268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7eff3c6dfea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7eff3c66b9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7eff3c6dfb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7eff3c777400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7eff3c605378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7eff3c605158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7eff3c631048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7eff3c5846a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7eff3c586b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7eff3c5868c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7eff3c586d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7eff3c6fae18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7eff4415dd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7eff3c5188c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7eff3c51cf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7eff9002fd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7eff9002fbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7eff3c56fc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7eff440eae18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7eff440ee620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7eff440ee840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7eff440ee510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7eff3c7a0950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7eff3c7a0510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7eff3c6aed08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7eff3c7a0598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7eff3c4e69d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "train_set = generate_dataset(HS)\n",
    "test_set = generate_dataset(HS_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = train_set.shuffle(500000).batch(1000)\n",
    "test_set = test_set.shuffle(50000).batch(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KahlerPotential(tf.keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(KahlerPotential, self).__init__()\n",
    "        self.layer_1 = ComplexDense(5, 60, activation=tf.square)\n",
    "        self.layer_2 = ComplexDense(60, 60, activation=tf.square)\n",
    "\n",
    "        #self.layer_4 = ComplexDense(50, 10, activation=tf.square)\n",
    "        #self.layer_3 = ComplexDense(10, 15, activation=tf.square)\n",
    "        self.g = ComplexG(60)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.layer_1(inputs)\n",
    "        x = self.layer_2(x)\n",
    "\n",
    "        #x = self.layer_4(x)\n",
    "        x = self.g(x)\n",
    "        x = tf.linalg.diag_part(tf.matmul(x, x, adjoint_b=True))\n",
    "        x = tf.math.log(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KahlerPotential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def volume_form(x, Omega_Omegabar, mass, restriction):\n",
    "\n",
    "    kahler_metric = complex_hessian(tf.math.real(model(x)), x)\n",
    "    volume_form = tf.linalg.det(tf.matmul(restriction, tf.matmul(kahler_metric, restriction, adjoint_b=True)))\n",
    "    weights = mass / tf.reduce_sum(mass)\n",
    "    factor = tf.reduce_sum(weights * volume_form / Omega_Omegabar)\n",
    "    #factor = tf.constant(4.380538, dtype=tf.complex64)\n",
    "    return volume_form / factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spencer/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:978: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  return float(self._numpy())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: loss = 0.7650\n",
      "step 50: loss = 0.5645\n",
      "step 100: loss = 0.4075\n",
      "step 150: loss = 0.3327\n",
      "step 200: loss = 0.3220\n",
      "step 250: loss = 0.2967\n",
      "step 300: loss = 0.2833\n",
      "step 350: loss = 0.2539\n",
      "step 400: loss = 0.2477\n",
      "step 450: loss = 0.2407\n",
      "test_loss: 0.23475555419921876\n",
      "step 0: loss = 0.2331\n",
      "step 50: loss = 0.2230\n",
      "step 100: loss = 0.2239\n",
      "step 150: loss = 0.2119\n",
      "step 200: loss = 0.2088\n",
      "step 250: loss = 0.2113\n",
      "step 300: loss = 0.1965\n",
      "step 350: loss = 0.2063\n",
      "step 400: loss = 0.1881\n",
      "step 450: loss = 0.1842\n",
      "test_loss: 0.17310842514038086\n",
      "step 0: loss = 0.1778\n",
      "step 50: loss = 0.1715\n",
      "step 100: loss = 0.1797\n",
      "step 150: loss = 0.1687\n",
      "step 200: loss = 0.1729\n",
      "step 250: loss = 0.1554\n",
      "step 300: loss = 0.1652\n",
      "step 350: loss = 0.1647\n",
      "step 400: loss = 0.1803\n",
      "step 450: loss = 0.1492\n",
      "test_loss: 0.14740344047546386\n",
      "step 0: loss = 0.1483\n",
      "step 50: loss = 0.1568\n",
      "step 100: loss = 0.1408\n",
      "step 150: loss = 0.1389\n",
      "step 200: loss = 0.1481\n",
      "step 250: loss = 0.1390\n",
      "step 300: loss = 0.1373\n",
      "step 350: loss = 0.1312\n",
      "step 400: loss = 0.1329\n",
      "step 450: loss = 0.1374\n",
      "test_loss: 0.13092284202575682\n",
      "step 0: loss = 0.1290\n",
      "step 50: loss = 0.1287\n",
      "step 100: loss = 0.1253\n",
      "step 150: loss = 0.1266\n",
      "step 200: loss = 0.1234\n",
      "step 250: loss = 0.1295\n",
      "step 300: loss = 0.1273\n",
      "step 350: loss = 0.1262\n",
      "step 400: loss = 0.1202\n",
      "step 450: loss = 0.1203\n",
      "test_loss: 0.12138251304626464\n",
      "step 0: loss = 0.1253\n",
      "step 50: loss = 0.1214\n",
      "step 100: loss = 0.1258\n",
      "step 150: loss = 0.1178\n",
      "step 200: loss = 0.1080\n",
      "step 250: loss = 0.1191\n",
      "step 300: loss = 0.1199\n",
      "step 350: loss = 0.1152\n",
      "step 400: loss = 0.1212\n",
      "step 450: loss = 0.1082\n",
      "test_loss: 0.11217638015747071\n",
      "step 0: loss = 0.1143\n",
      "step 50: loss = 0.1056\n",
      "step 100: loss = 0.1103\n",
      "step 150: loss = 0.1070\n",
      "step 200: loss = 0.1155\n",
      "step 250: loss = 0.1080\n",
      "step 300: loss = 0.1035\n",
      "step 350: loss = 0.1029\n",
      "step 400: loss = 0.1047\n",
      "step 450: loss = 0.1048\n",
      "test_loss: 0.10210725784301758\n",
      "step 0: loss = 0.1030\n",
      "step 50: loss = 0.1073\n",
      "step 100: loss = 0.1050\n",
      "step 150: loss = 0.1036\n",
      "step 200: loss = 0.1106\n",
      "step 250: loss = 0.1039\n",
      "step 300: loss = 0.1008\n",
      "step 350: loss = 0.1011\n",
      "step 400: loss = 0.1023\n",
      "step 450: loss = 0.0965\n",
      "test_loss: 0.10391731262207031\n",
      "step 0: loss = 0.1022\n",
      "step 50: loss = 0.0933\n",
      "step 100: loss = 0.0993\n",
      "step 150: loss = 0.0957\n",
      "step 200: loss = 0.0996\n",
      "step 250: loss = 0.0962\n",
      "step 300: loss = 0.0975\n",
      "step 350: loss = 0.0975\n",
      "step 400: loss = 0.0911\n",
      "step 450: loss = 0.0971\n",
      "test_loss: 0.09460752487182617\n",
      "step 0: loss = 0.0902\n",
      "step 50: loss = 0.0923\n",
      "step 100: loss = 0.0954\n",
      "step 150: loss = 0.0883\n",
      "step 200: loss = 0.1027\n",
      "step 250: loss = 0.0930\n",
      "step 300: loss = 0.0906\n",
      "step 350: loss = 0.0920\n",
      "step 400: loss = 0.0931\n",
      "step 450: loss = 0.0920\n",
      "test_loss: 0.0934727954864502\n",
      "step 0: loss = 0.0929\n",
      "step 50: loss = 0.0970\n",
      "step 100: loss = 0.0933\n",
      "step 150: loss = 0.0918\n",
      "step 200: loss = 0.0904\n",
      "step 250: loss = 0.0900\n",
      "step 300: loss = 0.0856\n",
      "step 350: loss = 0.0898\n",
      "step 400: loss = 0.0892\n",
      "step 450: loss = 0.0869\n",
      "test_loss: 0.0901924991607666\n",
      "step 0: loss = 0.0897\n",
      "step 50: loss = 0.0809\n",
      "step 100: loss = 0.0950\n",
      "step 150: loss = 0.0864\n",
      "step 200: loss = 0.0929\n",
      "step 250: loss = 0.0871\n",
      "step 300: loss = 0.0869\n",
      "step 350: loss = 0.0863\n",
      "step 400: loss = 0.0872\n",
      "step 450: loss = 0.0852\n",
      "test_loss: 0.08749539375305176\n",
      "step 0: loss = 0.0864\n",
      "step 50: loss = 0.0889\n",
      "step 100: loss = 0.0917\n",
      "step 150: loss = 0.0879\n",
      "step 200: loss = 0.0886\n",
      "step 250: loss = 0.0879\n",
      "step 300: loss = 0.0832\n",
      "step 350: loss = 0.0845\n",
      "step 400: loss = 0.0875\n",
      "step 450: loss = 0.0853\n",
      "test_loss: 0.08462389945983886\n",
      "step 0: loss = 0.0817\n",
      "step 50: loss = 0.0841\n",
      "step 100: loss = 0.0866\n",
      "step 150: loss = 0.0908\n",
      "step 200: loss = 0.0864\n",
      "step 250: loss = 0.0841\n",
      "step 300: loss = 0.0800\n",
      "step 350: loss = 0.0831\n",
      "step 400: loss = 0.0802\n",
      "step 450: loss = 0.0802\n",
      "test_loss: 0.08499712944030761\n",
      "step 0: loss = 0.0823\n",
      "step 50: loss = 0.0786\n",
      "step 100: loss = 0.0805\n",
      "step 150: loss = 0.0780\n",
      "step 200: loss = 0.0780\n",
      "step 250: loss = 0.0868\n",
      "step 300: loss = 0.0815\n",
      "step 350: loss = 0.0784\n",
      "step 400: loss = 0.0824\n",
      "step 450: loss = 0.0833\n",
      "test_loss: 0.0803773307800293\n",
      "step 0: loss = 0.0768\n",
      "step 50: loss = 0.0759\n",
      "step 100: loss = 0.0795\n",
      "step 150: loss = 0.0765\n",
      "step 200: loss = 0.0843\n",
      "step 250: loss = 0.0801\n",
      "step 300: loss = 0.0779\n",
      "step 350: loss = 0.0817\n",
      "step 400: loss = 0.0780\n",
      "step 450: loss = 0.0774\n",
      "test_loss: 0.07842941284179687\n",
      "step 0: loss = 0.0794\n",
      "step 50: loss = 0.0782\n",
      "step 100: loss = 0.0794\n",
      "step 150: loss = 0.0792\n",
      "step 200: loss = 0.0747\n",
      "step 250: loss = 0.0781\n",
      "step 300: loss = 0.0756\n",
      "step 350: loss = 0.0775\n",
      "step 400: loss = 0.0788\n",
      "step 450: loss = 0.0756\n",
      "test_loss: 0.07574438571929931\n",
      "step 0: loss = 0.0754\n",
      "step 50: loss = 0.0770\n",
      "step 100: loss = 0.0762\n",
      "step 150: loss = 0.0729\n",
      "step 200: loss = 0.0771\n",
      "step 250: loss = 0.0815\n",
      "step 300: loss = 0.0763\n",
      "step 350: loss = 0.0787\n",
      "step 400: loss = 0.0793\n",
      "step 450: loss = 0.0728\n",
      "test_loss: 0.0754257583618164\n",
      "step 0: loss = 0.0757\n",
      "step 50: loss = 0.0754\n",
      "step 100: loss = 0.0695\n",
      "step 150: loss = 0.0770\n",
      "step 200: loss = 0.0765\n",
      "step 250: loss = 0.0778\n",
      "step 300: loss = 0.0724\n",
      "step 350: loss = 0.0741\n",
      "step 400: loss = 0.0738\n",
      "step 450: loss = 0.0742\n",
      "test_loss: 0.07338563442230224\n",
      "step 0: loss = 0.0768\n",
      "step 50: loss = 0.0732\n",
      "step 100: loss = 0.0749\n",
      "step 150: loss = 0.0783\n",
      "step 200: loss = 0.0685\n",
      "step 250: loss = 0.0750\n",
      "step 300: loss = 0.0725\n",
      "step 350: loss = 0.0730\n",
      "step 400: loss = 0.0722\n",
      "step 450: loss = 0.0728\n",
      "test_loss: 0.07480466365814209\n",
      "step 0: loss = 0.0769\n",
      "step 50: loss = 0.0760\n",
      "step 100: loss = 0.0690\n",
      "step 150: loss = 0.0737\n",
      "step 200: loss = 0.0767\n",
      "step 250: loss = 0.0732\n",
      "step 300: loss = 0.0706\n",
      "step 350: loss = 0.0744\n",
      "step 400: loss = 0.0747\n",
      "step 450: loss = 0.0737\n",
      "test_loss: 0.07312767505645752\n",
      "step 0: loss = 0.0731\n",
      "step 50: loss = 0.0720\n",
      "step 100: loss = 0.0734\n",
      "step 150: loss = 0.0734\n",
      "step 200: loss = 0.0688\n",
      "step 250: loss = 0.0696\n",
      "step 300: loss = 0.0725\n",
      "step 350: loss = 0.0692\n",
      "step 400: loss = 0.0726\n",
      "step 450: loss = 0.0732\n",
      "test_loss: 0.07164854049682617\n",
      "step 0: loss = 0.0728\n",
      "step 50: loss = 0.0712\n",
      "step 100: loss = 0.0734\n",
      "step 150: loss = 0.0708\n",
      "step 200: loss = 0.0704\n",
      "step 250: loss = 0.0733\n",
      "step 300: loss = 0.0722\n",
      "step 350: loss = 0.0722\n",
      "step 400: loss = 0.0714\n",
      "step 450: loss = 0.0729\n",
      "test_loss: 0.07141798019409179\n",
      "step 0: loss = 0.0695\n",
      "step 50: loss = 0.0715\n",
      "step 100: loss = 0.0710\n",
      "step 150: loss = 0.0699\n",
      "step 200: loss = 0.0733\n",
      "step 250: loss = 0.0689\n",
      "step 300: loss = 0.0686\n",
      "step 350: loss = 0.0685\n",
      "step 400: loss = 0.0688\n",
      "step 450: loss = 0.0742\n",
      "test_loss: 0.07006401538848878\n",
      "step 0: loss = 0.0674\n",
      "step 50: loss = 0.0704\n",
      "step 100: loss = 0.0702\n",
      "step 150: loss = 0.0755\n",
      "step 200: loss = 0.0680\n",
      "step 250: loss = 0.0694\n",
      "step 300: loss = 0.0710\n",
      "step 350: loss = 0.0677\n",
      "step 400: loss = 0.0697\n",
      "step 450: loss = 0.0697\n",
      "test_loss: 0.07052552223205566\n",
      "step 0: loss = 0.0671\n",
      "step 50: loss = 0.0684\n",
      "step 100: loss = 0.0696\n",
      "step 150: loss = 0.0673\n",
      "step 200: loss = 0.0690\n",
      "step 250: loss = 0.0725\n",
      "step 300: loss = 0.0672\n",
      "step 350: loss = 0.0649\n",
      "step 400: loss = 0.0708\n",
      "step 450: loss = 0.0716\n",
      "test_loss: 0.0700593090057373\n",
      "step 0: loss = 0.0681\n",
      "step 50: loss = 0.0691\n",
      "step 100: loss = 0.0709\n",
      "step 150: loss = 0.0731\n",
      "step 200: loss = 0.0698\n",
      "step 250: loss = 0.0673\n",
      "step 300: loss = 0.0689\n",
      "step 350: loss = 0.0696\n",
      "step 400: loss = 0.0698\n",
      "step 450: loss = 0.0670\n",
      "test_loss: 0.06859391212463378\n",
      "step 0: loss = 0.0675\n",
      "step 50: loss = 0.0675\n",
      "step 100: loss = 0.0705\n",
      "step 150: loss = 0.0668\n",
      "step 200: loss = 0.0663\n",
      "step 250: loss = 0.0676\n",
      "step 300: loss = 0.0685\n",
      "step 350: loss = 0.0684\n",
      "step 400: loss = 0.0669\n",
      "step 450: loss = 0.0663\n",
      "test_loss: 0.06735620975494384\n",
      "step 0: loss = 0.0646\n",
      "step 50: loss = 0.0701\n",
      "step 100: loss = 0.0648\n",
      "step 150: loss = 0.0713\n",
      "step 200: loss = 0.0676\n",
      "step 250: loss = 0.0682\n",
      "step 300: loss = 0.0691\n",
      "step 350: loss = 0.0686\n",
      "step 400: loss = 0.0676\n",
      "step 450: loss = 0.0677\n",
      "test_loss: 0.06746560573577881\n",
      "step 0: loss = 0.0647\n",
      "step 50: loss = 0.0678\n",
      "step 100: loss = 0.0692\n",
      "step 150: loss = 0.0688\n",
      "step 200: loss = 0.0692\n",
      "step 250: loss = 0.0698\n",
      "step 300: loss = 0.0660\n",
      "step 350: loss = 0.0677\n",
      "step 400: loss = 0.0641\n",
      "step 450: loss = 0.0650\n",
      "test_loss: 0.06735413074493408\n",
      "step 0: loss = 0.0660\n",
      "step 50: loss = 0.0689\n",
      "step 100: loss = 0.0659\n",
      "step 150: loss = 0.0649\n",
      "step 200: loss = 0.0658\n",
      "step 250: loss = 0.0692\n",
      "step 300: loss = 0.0643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350: loss = 0.0632\n",
      "step 400: loss = 0.0680\n",
      "step 450: loss = 0.0671\n",
      "test_loss: 0.06661455631256104\n",
      "step 0: loss = 0.0673\n",
      "step 50: loss = 0.0636\n",
      "step 100: loss = 0.0685\n",
      "step 150: loss = 0.0680\n",
      "step 200: loss = 0.0668\n",
      "step 250: loss = 0.0697\n",
      "step 300: loss = 0.0669\n",
      "step 350: loss = 0.0665\n",
      "step 400: loss = 0.0640\n",
      "step 450: loss = 0.0693\n",
      "test_loss: 0.06656136512756347\n",
      "step 0: loss = 0.0669\n",
      "step 50: loss = 0.0613\n",
      "step 100: loss = 0.0649\n",
      "step 150: loss = 0.0683\n",
      "step 200: loss = 0.0644\n",
      "step 250: loss = 0.0674\n",
      "step 300: loss = 0.0672\n",
      "step 350: loss = 0.0670\n",
      "step 400: loss = 0.0669\n",
      "step 450: loss = 0.0650\n",
      "test_loss: 0.06557498455047607\n",
      "step 0: loss = 0.0660\n",
      "step 50: loss = 0.0644\n",
      "step 100: loss = 0.0664\n",
      "step 150: loss = 0.0655\n",
      "step 200: loss = 0.0675\n",
      "step 250: loss = 0.0640\n",
      "step 300: loss = 0.0643\n",
      "step 350: loss = 0.0649\n",
      "step 400: loss = 0.0617\n",
      "step 450: loss = 0.0662\n",
      "test_loss: 0.06756846904754639\n",
      "step 0: loss = 0.0650\n",
      "step 50: loss = 0.0672\n",
      "step 100: loss = 0.0649\n",
      "step 150: loss = 0.0665\n",
      "step 200: loss = 0.0687\n",
      "step 250: loss = 0.0620\n",
      "step 300: loss = 0.0630\n",
      "step 350: loss = 0.0671\n",
      "step 400: loss = 0.0644\n",
      "step 450: loss = 0.0661\n",
      "test_loss: 0.06531773090362548\n",
      "step 0: loss = 0.0639\n",
      "step 50: loss = 0.0637\n",
      "step 100: loss = 0.0645\n",
      "step 150: loss = 0.0665\n",
      "step 200: loss = 0.0669\n",
      "step 250: loss = 0.0631\n",
      "step 300: loss = 0.0634\n",
      "step 350: loss = 0.0651\n",
      "step 400: loss = 0.0655\n",
      "step 450: loss = 0.0672\n",
      "test_loss: 0.0651792812347412\n",
      "step 0: loss = 0.0656\n",
      "step 50: loss = 0.0658\n",
      "step 100: loss = 0.0627\n",
      "step 150: loss = 0.0662\n",
      "step 200: loss = 0.0629\n",
      "step 250: loss = 0.0643\n",
      "step 300: loss = 0.0659\n",
      "step 350: loss = 0.0666\n",
      "step 400: loss = 0.0638\n",
      "step 450: loss = 0.0680\n",
      "test_loss: 0.06421692371368408\n",
      "step 0: loss = 0.0616\n",
      "step 50: loss = 0.0615\n",
      "step 100: loss = 0.0681\n",
      "step 150: loss = 0.0608\n",
      "step 200: loss = 0.0623\n",
      "step 250: loss = 0.0612\n",
      "step 300: loss = 0.0637\n",
      "step 350: loss = 0.0629\n",
      "step 400: loss = 0.0665\n",
      "step 450: loss = 0.0641\n",
      "test_loss: 0.06337998867034912\n",
      "step 0: loss = 0.0619\n",
      "step 50: loss = 0.0637\n",
      "step 100: loss = 0.0610\n",
      "step 150: loss = 0.0644\n",
      "step 200: loss = 0.0611\n",
      "step 250: loss = 0.0660\n",
      "step 300: loss = 0.0647\n",
      "step 350: loss = 0.0612\n",
      "step 400: loss = 0.0636\n",
      "step 450: loss = 0.0657\n",
      "test_loss: 0.06388888835906982\n",
      "step 0: loss = 0.0610\n",
      "step 50: loss = 0.0627\n",
      "step 100: loss = 0.0686\n",
      "step 150: loss = 0.0653\n",
      "step 200: loss = 0.0625\n",
      "step 250: loss = 0.0647\n",
      "step 300: loss = 0.0645\n",
      "step 350: loss = 0.0596\n",
      "step 400: loss = 0.0610\n",
      "step 450: loss = 0.0629\n",
      "test_loss: 0.06240968704223633\n",
      "step 0: loss = 0.0607\n",
      "step 50: loss = 0.0625\n",
      "step 100: loss = 0.0612\n",
      "step 150: loss = 0.0597\n",
      "step 200: loss = 0.0632\n",
      "step 250: loss = 0.0619\n",
      "step 300: loss = 0.0648\n",
      "step 350: loss = 0.0612\n",
      "step 400: loss = 0.0616\n",
      "step 450: loss = 0.0651\n",
      "test_loss: 0.06310301303863525\n",
      "step 0: loss = 0.0645\n",
      "step 50: loss = 0.0596\n",
      "step 100: loss = 0.0622\n",
      "step 150: loss = 0.0652\n",
      "step 200: loss = 0.0643\n",
      "step 250: loss = 0.0631\n",
      "step 300: loss = 0.0575\n",
      "step 350: loss = 0.0632\n",
      "step 400: loss = 0.0598\n",
      "step 450: loss = 0.0646\n",
      "test_loss: 0.062289643287658694\n",
      "step 0: loss = 0.0604\n",
      "step 50: loss = 0.0617\n",
      "step 100: loss = 0.0648\n",
      "step 150: loss = 0.0629\n",
      "step 200: loss = 0.0597\n",
      "step 250: loss = 0.0602\n",
      "step 300: loss = 0.0574\n",
      "step 350: loss = 0.0608\n",
      "step 400: loss = 0.0632\n",
      "step 450: loss = 0.0603\n",
      "test_loss: 0.06126264095306397\n",
      "step 0: loss = 0.0583\n",
      "step 50: loss = 0.0626\n",
      "step 100: loss = 0.0587\n",
      "step 150: loss = 0.0622\n",
      "step 200: loss = 0.0622\n",
      "step 250: loss = 0.0574\n",
      "step 300: loss = 0.0632\n",
      "step 350: loss = 0.0613\n",
      "step 400: loss = 0.0610\n",
      "step 450: loss = 0.0620\n",
      "test_loss: 0.06192967414855957\n",
      "step 0: loss = 0.0611\n",
      "step 50: loss = 0.0622\n",
      "step 100: loss = 0.0614\n",
      "step 150: loss = 0.0610\n",
      "step 200: loss = 0.0599\n",
      "step 250: loss = 0.0618\n",
      "step 300: loss = 0.0601\n",
      "step 350: loss = 0.0617\n",
      "step 400: loss = 0.0604\n",
      "step 450: loss = 0.0604\n",
      "test_loss: 0.06004359245300293\n",
      "step 0: loss = 0.0594\n",
      "step 50: loss = 0.0611\n",
      "step 100: loss = 0.0600\n",
      "step 150: loss = 0.0583\n",
      "step 200: loss = 0.0616\n",
      "step 250: loss = 0.0610\n",
      "step 300: loss = 0.0604\n",
      "step 350: loss = 0.0584\n",
      "step 400: loss = 0.0593\n",
      "step 450: loss = 0.0605\n",
      "test_loss: 0.060744194984436034\n",
      "step 0: loss = 0.0599\n",
      "step 50: loss = 0.0593\n",
      "step 100: loss = 0.0595\n",
      "step 150: loss = 0.0573\n",
      "step 200: loss = 0.0577\n",
      "step 250: loss = 0.0587\n",
      "step 300: loss = 0.0607\n",
      "step 350: loss = 0.0594\n",
      "step 400: loss = 0.0561\n",
      "step 450: loss = 0.0590\n",
      "test_loss: 0.06030137062072754\n",
      "step 0: loss = 0.0618\n",
      "step 50: loss = 0.0565\n",
      "step 100: loss = 0.0616\n",
      "step 150: loss = 0.0580\n",
      "step 200: loss = 0.0602\n",
      "step 250: loss = 0.0597\n",
      "step 300: loss = 0.0587\n",
      "step 350: loss = 0.0591\n",
      "step 400: loss = 0.0613\n",
      "step 450: loss = 0.0591\n",
      "test_loss: 0.059893712997436524\n",
      "step 0: loss = 0.0581\n",
      "step 50: loss = 0.0601\n",
      "step 100: loss = 0.0632\n",
      "step 150: loss = 0.0620\n",
      "step 200: loss = 0.0594\n",
      "step 250: loss = 0.0584\n",
      "step 300: loss = 0.0585\n",
      "step 350: loss = 0.0612\n",
      "step 400: loss = 0.0604\n",
      "step 450: loss = 0.0593\n",
      "test_loss: 0.05881836414337158\n",
      "step 0: loss = 0.0585\n",
      "step 50: loss = 0.0596\n",
      "step 100: loss = 0.0583\n",
      "step 150: loss = 0.0557\n",
      "step 200: loss = 0.0612\n",
      "step 250: loss = 0.0618\n",
      "step 300: loss = 0.0608\n",
      "step 350: loss = 0.0594\n",
      "step 400: loss = 0.0619\n",
      "step 450: loss = 0.0565\n",
      "test_loss: 0.05803184986114502\n",
      "step 0: loss = 0.0565\n",
      "step 50: loss = 0.0592\n",
      "step 100: loss = 0.0596\n",
      "step 150: loss = 0.0598\n",
      "step 200: loss = 0.0557\n",
      "step 250: loss = 0.0580\n",
      "step 300: loss = 0.0586\n",
      "step 350: loss = 0.0598\n",
      "step 400: loss = 0.0577\n",
      "step 450: loss = 0.0575\n",
      "test_loss: 0.05898810863494873\n",
      "step 0: loss = 0.0576\n",
      "step 50: loss = 0.0598\n",
      "step 100: loss = 0.0600\n",
      "step 150: loss = 0.0578\n",
      "step 200: loss = 0.0580\n",
      "step 250: loss = 0.0566\n",
      "step 300: loss = 0.0589\n",
      "step 350: loss = 0.0602\n",
      "step 400: loss = 0.0548\n",
      "step 450: loss = 0.0565\n",
      "test_loss: 0.05779081344604492\n",
      "step 0: loss = 0.0552\n",
      "step 50: loss = 0.0585\n",
      "step 100: loss = 0.0556\n",
      "step 150: loss = 0.0560\n",
      "step 200: loss = 0.0568\n",
      "step 250: loss = 0.0570\n",
      "step 300: loss = 0.0600\n",
      "step 350: loss = 0.0561\n",
      "step 400: loss = 0.0592\n",
      "step 450: loss = 0.0607\n",
      "test_loss: 0.05882920265197754\n",
      "step 0: loss = 0.0609\n",
      "step 50: loss = 0.0538\n",
      "step 100: loss = 0.0566\n",
      "step 150: loss = 0.0567\n",
      "step 200: loss = 0.0592\n",
      "step 250: loss = 0.0558\n",
      "step 300: loss = 0.0574\n",
      "step 350: loss = 0.0609\n",
      "step 400: loss = 0.0576\n",
      "step 450: loss = 0.0562\n",
      "test_loss: 0.05723950862884521\n",
      "step 0: loss = 0.0564\n",
      "step 50: loss = 0.0587\n",
      "step 100: loss = 0.0550\n",
      "step 150: loss = 0.0563\n",
      "step 200: loss = 0.0575\n",
      "step 250: loss = 0.0546\n",
      "step 300: loss = 0.0549\n",
      "step 350: loss = 0.0562\n",
      "step 400: loss = 0.0565\n",
      "step 450: loss = 0.0546\n",
      "test_loss: 0.05722249984741211\n",
      "step 0: loss = 0.0595\n",
      "step 50: loss = 0.0607\n",
      "step 100: loss = 0.0562\n",
      "step 150: loss = 0.0583\n",
      "step 200: loss = 0.0549\n",
      "step 250: loss = 0.0553\n",
      "step 300: loss = 0.0569\n",
      "step 350: loss = 0.0585\n",
      "step 400: loss = 0.0578\n",
      "step 450: loss = 0.0562\n",
      "test_loss: 0.056571731567382814\n",
      "step 0: loss = 0.0534\n",
      "step 50: loss = 0.0568\n",
      "step 100: loss = 0.0562\n",
      "step 150: loss = 0.0525\n",
      "step 200: loss = 0.0577\n",
      "step 250: loss = 0.0559\n",
      "step 300: loss = 0.0583\n",
      "step 350: loss = 0.0580\n",
      "step 400: loss = 0.0570\n",
      "step 450: loss = 0.0578\n",
      "test_loss: 0.05696206092834473\n",
      "step 0: loss = 0.0551\n",
      "step 50: loss = 0.0565\n",
      "step 100: loss = 0.0541\n",
      "step 150: loss = 0.0590\n",
      "step 200: loss = 0.0548\n",
      "step 250: loss = 0.0553\n",
      "step 300: loss = 0.0593\n",
      "step 350: loss = 0.0606\n",
      "step 400: loss = 0.0558\n",
      "step 450: loss = 0.0567\n",
      "test_loss: 0.056979012489318845\n",
      "step 0: loss = 0.0563\n",
      "step 50: loss = 0.0559\n",
      "step 100: loss = 0.0539\n",
      "step 150: loss = 0.0540\n",
      "step 200: loss = 0.0556\n",
      "step 250: loss = 0.0578\n",
      "step 300: loss = 0.0565\n",
      "step 350: loss = 0.0563\n",
      "step 400: loss = 0.0554\n",
      "step 450: loss = 0.0547\n",
      "test_loss: 0.05573381423950195\n",
      "step 0: loss = 0.0559\n",
      "step 50: loss = 0.0573\n",
      "step 100: loss = 0.0528\n",
      "step 150: loss = 0.0544\n",
      "step 200: loss = 0.0526\n",
      "step 250: loss = 0.0558\n",
      "step 300: loss = 0.0538\n",
      "step 350: loss = 0.0563\n",
      "step 400: loss = 0.0553\n",
      "step 450: loss = 0.0571\n",
      "test_loss: 0.05590495109558105\n",
      "step 0: loss = 0.0551\n",
      "step 50: loss = 0.0550\n",
      "step 100: loss = 0.0573\n",
      "step 150: loss = 0.0544\n",
      "step 200: loss = 0.0556\n",
      "step 250: loss = 0.0549\n",
      "step 300: loss = 0.0580\n",
      "step 350: loss = 0.0543\n",
      "step 400: loss = 0.0557\n",
      "step 450: loss = 0.0564\n",
      "test_loss: 0.055601367950439455\n",
      "step 0: loss = 0.0539\n",
      "step 50: loss = 0.0566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 100: loss = 0.0535\n",
      "step 150: loss = 0.0549\n",
      "step 200: loss = 0.0567\n",
      "step 250: loss = 0.0565\n",
      "step 300: loss = 0.0544\n",
      "step 350: loss = 0.0558\n",
      "step 400: loss = 0.0568\n",
      "step 450: loss = 0.0586\n",
      "test_loss: 0.05580528736114502\n",
      "step 0: loss = 0.0530\n",
      "step 50: loss = 0.0581\n",
      "step 100: loss = 0.0575\n",
      "step 150: loss = 0.0582\n",
      "step 200: loss = 0.0546\n",
      "step 250: loss = 0.0532\n",
      "step 300: loss = 0.0561\n",
      "step 350: loss = 0.0562\n",
      "step 400: loss = 0.0551\n",
      "step 450: loss = 0.0560\n",
      "test_loss: 0.05457709789276123\n",
      "step 0: loss = 0.0545\n",
      "step 50: loss = 0.0523\n",
      "step 100: loss = 0.0513\n",
      "step 150: loss = 0.0582\n",
      "step 200: loss = 0.0575\n",
      "step 250: loss = 0.0563\n",
      "step 300: loss = 0.0540\n",
      "step 350: loss = 0.0543\n",
      "step 400: loss = 0.0548\n",
      "step 450: loss = 0.0563\n",
      "test_loss: 0.05488643646240234\n",
      "step 0: loss = 0.0548\n",
      "step 50: loss = 0.0550\n",
      "step 100: loss = 0.0577\n",
      "step 150: loss = 0.0543\n",
      "step 200: loss = 0.0529\n",
      "step 250: loss = 0.0544\n",
      "step 300: loss = 0.0549\n",
      "step 350: loss = 0.0547\n",
      "step 400: loss = 0.0560\n",
      "step 450: loss = 0.0549\n",
      "test_loss: 0.05455422401428223\n",
      "step 0: loss = 0.0583\n",
      "step 50: loss = 0.0577\n",
      "step 100: loss = 0.0550\n",
      "step 150: loss = 0.0530\n",
      "step 200: loss = 0.0531\n",
      "step 250: loss = 0.0543\n",
      "step 300: loss = 0.0563\n",
      "step 350: loss = 0.0540\n",
      "step 400: loss = 0.0565\n",
      "step 450: loss = 0.0566\n",
      "test_loss: 0.0541496753692627\n",
      "step 0: loss = 0.0554\n",
      "step 50: loss = 0.0557\n",
      "step 100: loss = 0.0540\n",
      "step 150: loss = 0.0524\n",
      "step 200: loss = 0.0524\n",
      "step 250: loss = 0.0534\n",
      "step 300: loss = 0.0528\n",
      "step 350: loss = 0.0553\n",
      "step 400: loss = 0.0529\n",
      "step 450: loss = 0.0561\n",
      "test_loss: 0.054571161270141604\n",
      "step 0: loss = 0.0535\n",
      "step 50: loss = 0.0531\n",
      "step 100: loss = 0.0532\n",
      "step 150: loss = 0.0563\n",
      "step 200: loss = 0.0555\n",
      "step 250: loss = 0.0564\n",
      "step 300: loss = 0.0511\n",
      "step 350: loss = 0.0557\n",
      "step 400: loss = 0.0553\n",
      "step 450: loss = 0.0547\n",
      "test_loss: 0.055065784454345706\n",
      "step 0: loss = 0.0546\n",
      "step 50: loss = 0.0519\n",
      "step 100: loss = 0.0547\n",
      "step 150: loss = 0.0543\n",
      "step 200: loss = 0.0552\n",
      "step 250: loss = 0.0557\n",
      "step 300: loss = 0.0537\n",
      "step 350: loss = 0.0569\n",
      "step 400: loss = 0.0546\n",
      "step 450: loss = 0.0551\n",
      "test_loss: 0.05447952747344971\n",
      "step 0: loss = 0.0529\n",
      "step 50: loss = 0.0570\n",
      "step 100: loss = 0.0524\n",
      "step 150: loss = 0.0570\n",
      "step 200: loss = 0.0544\n",
      "step 250: loss = 0.0534\n",
      "step 300: loss = 0.0536\n",
      "step 350: loss = 0.0526\n",
      "step 400: loss = 0.0533\n",
      "step 450: loss = 0.0539\n",
      "test_loss: 0.05452965259552002\n",
      "step 0: loss = 0.0543\n",
      "step 50: loss = 0.0531\n",
      "step 100: loss = 0.0527\n",
      "step 150: loss = 0.0538\n",
      "step 200: loss = 0.0553\n",
      "step 250: loss = 0.0503\n",
      "step 300: loss = 0.0527\n",
      "step 350: loss = 0.0567\n",
      "step 400: loss = 0.0520\n",
      "step 450: loss = 0.0544\n",
      "test_loss: 0.054252209663391116\n",
      "step 0: loss = 0.0534\n",
      "step 50: loss = 0.0550\n",
      "step 100: loss = 0.0510\n",
      "step 150: loss = 0.0519\n",
      "step 200: loss = 0.0532\n",
      "step 250: loss = 0.0534\n",
      "step 300: loss = 0.0523\n",
      "step 350: loss = 0.0531\n",
      "step 400: loss = 0.0531\n",
      "step 450: loss = 0.0555\n",
      "test_loss: 0.05426167964935303\n",
      "step 0: loss = 0.0534\n",
      "step 50: loss = 0.0532\n",
      "step 100: loss = 0.0538\n",
      "step 150: loss = 0.0509\n",
      "step 200: loss = 0.0531\n",
      "step 250: loss = 0.0518\n",
      "step 300: loss = 0.0532\n",
      "step 350: loss = 0.0552\n",
      "step 400: loss = 0.0554\n",
      "step 450: loss = 0.0531\n",
      "test_loss: 0.05343255043029785\n",
      "step 0: loss = 0.0535\n",
      "step 50: loss = 0.0523\n",
      "step 100: loss = 0.0520\n",
      "step 150: loss = 0.0540\n",
      "step 200: loss = 0.0569\n",
      "step 250: loss = 0.0535\n",
      "step 300: loss = 0.0537\n",
      "step 350: loss = 0.0518\n",
      "step 400: loss = 0.0538\n",
      "step 450: loss = 0.0555\n",
      "test_loss: 0.05284505367279053\n",
      "step 0: loss = 0.0517\n",
      "step 50: loss = 0.0519\n",
      "step 100: loss = 0.0526\n",
      "step 150: loss = 0.0517\n",
      "step 200: loss = 0.0523\n",
      "step 250: loss = 0.0525\n",
      "step 300: loss = 0.0526\n",
      "step 350: loss = 0.0546\n",
      "step 400: loss = 0.0524\n",
      "step 450: loss = 0.0549\n",
      "test_loss: 0.053041329383850096\n",
      "step 0: loss = 0.0482\n",
      "step 50: loss = 0.0538\n",
      "step 100: loss = 0.0537\n",
      "step 150: loss = 0.0535\n",
      "step 200: loss = 0.0533\n",
      "step 250: loss = 0.0516\n",
      "step 300: loss = 0.0533\n",
      "step 350: loss = 0.0509\n",
      "step 400: loss = 0.0535\n",
      "step 450: loss = 0.0511\n",
      "test_loss: 0.053971967697143554\n",
      "step 0: loss = 0.0550\n",
      "step 50: loss = 0.0569\n",
      "step 100: loss = 0.0547\n",
      "step 150: loss = 0.0514\n",
      "step 200: loss = 0.0518\n",
      "step 250: loss = 0.0568\n",
      "step 300: loss = 0.0526\n",
      "step 350: loss = 0.0510\n",
      "step 400: loss = 0.0516\n",
      "step 450: loss = 0.0522\n",
      "test_loss: 0.053531336784362796\n",
      "step 0: loss = 0.0532\n",
      "step 50: loss = 0.0513\n",
      "step 100: loss = 0.0516\n",
      "step 150: loss = 0.0545\n",
      "step 200: loss = 0.0537\n",
      "step 250: loss = 0.0564\n",
      "step 300: loss = 0.0538\n",
      "step 350: loss = 0.0515\n",
      "step 400: loss = 0.0536\n",
      "step 450: loss = 0.0538\n",
      "test_loss: 0.052956252098083495\n",
      "step 0: loss = 0.0541\n",
      "step 50: loss = 0.0531\n",
      "step 100: loss = 0.0533\n",
      "step 150: loss = 0.0552\n",
      "step 200: loss = 0.0512\n",
      "step 250: loss = 0.0536\n",
      "step 300: loss = 0.0538\n",
      "step 350: loss = 0.0528\n",
      "step 400: loss = 0.0509\n",
      "step 450: loss = 0.0533\n",
      "test_loss: 0.05300673007965088\n",
      "step 0: loss = 0.0513\n",
      "step 50: loss = 0.0533\n",
      "step 100: loss = 0.0520\n",
      "step 150: loss = 0.0540\n",
      "step 200: loss = 0.0522\n",
      "step 250: loss = 0.0517\n",
      "step 300: loss = 0.0535\n",
      "step 350: loss = 0.0525\n",
      "step 400: loss = 0.0505\n",
      "step 450: loss = 0.0504\n",
      "test_loss: 0.052973241806030275\n",
      "step 0: loss = 0.0519\n",
      "step 50: loss = 0.0515\n",
      "step 100: loss = 0.0538\n",
      "step 150: loss = 0.0524\n",
      "step 200: loss = 0.0521\n",
      "step 250: loss = 0.0543\n",
      "step 300: loss = 0.0483\n",
      "step 350: loss = 0.0549\n",
      "step 400: loss = 0.0528\n",
      "step 450: loss = 0.0529\n",
      "test_loss: 0.05382730007171631\n",
      "step 0: loss = 0.0530\n",
      "step 50: loss = 0.0538\n",
      "step 100: loss = 0.0544\n",
      "step 150: loss = 0.0543\n",
      "step 200: loss = 0.0523\n",
      "step 250: loss = 0.0551\n",
      "step 300: loss = 0.0504\n",
      "step 350: loss = 0.0538\n",
      "step 400: loss = 0.0511\n",
      "step 450: loss = 0.0525\n",
      "test_loss: 0.0523157262802124\n",
      "step 0: loss = 0.0517\n",
      "step 50: loss = 0.0511\n",
      "step 100: loss = 0.0534\n",
      "step 150: loss = 0.0514\n",
      "step 200: loss = 0.0548\n",
      "step 250: loss = 0.0508\n",
      "step 300: loss = 0.0514\n",
      "step 350: loss = 0.0532\n",
      "step 400: loss = 0.0486\n",
      "step 450: loss = 0.0508\n",
      "test_loss: 0.05305689811706543\n",
      "step 0: loss = 0.0537\n",
      "step 50: loss = 0.0532\n",
      "step 100: loss = 0.0514\n",
      "step 150: loss = 0.0555\n",
      "step 200: loss = 0.0528\n",
      "step 250: loss = 0.0501\n",
      "step 300: loss = 0.0543\n",
      "step 350: loss = 0.0529\n",
      "step 400: loss = 0.0522\n",
      "step 450: loss = 0.0521\n",
      "test_loss: 0.052526607513427734\n",
      "step 0: loss = 0.0507\n",
      "step 50: loss = 0.0510\n",
      "step 100: loss = 0.0527\n",
      "step 150: loss = 0.0519\n",
      "step 200: loss = 0.0516\n",
      "step 250: loss = 0.0516\n",
      "step 300: loss = 0.0538\n",
      "step 350: loss = 0.0514\n",
      "step 400: loss = 0.0499\n",
      "step 450: loss = 0.0520\n",
      "test_loss: 0.052438063621520994\n",
      "step 0: loss = 0.0526\n",
      "step 50: loss = 0.0500\n",
      "step 100: loss = 0.0499\n",
      "step 150: loss = 0.0557\n",
      "step 200: loss = 0.0521\n",
      "step 250: loss = 0.0507\n",
      "step 300: loss = 0.0527\n",
      "step 350: loss = 0.0511\n",
      "step 400: loss = 0.0507\n",
      "step 450: loss = 0.0520\n",
      "test_loss: 0.05213547706604004\n",
      "step 0: loss = 0.0496\n",
      "step 50: loss = 0.0528\n",
      "step 100: loss = 0.0514\n",
      "step 150: loss = 0.0542\n",
      "step 200: loss = 0.0522\n",
      "step 250: loss = 0.0534\n",
      "step 300: loss = 0.0517\n",
      "step 350: loss = 0.0508\n",
      "step 400: loss = 0.0531\n",
      "step 450: loss = 0.0506\n",
      "test_loss: 0.05194148540496826\n",
      "step 0: loss = 0.0523\n",
      "step 50: loss = 0.0550\n",
      "step 100: loss = 0.0525\n",
      "step 150: loss = 0.0501\n",
      "step 200: loss = 0.0545\n",
      "step 250: loss = 0.0520\n",
      "step 300: loss = 0.0516\n",
      "step 350: loss = 0.0506\n",
      "step 400: loss = 0.0520\n",
      "step 450: loss = 0.0520\n",
      "test_loss: 0.05241405487060547\n",
      "step 0: loss = 0.0549\n",
      "step 50: loss = 0.0519\n",
      "step 100: loss = 0.0529\n",
      "step 150: loss = 0.0542\n",
      "step 200: loss = 0.0508\n",
      "step 250: loss = 0.0520\n",
      "step 300: loss = 0.0536\n",
      "step 350: loss = 0.0494\n",
      "step 400: loss = 0.0512\n",
      "step 450: loss = 0.0507\n",
      "test_loss: 0.05201882362365723\n",
      "step 0: loss = 0.0498\n",
      "step 50: loss = 0.0513\n",
      "step 100: loss = 0.0505\n",
      "step 150: loss = 0.0491\n",
      "step 200: loss = 0.0519\n",
      "step 250: loss = 0.0527\n",
      "step 300: loss = 0.0498\n",
      "step 350: loss = 0.0516\n",
      "step 400: loss = 0.0492\n",
      "step 450: loss = 0.0538\n",
      "test_loss: 0.0513053035736084\n",
      "step 0: loss = 0.0505\n",
      "step 50: loss = 0.0507\n",
      "step 100: loss = 0.0507\n",
      "step 150: loss = 0.0484\n",
      "step 200: loss = 0.0518\n",
      "step 250: loss = 0.0496\n",
      "step 300: loss = 0.0506\n",
      "step 350: loss = 0.0538\n",
      "step 400: loss = 0.0499\n",
      "step 450: loss = 0.0514\n",
      "test_loss: 0.05178990840911865\n",
      "step 0: loss = 0.0524\n",
      "step 50: loss = 0.0534\n",
      "step 100: loss = 0.0502\n",
      "step 150: loss = 0.0527\n",
      "step 200: loss = 0.0516\n",
      "step 250: loss = 0.0502\n",
      "step 300: loss = 0.0537\n",
      "step 350: loss = 0.0489\n",
      "step 400: loss = 0.0512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 450: loss = 0.0523\n",
      "test_loss: 0.05127671241760254\n",
      "step 0: loss = 0.0502\n",
      "step 50: loss = 0.0503\n",
      "step 100: loss = 0.0522\n",
      "step 150: loss = 0.0514\n",
      "step 200: loss = 0.0510\n",
      "step 250: loss = 0.0513\n",
      "step 300: loss = 0.0517\n",
      "step 350: loss = 0.0509\n",
      "step 400: loss = 0.0562\n",
      "step 450: loss = 0.0509\n",
      "test_loss: 0.05165863037109375\n",
      "step 0: loss = 0.0495\n",
      "step 50: loss = 0.0503\n",
      "step 100: loss = 0.0521\n",
      "step 150: loss = 0.0510\n",
      "step 200: loss = 0.0489\n",
      "step 250: loss = 0.0546\n",
      "step 300: loss = 0.0531\n",
      "step 350: loss = 0.0516\n",
      "step 400: loss = 0.0501\n",
      "step 450: loss = 0.0506\n",
      "test_loss: 0.05186009407043457\n",
      "step 0: loss = 0.0521\n",
      "step 50: loss = 0.0505\n",
      "step 100: loss = 0.0505\n",
      "step 150: loss = 0.0506\n",
      "step 200: loss = 0.0536\n",
      "step 250: loss = 0.0540\n",
      "step 300: loss = 0.0503\n",
      "step 350: loss = 0.0508\n",
      "step 400: loss = 0.0512\n",
      "step 450: loss = 0.0506\n",
      "test_loss: 0.050396156311035153\n",
      "step 0: loss = 0.0527\n",
      "step 50: loss = 0.0507\n",
      "step 100: loss = 0.0540\n",
      "step 150: loss = 0.0506\n",
      "step 200: loss = 0.0516\n",
      "step 250: loss = 0.0500\n",
      "step 300: loss = 0.0516\n",
      "step 350: loss = 0.0479\n",
      "step 400: loss = 0.0462\n",
      "step 450: loss = 0.0506\n",
      "test_loss: 0.05109934329986572\n",
      "step 0: loss = 0.0536\n",
      "step 50: loss = 0.0505\n",
      "step 100: loss = 0.0527\n",
      "step 150: loss = 0.0505\n",
      "step 200: loss = 0.0531\n",
      "step 250: loss = 0.0490\n",
      "step 300: loss = 0.0505\n",
      "step 350: loss = 0.0506\n",
      "step 400: loss = 0.0507\n",
      "step 450: loss = 0.0512\n",
      "test_loss: 0.05145021915435791\n",
      "step 0: loss = 0.0510\n",
      "step 50: loss = 0.0485\n",
      "step 100: loss = 0.0521\n",
      "step 150: loss = 0.0493\n",
      "step 200: loss = 0.0490\n",
      "step 250: loss = 0.0475\n",
      "step 300: loss = 0.0501\n",
      "step 350: loss = 0.0497\n",
      "step 400: loss = 0.0489\n",
      "step 450: loss = 0.0510\n",
      "test_loss: 0.051036109924316404\n",
      "step 0: loss = 0.0495\n",
      "step 50: loss = 0.0507\n",
      "step 100: loss = 0.0525\n",
      "step 150: loss = 0.0518\n",
      "step 200: loss = 0.0524\n",
      "step 250: loss = 0.0527\n",
      "step 300: loss = 0.0507\n",
      "step 350: loss = 0.0529\n",
      "step 400: loss = 0.0501\n",
      "step 450: loss = 0.0518\n",
      "test_loss: 0.051105804443359375\n",
      "step 0: loss = 0.0512\n",
      "step 50: loss = 0.0509\n",
      "step 100: loss = 0.0499\n",
      "step 150: loss = 0.0500\n",
      "step 200: loss = 0.0531\n",
      "step 250: loss = 0.0510\n",
      "step 300: loss = 0.0492\n",
      "step 350: loss = 0.0524\n",
      "step 400: loss = 0.0490\n",
      "step 450: loss = 0.0503\n",
      "test_loss: 0.05094101428985596\n",
      "step 0: loss = 0.0484\n",
      "step 50: loss = 0.0510\n",
      "step 100: loss = 0.0505\n",
      "step 150: loss = 0.0497\n",
      "step 200: loss = 0.0487\n",
      "step 250: loss = 0.0517\n",
      "step 300: loss = 0.0504\n",
      "step 350: loss = 0.0479\n",
      "step 400: loss = 0.0497\n",
      "step 450: loss = 0.0552\n",
      "test_loss: 0.050753574371337894\n",
      "step 0: loss = 0.0487\n",
      "step 50: loss = 0.0506\n",
      "step 100: loss = 0.0502\n",
      "step 150: loss = 0.0506\n",
      "step 200: loss = 0.0513\n",
      "step 250: loss = 0.0515\n",
      "step 300: loss = 0.0537\n",
      "step 350: loss = 0.0501\n",
      "step 400: loss = 0.0511\n",
      "step 450: loss = 0.0496\n",
      "test_loss: 0.05110194206237793\n",
      "step 0: loss = 0.0500\n",
      "step 50: loss = 0.0512\n",
      "step 100: loss = 0.0513\n",
      "step 150: loss = 0.0531\n",
      "step 200: loss = 0.0534\n",
      "step 250: loss = 0.0515\n",
      "step 300: loss = 0.0500\n",
      "step 350: loss = 0.0539\n",
      "step 400: loss = 0.0515\n",
      "step 450: loss = 0.0495\n",
      "test_loss: 0.050409092903137206\n",
      "step 0: loss = 0.0492\n",
      "step 50: loss = 0.0515\n",
      "step 100: loss = 0.0516\n",
      "step 150: loss = 0.0515\n",
      "step 200: loss = 0.0473\n",
      "step 250: loss = 0.0510\n",
      "step 300: loss = 0.0493\n",
      "step 350: loss = 0.0507\n",
      "step 400: loss = 0.0518\n",
      "step 450: loss = 0.0507\n",
      "test_loss: 0.05059441566467285\n",
      "step 0: loss = 0.0479\n",
      "step 50: loss = 0.0523\n",
      "step 100: loss = 0.0506\n",
      "step 150: loss = 0.0533\n",
      "step 200: loss = 0.0499\n",
      "step 250: loss = 0.0502\n",
      "step 300: loss = 0.0528\n",
      "step 350: loss = 0.0526\n",
      "step 400: loss = 0.0525\n",
      "step 450: loss = 0.0512\n",
      "test_loss: 0.05104284286499024\n",
      "step 0: loss = 0.0513\n",
      "step 50: loss = 0.0499\n",
      "step 100: loss = 0.0517\n",
      "step 150: loss = 0.0511\n",
      "step 200: loss = 0.0491\n",
      "step 250: loss = 0.0532\n",
      "step 300: loss = 0.0506\n",
      "step 350: loss = 0.0517\n",
      "step 400: loss = 0.0501\n",
      "step 450: loss = 0.0513\n",
      "test_loss: 0.050510225296020506\n",
      "step 0: loss = 0.0491\n",
      "step 50: loss = 0.0496\n",
      "step 100: loss = 0.0505\n",
      "step 150: loss = 0.0528\n",
      "step 200: loss = 0.0510\n",
      "step 250: loss = 0.0506\n",
      "step 300: loss = 0.0500\n",
      "step 350: loss = 0.0476\n",
      "step 400: loss = 0.0489\n",
      "step 450: loss = 0.0514\n",
      "test_loss: 0.05108290672302246\n",
      "step 0: loss = 0.0525\n",
      "step 50: loss = 0.0520\n",
      "step 100: loss = 0.0475\n",
      "step 150: loss = 0.0496\n",
      "step 200: loss = 0.0510\n",
      "step 250: loss = 0.0502\n",
      "step 300: loss = 0.0495\n",
      "step 350: loss = 0.0483\n",
      "step 400: loss = 0.0491\n",
      "step 450: loss = 0.0496\n",
      "test_loss: 0.050553722381591795\n",
      "step 0: loss = 0.0495\n",
      "step 50: loss = 0.0498\n",
      "step 100: loss = 0.0506\n",
      "step 150: loss = 0.0491\n",
      "step 200: loss = 0.0487\n",
      "step 250: loss = 0.0510\n",
      "step 300: loss = 0.0505\n",
      "step 350: loss = 0.0522\n",
      "step 400: loss = 0.0506\n",
      "step 450: loss = 0.0513\n",
      "test_loss: 0.050922703742980954\n",
      "step 0: loss = 0.0522\n",
      "step 50: loss = 0.0529\n",
      "step 100: loss = 0.0528\n",
      "step 150: loss = 0.0514\n",
      "step 200: loss = 0.0515\n",
      "step 250: loss = 0.0497\n",
      "step 300: loss = 0.0519\n",
      "step 350: loss = 0.0505\n",
      "step 400: loss = 0.0480\n",
      "step 450: loss = 0.0516\n",
      "test_loss: 0.05055343627929688\n",
      "step 0: loss = 0.0496\n",
      "step 50: loss = 0.0501\n",
      "step 100: loss = 0.0476\n",
      "step 150: loss = 0.0508\n",
      "step 200: loss = 0.0522\n",
      "step 250: loss = 0.0507\n",
      "step 300: loss = 0.0514\n",
      "step 350: loss = 0.0500\n",
      "step 400: loss = 0.0484\n",
      "step 450: loss = 0.0504\n",
      "test_loss: 0.05090210437774658\n",
      "step 0: loss = 0.0492\n",
      "step 50: loss = 0.0495\n",
      "step 100: loss = 0.0498\n",
      "step 150: loss = 0.0506\n",
      "step 200: loss = 0.0536\n",
      "step 250: loss = 0.0498\n",
      "step 300: loss = 0.0499\n",
      "step 350: loss = 0.0533\n",
      "step 400: loss = 0.0528\n",
      "step 450: loss = 0.0498\n",
      "test_loss: 0.050593905448913574\n",
      "step 0: loss = 0.0509\n",
      "step 50: loss = 0.0517\n",
      "step 100: loss = 0.0498\n",
      "step 150: loss = 0.0521\n",
      "step 200: loss = 0.0508\n",
      "step 250: loss = 0.0502\n",
      "step 300: loss = 0.0509\n",
      "step 350: loss = 0.0488\n",
      "step 400: loss = 0.0519\n",
      "step 450: loss = 0.0516\n",
      "test_loss: 0.05063100337982178\n",
      "step 0: loss = 0.0476\n",
      "step 50: loss = 0.0501\n",
      "step 100: loss = 0.0505\n",
      "step 150: loss = 0.0493\n",
      "step 200: loss = 0.0503\n",
      "step 250: loss = 0.0505\n",
      "step 300: loss = 0.0462\n",
      "step 350: loss = 0.0501\n",
      "step 400: loss = 0.0491\n",
      "step 450: loss = 0.0527\n",
      "test_loss: 0.050606608390808105\n",
      "step 0: loss = 0.0490\n",
      "step 50: loss = 0.0491\n",
      "step 100: loss = 0.0514\n",
      "step 150: loss = 0.0518\n",
      "step 200: loss = 0.0493\n",
      "step 250: loss = 0.0504\n",
      "step 300: loss = 0.0490\n",
      "step 350: loss = 0.0529\n",
      "step 400: loss = 0.0493\n",
      "step 450: loss = 0.0514\n",
      "test_loss: 0.05111423969268799\n",
      "step 0: loss = 0.0508\n",
      "step 50: loss = 0.0498\n",
      "step 100: loss = 0.0514\n",
      "step 150: loss = 0.0485\n",
      "step 200: loss = 0.0516\n",
      "step 250: loss = 0.0502\n",
      "step 300: loss = 0.0483\n",
      "step 350: loss = 0.0520\n",
      "step 400: loss = 0.0530\n",
      "step 450: loss = 0.0479\n",
      "test_loss: 0.05043315887451172\n",
      "step 0: loss = 0.0504\n",
      "step 50: loss = 0.0526\n",
      "step 100: loss = 0.0490\n",
      "step 150: loss = 0.0480\n",
      "step 200: loss = 0.0528\n",
      "step 250: loss = 0.0496\n",
      "step 300: loss = 0.0503\n",
      "step 350: loss = 0.0489\n",
      "step 400: loss = 0.0494\n",
      "step 450: loss = 0.0493\n",
      "test_loss: 0.05046006202697754\n",
      "step 0: loss = 0.0493\n",
      "step 50: loss = 0.0503\n",
      "step 100: loss = 0.0521\n",
      "step 150: loss = 0.0500\n",
      "step 200: loss = 0.0486\n",
      "step 250: loss = 0.0491\n",
      "step 300: loss = 0.0505\n",
      "step 350: loss = 0.0482\n",
      "step 400: loss = 0.0492\n",
      "step 450: loss = 0.0501\n",
      "test_loss: 0.050398397445678714\n",
      "step 0: loss = 0.0502\n",
      "step 50: loss = 0.0484\n",
      "step 100: loss = 0.0508\n",
      "step 150: loss = 0.0497\n",
      "step 200: loss = 0.0503\n",
      "step 250: loss = 0.0529\n",
      "step 300: loss = 0.0482\n",
      "step 350: loss = 0.0515\n",
      "step 400: loss = 0.0469\n",
      "step 450: loss = 0.0506\n",
      "test_loss: 0.050074806213378904\n",
      "step 0: loss = 0.0511\n",
      "step 50: loss = 0.0508\n",
      "step 100: loss = 0.0493\n",
      "step 150: loss = 0.0520\n",
      "step 200: loss = 0.0473\n",
      "step 250: loss = 0.0514\n",
      "step 300: loss = 0.0482\n",
      "step 350: loss = 0.0533\n",
      "step 400: loss = 0.0519\n",
      "step 450: loss = 0.0520\n",
      "test_loss: 0.050126147270202634\n",
      "step 0: loss = 0.0484\n",
      "step 50: loss = 0.0469\n",
      "step 100: loss = 0.0497\n",
      "step 150: loss = 0.0517\n",
      "step 200: loss = 0.0493\n",
      "step 250: loss = 0.0475\n",
      "step 300: loss = 0.0497\n",
      "step 350: loss = 0.0517\n",
      "step 400: loss = 0.0468\n",
      "step 450: loss = 0.0494\n",
      "test_loss: 0.05014997959136963\n",
      "step 0: loss = 0.0499\n",
      "step 50: loss = 0.0487\n",
      "step 100: loss = 0.0524\n",
      "step 150: loss = 0.0535\n",
      "step 200: loss = 0.0520\n",
      "step 250: loss = 0.0509\n",
      "step 300: loss = 0.0509\n",
      "step 350: loss = 0.0496\n",
      "step 400: loss = 0.0487\n",
      "step 450: loss = 0.0511\n",
      "test_loss: 0.05048524856567383\n",
      "step 0: loss = 0.0492\n",
      "step 50: loss = 0.0489\n",
      "step 100: loss = 0.0508\n",
      "step 150: loss = 0.0502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 200: loss = 0.0483\n",
      "step 250: loss = 0.0482\n",
      "step 300: loss = 0.0506\n",
      "step 350: loss = 0.0504\n",
      "step 400: loss = 0.0503\n",
      "step 450: loss = 0.0482\n",
      "test_loss: 0.05014061450958252\n",
      "step 0: loss = 0.0504\n",
      "step 50: loss = 0.0482\n",
      "step 100: loss = 0.0500\n",
      "step 150: loss = 0.0499\n",
      "step 200: loss = 0.0488\n",
      "step 250: loss = 0.0508\n",
      "step 300: loss = 0.0485\n",
      "step 350: loss = 0.0492\n",
      "step 400: loss = 0.0511\n",
      "step 450: loss = 0.0496\n",
      "test_loss: 0.05040798187255859\n",
      "step 0: loss = 0.0494\n",
      "step 50: loss = 0.0476\n",
      "step 100: loss = 0.0518\n",
      "step 150: loss = 0.0516\n",
      "step 200: loss = 0.0500\n",
      "step 250: loss = 0.0492\n",
      "step 300: loss = 0.0489\n",
      "step 350: loss = 0.0479\n",
      "step 400: loss = 0.0502\n",
      "step 450: loss = 0.0507\n",
      "test_loss: 0.04958394527435303\n",
      "step 0: loss = 0.0500\n",
      "step 50: loss = 0.0491\n",
      "step 100: loss = 0.0475\n",
      "step 150: loss = 0.0471\n",
      "step 200: loss = 0.0515\n",
      "step 250: loss = 0.0514\n",
      "step 300: loss = 0.0512\n",
      "step 350: loss = 0.0492\n",
      "step 400: loss = 0.0498\n",
      "step 450: loss = 0.0528\n",
      "test_loss: 0.05033284664154053\n",
      "step 0: loss = 0.0489\n",
      "step 50: loss = 0.0500\n",
      "step 100: loss = 0.0495\n",
      "step 150: loss = 0.0510\n",
      "step 200: loss = 0.0499\n",
      "step 250: loss = 0.0485\n",
      "step 300: loss = 0.0522\n",
      "step 350: loss = 0.0504\n",
      "step 400: loss = 0.0482\n",
      "step 450: loss = 0.0504\n",
      "test_loss: 0.05022035598754883\n",
      "step 0: loss = 0.0509\n",
      "step 50: loss = 0.0463\n",
      "step 100: loss = 0.0500\n",
      "step 150: loss = 0.0482\n",
      "step 200: loss = 0.0474\n",
      "step 250: loss = 0.0519\n",
      "step 300: loss = 0.0521\n",
      "step 350: loss = 0.0486\n",
      "step 400: loss = 0.0490\n",
      "step 450: loss = 0.0494\n",
      "test_loss: 0.04962850570678711\n",
      "step 0: loss = 0.0496\n",
      "step 50: loss = 0.0507\n",
      "step 100: loss = 0.0485\n",
      "step 150: loss = 0.0492\n",
      "step 200: loss = 0.0518\n",
      "step 250: loss = 0.0488\n",
      "step 300: loss = 0.0490\n",
      "step 350: loss = 0.0485\n",
      "step 400: loss = 0.0510\n",
      "step 450: loss = 0.0474\n",
      "test_loss: 0.04966360092163086\n",
      "step 0: loss = 0.0490\n",
      "step 50: loss = 0.0512\n",
      "step 100: loss = 0.0478\n",
      "step 150: loss = 0.0489\n",
      "step 200: loss = 0.0500\n",
      "step 250: loss = 0.0480\n",
      "step 300: loss = 0.0520\n",
      "step 350: loss = 0.0500\n",
      "step 400: loss = 0.0489\n",
      "step 450: loss = 0.0484\n",
      "test_loss: 0.0500078821182251\n",
      "step 0: loss = 0.0479\n",
      "step 50: loss = 0.0498\n",
      "step 100: loss = 0.0505\n",
      "step 150: loss = 0.0477\n",
      "step 200: loss = 0.0511\n",
      "step 250: loss = 0.0495\n",
      "step 300: loss = 0.0491\n",
      "step 350: loss = 0.0497\n",
      "step 400: loss = 0.0483\n",
      "step 450: loss = 0.0495\n",
      "test_loss: 0.0499114990234375\n",
      "step 0: loss = 0.0488\n",
      "step 50: loss = 0.0492\n",
      "step 100: loss = 0.0505\n",
      "step 150: loss = 0.0509\n",
      "step 200: loss = 0.0502\n",
      "step 250: loss = 0.0516\n",
      "step 300: loss = 0.0467\n",
      "step 350: loss = 0.0506\n",
      "step 400: loss = 0.0506\n",
      "step 450: loss = 0.0493\n",
      "test_loss: 0.0497231912612915\n",
      "step 0: loss = 0.0477\n",
      "step 50: loss = 0.0506\n",
      "step 100: loss = 0.0489\n",
      "step 150: loss = 0.0467\n",
      "step 200: loss = 0.0494\n",
      "step 250: loss = 0.0483\n",
      "step 300: loss = 0.0502\n",
      "step 350: loss = 0.0488\n",
      "step 400: loss = 0.0502\n",
      "step 450: loss = 0.0497\n",
      "test_loss: 0.04946410655975342\n",
      "step 0: loss = 0.0488\n",
      "step 50: loss = 0.0500\n",
      "step 100: loss = 0.0486\n",
      "step 150: loss = 0.0472\n",
      "step 200: loss = 0.0492\n",
      "step 250: loss = 0.0492\n",
      "step 300: loss = 0.0516\n",
      "step 350: loss = 0.0476\n",
      "step 400: loss = 0.0496\n",
      "step 450: loss = 0.0491\n",
      "test_loss: 0.04964874744415283\n",
      "step 0: loss = 0.0463\n",
      "step 50: loss = 0.0515\n",
      "step 100: loss = 0.0471\n",
      "step 150: loss = 0.0497\n",
      "step 200: loss = 0.0493\n",
      "step 250: loss = 0.0480\n",
      "step 300: loss = 0.0486\n",
      "step 350: loss = 0.0487\n",
      "step 400: loss = 0.0489\n",
      "step 450: loss = 0.0509\n",
      "test_loss: 0.04976690292358398\n",
      "step 0: loss = 0.0490\n",
      "step 50: loss = 0.0492\n",
      "step 100: loss = 0.0496\n",
      "step 150: loss = 0.0488\n",
      "step 200: loss = 0.0496\n",
      "step 250: loss = 0.0507\n",
      "step 300: loss = 0.0497\n",
      "step 350: loss = 0.0510\n",
      "step 400: loss = 0.0489\n",
      "step 450: loss = 0.0485\n",
      "test_loss: 0.04969268321990967\n",
      "step 0: loss = 0.0482\n",
      "step 50: loss = 0.0495\n",
      "step 100: loss = 0.0503\n",
      "step 150: loss = 0.0494\n",
      "step 200: loss = 0.0490\n",
      "step 250: loss = 0.0505\n",
      "step 300: loss = 0.0517\n",
      "step 350: loss = 0.0482\n",
      "step 400: loss = 0.0480\n",
      "step 450: loss = 0.0489\n",
      "test_loss: 0.049719805717468264\n",
      "step 0: loss = 0.0475\n",
      "step 50: loss = 0.0477\n",
      "step 100: loss = 0.0522\n",
      "step 150: loss = 0.0493\n",
      "step 200: loss = 0.0496\n",
      "step 250: loss = 0.0516\n",
      "step 300: loss = 0.0512\n",
      "step 350: loss = 0.0481\n",
      "step 400: loss = 0.0468\n",
      "step 450: loss = 0.0493\n",
      "test_loss: 0.04946147918701172\n",
      "step 0: loss = 0.0463\n",
      "step 50: loss = 0.0496\n",
      "step 100: loss = 0.0491\n",
      "step 150: loss = 0.0473\n",
      "step 200: loss = 0.0495\n",
      "step 250: loss = 0.0487\n",
      "step 300: loss = 0.0503\n",
      "step 350: loss = 0.0505\n",
      "step 400: loss = 0.0512\n",
      "step 450: loss = 0.0514\n",
      "test_loss: 0.04984733581542969\n",
      "step 0: loss = 0.0483\n",
      "step 50: loss = 0.0470\n",
      "step 100: loss = 0.0495\n",
      "step 150: loss = 0.0494\n",
      "step 200: loss = 0.0495\n",
      "step 250: loss = 0.0517\n",
      "step 300: loss = 0.0492\n",
      "step 350: loss = 0.0484\n",
      "step 400: loss = 0.0476\n",
      "step 450: loss = 0.0478\n",
      "test_loss: 0.04935990810394287\n",
      "step 0: loss = 0.0480\n",
      "step 50: loss = 0.0486\n",
      "step 100: loss = 0.0515\n",
      "step 150: loss = 0.0484\n",
      "step 200: loss = 0.0481\n",
      "step 250: loss = 0.0492\n",
      "step 300: loss = 0.0491\n",
      "step 350: loss = 0.0507\n",
      "step 400: loss = 0.0504\n",
      "step 450: loss = 0.0489\n",
      "test_loss: 0.04917756080627442\n",
      "step 0: loss = 0.0466\n",
      "step 50: loss = 0.0496\n",
      "step 100: loss = 0.0478\n",
      "step 150: loss = 0.0488\n",
      "step 200: loss = 0.0534\n",
      "step 250: loss = 0.0465\n",
      "step 300: loss = 0.0488\n",
      "step 350: loss = 0.0486\n",
      "step 400: loss = 0.0471\n",
      "step 450: loss = 0.0481\n",
      "test_loss: 0.04954183578491211\n",
      "step 0: loss = 0.0465\n",
      "step 50: loss = 0.0494\n",
      "step 100: loss = 0.0480\n",
      "step 150: loss = 0.0469\n",
      "step 200: loss = 0.0503\n",
      "step 250: loss = 0.0501\n",
      "step 300: loss = 0.0452\n",
      "step 350: loss = 0.0493\n",
      "step 400: loss = 0.0509\n",
      "step 450: loss = 0.0487\n",
      "test_loss: 0.049121613502502444\n",
      "step 0: loss = 0.0489\n",
      "step 50: loss = 0.0494\n",
      "step 100: loss = 0.0467\n",
      "step 150: loss = 0.0500\n",
      "step 200: loss = 0.0479\n",
      "step 250: loss = 0.0486\n",
      "step 300: loss = 0.0500\n",
      "step 350: loss = 0.0470\n",
      "step 400: loss = 0.0505\n",
      "step 450: loss = 0.0472\n",
      "test_loss: 0.04937081813812256\n",
      "step 0: loss = 0.0478\n",
      "step 50: loss = 0.0525\n",
      "step 100: loss = 0.0496\n",
      "step 150: loss = 0.0488\n",
      "step 200: loss = 0.0482\n",
      "step 250: loss = 0.0517\n",
      "step 300: loss = 0.0498\n",
      "step 350: loss = 0.0502\n",
      "step 400: loss = 0.0492\n",
      "step 450: loss = 0.0496\n",
      "test_loss: 0.04959331512451172\n",
      "step 0: loss = 0.0481\n",
      "step 50: loss = 0.0488\n",
      "step 100: loss = 0.0501\n",
      "step 150: loss = 0.0483\n",
      "step 200: loss = 0.0470\n",
      "step 250: loss = 0.0515\n",
      "step 300: loss = 0.0484\n",
      "step 350: loss = 0.0525\n",
      "step 400: loss = 0.0508\n",
      "step 450: loss = 0.0477\n",
      "test_loss: 0.049815702438354495\n",
      "step 0: loss = 0.0477\n",
      "step 50: loss = 0.0499\n",
      "step 100: loss = 0.0457\n",
      "step 150: loss = 0.0457\n",
      "step 200: loss = 0.0489\n",
      "step 250: loss = 0.0488\n",
      "step 300: loss = 0.0498\n",
      "step 350: loss = 0.0501\n",
      "step 400: loss = 0.0468\n",
      "step 450: loss = 0.0491\n",
      "test_loss: 0.049199814796447756\n",
      "step 0: loss = 0.0459\n",
      "step 50: loss = 0.0492\n",
      "step 100: loss = 0.0499\n",
      "step 150: loss = 0.0496\n",
      "step 200: loss = 0.0470\n",
      "step 250: loss = 0.0500\n",
      "step 300: loss = 0.0471\n",
      "step 350: loss = 0.0511\n",
      "step 400: loss = 0.0469\n",
      "step 450: loss = 0.0510\n",
      "test_loss: 0.04912667751312256\n",
      "step 0: loss = 0.0506\n",
      "step 50: loss = 0.0477\n",
      "step 100: loss = 0.0477\n",
      "step 150: loss = 0.0495\n",
      "step 200: loss = 0.0502\n",
      "step 250: loss = 0.0505\n",
      "step 300: loss = 0.0481\n",
      "step 350: loss = 0.0499\n",
      "step 400: loss = 0.0521\n",
      "step 450: loss = 0.0482\n",
      "test_loss: 0.049290833473205564\n",
      "step 0: loss = 0.0481\n",
      "step 50: loss = 0.0489\n",
      "step 100: loss = 0.0484\n",
      "step 150: loss = 0.0501\n",
      "step 200: loss = 0.0508\n",
      "step 250: loss = 0.0481\n",
      "step 300: loss = 0.0483\n",
      "step 350: loss = 0.0476\n",
      "step 400: loss = 0.0489\n",
      "step 450: loss = 0.0486\n",
      "test_loss: 0.04877713203430176\n",
      "step 0: loss = 0.0516\n",
      "step 50: loss = 0.0505\n",
      "step 100: loss = 0.0478\n",
      "step 150: loss = 0.0467\n",
      "step 200: loss = 0.0511\n",
      "step 250: loss = 0.0498\n",
      "step 300: loss = 0.0481\n",
      "step 350: loss = 0.0509\n",
      "step 400: loss = 0.0502\n",
      "step 450: loss = 0.0490\n",
      "test_loss: 0.04896468162536621\n",
      "step 0: loss = 0.0502\n",
      "step 50: loss = 0.0470\n",
      "step 100: loss = 0.0474\n",
      "step 150: loss = 0.0480\n",
      "step 200: loss = 0.0511\n",
      "step 250: loss = 0.0488\n",
      "step 300: loss = 0.0479\n",
      "step 350: loss = 0.0462\n",
      "step 400: loss = 0.0458\n",
      "step 450: loss = 0.0533\n",
      "test_loss: 0.0489990234375\n",
      "step 0: loss = 0.0483\n",
      "step 50: loss = 0.0494\n",
      "step 100: loss = 0.0490\n",
      "step 150: loss = 0.0513\n",
      "step 200: loss = 0.0515\n",
      "step 250: loss = 0.0461\n",
      "step 300: loss = 0.0475\n",
      "step 350: loss = 0.0483\n",
      "step 400: loss = 0.0486\n",
      "step 450: loss = 0.0502\n",
      "test_loss: 0.04899600505828858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: loss = 0.0494\n",
      "step 50: loss = 0.0500\n",
      "step 100: loss = 0.0484\n",
      "step 150: loss = 0.0483\n",
      "step 200: loss = 0.0508\n",
      "step 250: loss = 0.0485\n",
      "step 300: loss = 0.0490\n",
      "step 350: loss = 0.0494\n",
      "step 400: loss = 0.0479\n",
      "step 450: loss = 0.0485\n",
      "test_loss: 0.04902827262878418\n",
      "step 0: loss = 0.0464\n",
      "step 50: loss = 0.0499\n",
      "step 100: loss = 0.0476\n",
      "step 150: loss = 0.0483\n",
      "step 200: loss = 0.0504\n",
      "step 250: loss = 0.0484\n",
      "step 300: loss = 0.0468\n",
      "step 350: loss = 0.0490\n",
      "step 400: loss = 0.0474\n",
      "step 450: loss = 0.0512\n",
      "test_loss: 0.0487215518951416\n",
      "step 0: loss = 0.0482\n",
      "step 50: loss = 0.0472\n",
      "step 100: loss = 0.0494\n",
      "step 150: loss = 0.0491\n",
      "step 200: loss = 0.0496\n",
      "step 250: loss = 0.0491\n",
      "step 300: loss = 0.0492\n",
      "step 350: loss = 0.0490\n",
      "step 400: loss = 0.0499\n",
      "step 450: loss = 0.0513\n",
      "test_loss: 0.04895973682403564\n",
      "step 0: loss = 0.0472\n",
      "step 50: loss = 0.0480\n",
      "step 100: loss = 0.0477\n",
      "step 150: loss = 0.0482\n",
      "step 200: loss = 0.0462\n",
      "step 250: loss = 0.0492\n",
      "step 300: loss = 0.0489\n",
      "step 350: loss = 0.0503\n",
      "step 400: loss = 0.0484\n",
      "step 450: loss = 0.0470\n",
      "test_loss: 0.04856959819793701\n",
      "step 0: loss = 0.0469\n",
      "step 50: loss = 0.0501\n",
      "step 100: loss = 0.0474\n",
      "step 150: loss = 0.0484\n",
      "step 200: loss = 0.0490\n",
      "step 250: loss = 0.0473\n",
      "step 300: loss = 0.0468\n",
      "step 350: loss = 0.0479\n",
      "step 400: loss = 0.0517\n",
      "step 450: loss = 0.0485\n",
      "test_loss: 0.04900982856750488\n",
      "step 0: loss = 0.0496\n",
      "step 50: loss = 0.0477\n",
      "step 100: loss = 0.0496\n",
      "step 150: loss = 0.0479\n",
      "step 200: loss = 0.0492\n",
      "step 250: loss = 0.0503\n",
      "step 300: loss = 0.0479\n",
      "step 350: loss = 0.0471\n",
      "step 400: loss = 0.0500\n",
      "step 450: loss = 0.0495\n",
      "test_loss: 0.04873106479644775\n",
      "step 0: loss = 0.0500\n",
      "step 50: loss = 0.0502\n",
      "step 100: loss = 0.0498\n",
      "step 150: loss = 0.0485\n",
      "step 200: loss = 0.0468\n",
      "step 250: loss = 0.0488\n",
      "step 300: loss = 0.0500\n",
      "step 350: loss = 0.0478\n",
      "step 400: loss = 0.0482\n",
      "step 450: loss = 0.0474\n",
      "test_loss: 0.048685102462768554\n",
      "step 0: loss = 0.0479\n",
      "step 50: loss = 0.0482\n",
      "step 100: loss = 0.0471\n",
      "step 150: loss = 0.0461\n",
      "step 200: loss = 0.0485\n",
      "step 250: loss = 0.0479\n",
      "step 300: loss = 0.0489\n",
      "step 350: loss = 0.0501\n",
      "step 400: loss = 0.0474\n",
      "step 450: loss = 0.0476\n",
      "test_loss: 0.04878031730651856\n",
      "step 0: loss = 0.0490\n",
      "step 50: loss = 0.0486\n",
      "step 100: loss = 0.0500\n",
      "step 150: loss = 0.0473\n",
      "step 200: loss = 0.0498\n",
      "step 250: loss = 0.0479\n",
      "step 300: loss = 0.0465\n",
      "step 350: loss = 0.0461\n",
      "step 400: loss = 0.0487\n",
      "step 450: loss = 0.0495\n",
      "test_loss: 0.048297452926635745\n",
      "step 0: loss = 0.0504\n",
      "step 50: loss = 0.0477\n",
      "step 100: loss = 0.0499\n",
      "step 150: loss = 0.0499\n",
      "step 200: loss = 0.0493\n",
      "step 250: loss = 0.0468\n",
      "step 300: loss = 0.0496\n",
      "step 350: loss = 0.0475\n",
      "step 400: loss = 0.0487\n",
      "step 450: loss = 0.0470\n",
      "test_loss: 0.04850601196289062\n",
      "step 0: loss = 0.0483\n",
      "step 50: loss = 0.0486\n",
      "step 100: loss = 0.0472\n",
      "step 150: loss = 0.0481\n",
      "step 200: loss = 0.0480\n",
      "step 250: loss = 0.0498\n",
      "step 300: loss = 0.0488\n",
      "step 350: loss = 0.0487\n",
      "step 400: loss = 0.0487\n",
      "step 450: loss = 0.0481\n",
      "test_loss: 0.048464059829711914\n",
      "step 0: loss = 0.0507\n",
      "step 50: loss = 0.0505\n",
      "step 100: loss = 0.0491\n",
      "step 150: loss = 0.0475\n",
      "step 200: loss = 0.0496\n",
      "step 250: loss = 0.0487\n",
      "step 300: loss = 0.0475\n",
      "step 350: loss = 0.0489\n",
      "step 400: loss = 0.0483\n",
      "step 450: loss = 0.0475\n",
      "test_loss: 0.04846306324005127\n",
      "step 0: loss = 0.0477\n",
      "step 50: loss = 0.0507\n",
      "step 100: loss = 0.0491\n",
      "step 150: loss = 0.0497\n",
      "step 200: loss = 0.0501\n",
      "step 250: loss = 0.0522\n",
      "step 300: loss = 0.0484\n",
      "step 350: loss = 0.0488\n",
      "step 400: loss = 0.0484\n",
      "step 450: loss = 0.0520\n",
      "test_loss: 0.0480957555770874\n",
      "step 0: loss = 0.0447\n",
      "step 50: loss = 0.0487\n",
      "step 100: loss = 0.0488\n",
      "step 150: loss = 0.0479\n",
      "step 200: loss = 0.0446\n",
      "step 250: loss = 0.0479\n",
      "step 300: loss = 0.0507\n",
      "step 350: loss = 0.0471\n",
      "step 400: loss = 0.0495\n",
      "step 450: loss = 0.0486\n",
      "test_loss: 0.04829029083251953\n",
      "step 0: loss = 0.0480\n",
      "step 50: loss = 0.0500\n",
      "step 100: loss = 0.0485\n",
      "step 150: loss = 0.0474\n",
      "step 200: loss = 0.0490\n",
      "step 250: loss = 0.0484\n",
      "step 300: loss = 0.0498\n",
      "step 350: loss = 0.0492\n",
      "step 400: loss = 0.0458\n",
      "step 450: loss = 0.0480\n",
      "test_loss: 0.04886819362640381\n",
      "step 0: loss = 0.0487\n",
      "step 50: loss = 0.0489\n",
      "step 100: loss = 0.0492\n",
      "step 150: loss = 0.0464\n",
      "step 200: loss = 0.0473\n",
      "step 250: loss = 0.0475\n",
      "step 300: loss = 0.0501\n",
      "step 350: loss = 0.0490\n",
      "step 400: loss = 0.0485\n",
      "step 450: loss = 0.0477\n",
      "test_loss: 0.048532414436340335\n",
      "step 0: loss = 0.0469\n",
      "step 50: loss = 0.0484\n",
      "step 100: loss = 0.0480\n",
      "step 150: loss = 0.0490\n",
      "step 200: loss = 0.0454\n",
      "step 250: loss = 0.0497\n",
      "step 300: loss = 0.0475\n",
      "step 350: loss = 0.0481\n",
      "step 400: loss = 0.0482\n",
      "step 450: loss = 0.0474\n",
      "test_loss: 0.048606648445129394\n",
      "step 0: loss = 0.0503\n",
      "step 50: loss = 0.0467\n",
      "step 100: loss = 0.0478\n",
      "step 150: loss = 0.0502\n",
      "step 200: loss = 0.0493\n",
      "step 250: loss = 0.0485\n",
      "step 300: loss = 0.0493\n",
      "step 350: loss = 0.0461\n",
      "step 400: loss = 0.0483\n",
      "step 450: loss = 0.0482\n",
      "test_loss: 0.04836281776428222\n",
      "step 0: loss = 0.0493\n",
      "step 50: loss = 0.0487\n",
      "step 100: loss = 0.0476\n",
      "step 150: loss = 0.0481\n",
      "step 200: loss = 0.0469\n",
      "step 250: loss = 0.0500\n",
      "step 300: loss = 0.0498\n",
      "step 350: loss = 0.0476\n",
      "step 400: loss = 0.0470\n",
      "step 450: loss = 0.0467\n",
      "test_loss: 0.04830364227294922\n",
      "step 0: loss = 0.0479\n",
      "step 50: loss = 0.0483\n",
      "step 100: loss = 0.0474\n",
      "step 150: loss = 0.0469\n",
      "step 200: loss = 0.0483\n",
      "step 250: loss = 0.0489\n",
      "step 300: loss = 0.0520\n",
      "step 350: loss = 0.0505\n",
      "step 400: loss = 0.0489\n",
      "step 450: loss = 0.0487\n",
      "test_loss: 0.04854166507720947\n",
      "step 0: loss = 0.0483\n",
      "step 50: loss = 0.0466\n",
      "step 100: loss = 0.0467\n",
      "step 150: loss = 0.0477\n",
      "step 200: loss = 0.0495\n",
      "step 250: loss = 0.0475\n",
      "step 300: loss = 0.0472\n",
      "step 350: loss = 0.0497\n",
      "step 400: loss = 0.0525\n",
      "step 450: loss = 0.0484\n",
      "test_loss: 0.0484205961227417\n",
      "step 0: loss = 0.0500\n",
      "step 50: loss = 0.0462\n",
      "step 100: loss = 0.0479\n",
      "step 150: loss = 0.0495\n",
      "step 200: loss = 0.0495\n",
      "step 250: loss = 0.0481\n",
      "step 300: loss = 0.0490\n",
      "step 350: loss = 0.0505\n",
      "step 400: loss = 0.0476\n",
      "step 450: loss = 0.0471\n",
      "test_loss: 0.04819990158081055\n",
      "step 0: loss = 0.0495\n",
      "step 50: loss = 0.0486\n",
      "step 100: loss = 0.0462\n",
      "step 150: loss = 0.0479\n",
      "step 200: loss = 0.0479\n",
      "step 250: loss = 0.0475\n",
      "step 300: loss = 0.0479\n",
      "step 350: loss = 0.0494\n",
      "step 400: loss = 0.0463\n",
      "step 450: loss = 0.0485\n",
      "test_loss: 0.04838897705078125\n",
      "step 0: loss = 0.0500\n",
      "step 50: loss = 0.0507\n",
      "step 100: loss = 0.0492\n",
      "step 150: loss = 0.0482\n",
      "step 200: loss = 0.0477\n",
      "step 250: loss = 0.0463\n",
      "step 300: loss = 0.0499\n",
      "step 350: loss = 0.0476\n",
      "step 400: loss = 0.0479\n",
      "step 450: loss = 0.0467\n",
      "test_loss: 0.04808724880218506\n",
      "step 0: loss = 0.0496\n",
      "step 50: loss = 0.0493\n",
      "step 100: loss = 0.0488\n",
      "step 150: loss = 0.0494\n",
      "step 200: loss = 0.0476\n",
      "step 250: loss = 0.0463\n",
      "step 300: loss = 0.0488\n",
      "step 350: loss = 0.0454\n",
      "step 400: loss = 0.0503\n",
      "step 450: loss = 0.0479\n",
      "test_loss: 0.048425822257995604\n",
      "step 0: loss = 0.0483\n",
      "step 50: loss = 0.0489\n",
      "step 100: loss = 0.0474\n",
      "step 150: loss = 0.0477\n",
      "step 200: loss = 0.0477\n",
      "step 250: loss = 0.0474\n",
      "step 300: loss = 0.0485\n",
      "step 350: loss = 0.0501\n",
      "step 400: loss = 0.0488\n",
      "step 450: loss = 0.0494\n",
      "test_loss: 0.04804318428039551\n",
      "step 0: loss = 0.0497\n",
      "step 50: loss = 0.0461\n",
      "step 100: loss = 0.0458\n",
      "step 150: loss = 0.0491\n",
      "step 200: loss = 0.0483\n",
      "step 250: loss = 0.0485\n",
      "step 300: loss = 0.0494\n",
      "step 350: loss = 0.0481\n",
      "step 400: loss = 0.0483\n",
      "step 450: loss = 0.0462\n",
      "test_loss: 0.048108887672424314\n",
      "step 0: loss = 0.0483\n",
      "step 50: loss = 0.0476\n",
      "step 100: loss = 0.0492\n",
      "step 150: loss = 0.0481\n",
      "step 200: loss = 0.0490\n",
      "step 250: loss = 0.0502\n",
      "step 300: loss = 0.0478\n",
      "step 350: loss = 0.0497\n",
      "step 400: loss = 0.0458\n",
      "step 450: loss = 0.0486\n",
      "test_loss: 0.04779341220855713\n",
      "step 0: loss = 0.0479\n",
      "step 50: loss = 0.0473\n",
      "step 100: loss = 0.0476\n",
      "step 150: loss = 0.0456\n",
      "step 200: loss = 0.0474\n",
      "step 250: loss = 0.0493\n",
      "step 300: loss = 0.0475\n",
      "step 350: loss = 0.0482\n",
      "step 400: loss = 0.0455\n",
      "step 450: loss = 0.0479\n",
      "test_loss: 0.04819634914398194\n",
      "step 0: loss = 0.0493\n",
      "step 50: loss = 0.0490\n",
      "step 100: loss = 0.0474\n",
      "step 150: loss = 0.0449\n",
      "step 200: loss = 0.0487\n",
      "step 250: loss = 0.0469\n",
      "step 300: loss = 0.0482\n",
      "step 350: loss = 0.0487\n",
      "step 400: loss = 0.0477\n",
      "step 450: loss = 0.0446\n",
      "test_loss: 0.04799900531768799\n",
      "step 0: loss = 0.0474\n",
      "step 50: loss = 0.0460\n",
      "step 100: loss = 0.0508\n",
      "step 150: loss = 0.0491\n",
      "step 200: loss = 0.0482\n",
      "step 250: loss = 0.0477\n",
      "step 300: loss = 0.0480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350: loss = 0.0475\n",
      "step 400: loss = 0.0474\n",
      "step 450: loss = 0.0451\n",
      "test_loss: 0.0483718204498291\n",
      "step 0: loss = 0.0489\n",
      "step 50: loss = 0.0454\n",
      "step 100: loss = 0.0476\n",
      "step 150: loss = 0.0449\n",
      "step 200: loss = 0.0511\n",
      "step 250: loss = 0.0483\n",
      "step 300: loss = 0.0493\n",
      "step 350: loss = 0.0492\n",
      "step 400: loss = 0.0490\n",
      "step 450: loss = 0.0474\n",
      "test_loss: 0.048009943962097165\n",
      "step 0: loss = 0.0476\n",
      "step 50: loss = 0.0522\n",
      "step 100: loss = 0.0464\n",
      "step 150: loss = 0.0479\n",
      "step 200: loss = 0.0475\n",
      "step 250: loss = 0.0487\n",
      "step 300: loss = 0.0496\n",
      "step 350: loss = 0.0505\n",
      "step 400: loss = 0.0467\n",
      "step 450: loss = 0.0469\n",
      "test_loss: 0.04831006050109863\n",
      "step 0: loss = 0.0493\n",
      "step 50: loss = 0.0498\n",
      "step 100: loss = 0.0470\n",
      "step 150: loss = 0.0458\n",
      "step 200: loss = 0.0504\n",
      "step 250: loss = 0.0439\n",
      "step 300: loss = 0.0478\n",
      "step 350: loss = 0.0479\n",
      "step 400: loss = 0.0491\n",
      "step 450: loss = 0.0480\n",
      "test_loss: 0.048287730216979984\n",
      "step 0: loss = 0.0455\n",
      "step 50: loss = 0.0482\n",
      "step 100: loss = 0.0467\n",
      "step 150: loss = 0.0459\n",
      "step 200: loss = 0.0481\n",
      "step 250: loss = 0.0470\n",
      "step 300: loss = 0.0464\n",
      "step 350: loss = 0.0481\n",
      "step 400: loss = 0.0500\n",
      "step 450: loss = 0.0472\n",
      "test_loss: 0.04814311981201172\n",
      "step 0: loss = 0.0478\n",
      "step 50: loss = 0.0476\n",
      "step 100: loss = 0.0498\n",
      "step 150: loss = 0.0472\n",
      "step 200: loss = 0.0476\n",
      "step 250: loss = 0.0460\n",
      "step 300: loss = 0.0478\n",
      "step 350: loss = 0.0468\n",
      "step 400: loss = 0.0439\n",
      "step 450: loss = 0.0482\n",
      "test_loss: 0.048281712532043455\n",
      "step 0: loss = 0.0497\n",
      "step 50: loss = 0.0478\n",
      "step 100: loss = 0.0448\n",
      "step 150: loss = 0.0467\n",
      "step 200: loss = 0.0468\n",
      "step 250: loss = 0.0465\n",
      "step 300: loss = 0.0474\n",
      "step 350: loss = 0.0476\n",
      "step 400: loss = 0.0482\n",
      "step 450: loss = 0.0466\n",
      "test_loss: 0.048040666580200196\n",
      "step 0: loss = 0.0457\n",
      "step 50: loss = 0.0451\n",
      "step 100: loss = 0.0458\n",
      "step 150: loss = 0.0485\n",
      "step 200: loss = 0.0486\n",
      "step 250: loss = 0.0475\n",
      "step 300: loss = 0.0500\n",
      "step 350: loss = 0.0477\n",
      "step 400: loss = 0.0480\n",
      "step 450: loss = 0.0479\n",
      "test_loss: 0.0480842399597168\n",
      "step 0: loss = 0.0448\n",
      "step 50: loss = 0.0492\n",
      "step 100: loss = 0.0498\n",
      "step 150: loss = 0.0449\n",
      "step 200: loss = 0.0492\n",
      "step 250: loss = 0.0510\n",
      "step 300: loss = 0.0472\n",
      "step 350: loss = 0.0456\n",
      "step 400: loss = 0.0489\n",
      "step 450: loss = 0.0494\n",
      "test_loss: 0.04751243591308594\n",
      "step 0: loss = 0.0455\n",
      "step 50: loss = 0.0497\n",
      "step 100: loss = 0.0477\n",
      "step 150: loss = 0.0488\n",
      "step 200: loss = 0.0492\n",
      "step 250: loss = 0.0456\n",
      "step 300: loss = 0.0491\n",
      "step 350: loss = 0.0482\n",
      "step 400: loss = 0.0489\n",
      "step 450: loss = 0.0476\n",
      "test_loss: 0.04774299144744873\n",
      "step 0: loss = 0.0493\n",
      "step 50: loss = 0.0469\n",
      "step 100: loss = 0.0481\n",
      "step 150: loss = 0.0485\n",
      "step 200: loss = 0.0476\n",
      "step 250: loss = 0.0485\n",
      "step 300: loss = 0.0458\n",
      "step 350: loss = 0.0494\n",
      "step 400: loss = 0.0468\n",
      "step 450: loss = 0.0480\n",
      "test_loss: 0.04825950622558594\n",
      "step 0: loss = 0.0465\n",
      "step 50: loss = 0.0463\n",
      "step 100: loss = 0.0484\n",
      "step 150: loss = 0.0487\n",
      "step 200: loss = 0.0446\n",
      "step 250: loss = 0.0446\n",
      "step 300: loss = 0.0467\n",
      "step 350: loss = 0.0430\n",
      "step 400: loss = 0.0516\n",
      "step 450: loss = 0.0490\n",
      "test_loss: 0.048072519302368166\n",
      "step 0: loss = 0.0473\n",
      "step 50: loss = 0.0492\n",
      "step 100: loss = 0.0477\n",
      "step 150: loss = 0.0484\n",
      "step 200: loss = 0.0482\n",
      "step 250: loss = 0.0473\n",
      "step 300: loss = 0.0479\n",
      "step 350: loss = 0.0496\n",
      "step 400: loss = 0.0489\n",
      "step 450: loss = 0.0465\n",
      "test_loss: 0.04799184322357178\n",
      "step 0: loss = 0.0503\n",
      "step 50: loss = 0.0477\n",
      "step 100: loss = 0.0474\n",
      "step 150: loss = 0.0495\n",
      "step 200: loss = 0.0475\n",
      "step 250: loss = 0.0490\n",
      "step 300: loss = 0.0441\n",
      "step 350: loss = 0.0478\n",
      "step 400: loss = 0.0494\n",
      "step 450: loss = 0.0473\n",
      "test_loss: 0.04768505096435547\n",
      "step 0: loss = 0.0448\n",
      "step 50: loss = 0.0484\n",
      "step 100: loss = 0.0464\n",
      "step 150: loss = 0.0461\n",
      "step 200: loss = 0.0470\n",
      "step 250: loss = 0.0493\n",
      "step 300: loss = 0.0474\n",
      "step 350: loss = 0.0482\n",
      "step 400: loss = 0.0476\n",
      "step 450: loss = 0.0493\n",
      "test_loss: 0.04788767814636231\n",
      "step 0: loss = 0.0460\n",
      "step 50: loss = 0.0471\n",
      "step 100: loss = 0.0469\n",
      "step 150: loss = 0.0490\n",
      "step 200: loss = 0.0467\n",
      "step 250: loss = 0.0481\n",
      "step 300: loss = 0.0477\n",
      "step 350: loss = 0.0491\n",
      "step 400: loss = 0.0481\n",
      "step 450: loss = 0.0475\n",
      "test_loss: 0.04756118774414062\n",
      "step 0: loss = 0.0450\n",
      "step 50: loss = 0.0492\n",
      "step 100: loss = 0.0468\n",
      "step 150: loss = 0.0448\n",
      "step 200: loss = 0.0455\n",
      "step 250: loss = 0.0477\n",
      "step 300: loss = 0.0494\n",
      "step 350: loss = 0.0474\n",
      "step 400: loss = 0.0481\n",
      "step 450: loss = 0.0473\n",
      "test_loss: 0.04749159336090088\n",
      "step 0: loss = 0.0464\n",
      "step 50: loss = 0.0515\n",
      "step 100: loss = 0.0485\n",
      "step 150: loss = 0.0472\n",
      "step 200: loss = 0.0466\n",
      "step 250: loss = 0.0459\n",
      "step 300: loss = 0.0480\n",
      "step 350: loss = 0.0470\n",
      "step 400: loss = 0.0459\n",
      "step 450: loss = 0.0464\n",
      "test_loss: 0.04793976783752441\n",
      "step 0: loss = 0.0473\n",
      "step 50: loss = 0.0494\n",
      "step 100: loss = 0.0499\n",
      "step 150: loss = 0.0483\n",
      "step 200: loss = 0.0476\n",
      "step 250: loss = 0.0471\n",
      "step 300: loss = 0.0468\n",
      "step 350: loss = 0.0471\n",
      "step 400: loss = 0.0475\n",
      "step 450: loss = 0.0437\n",
      "test_loss: 0.04767360210418701\n",
      "step 0: loss = 0.0476\n",
      "step 50: loss = 0.0458\n",
      "step 100: loss = 0.0465\n",
      "step 150: loss = 0.0481\n",
      "step 200: loss = 0.0466\n",
      "step 250: loss = 0.0475\n",
      "step 300: loss = 0.0480\n",
      "step 350: loss = 0.0490\n",
      "step 400: loss = 0.0470\n",
      "step 450: loss = 0.0492\n",
      "test_loss: 0.047525148391723636\n",
      "step 0: loss = 0.0459\n",
      "step 50: loss = 0.0475\n",
      "step 100: loss = 0.0487\n",
      "step 150: loss = 0.0476\n",
      "step 200: loss = 0.0444\n",
      "step 250: loss = 0.0457\n",
      "step 300: loss = 0.0469\n",
      "step 350: loss = 0.0477\n",
      "step 400: loss = 0.0468\n",
      "step 450: loss = 0.0476\n",
      "test_loss: 0.04791885375976562\n",
      "step 0: loss = 0.0476\n",
      "step 50: loss = 0.0463\n",
      "step 100: loss = 0.0491\n",
      "step 150: loss = 0.0459\n",
      "step 200: loss = 0.0472\n",
      "step 250: loss = 0.0457\n",
      "step 300: loss = 0.0486\n",
      "step 350: loss = 0.0476\n",
      "step 400: loss = 0.0464\n",
      "step 450: loss = 0.0466\n",
      "test_loss: 0.04787506103515625\n",
      "step 0: loss = 0.0473\n",
      "step 50: loss = 0.0477\n",
      "step 100: loss = 0.0478\n",
      "step 150: loss = 0.0494\n",
      "step 200: loss = 0.0460\n",
      "step 250: loss = 0.0489\n",
      "step 300: loss = 0.0457\n",
      "step 350: loss = 0.0476\n",
      "step 400: loss = 0.0463\n",
      "step 450: loss = 0.0483\n",
      "test_loss: 0.04789077758789063\n",
      "step 0: loss = 0.0486\n",
      "step 50: loss = 0.0469\n",
      "step 100: loss = 0.0484\n",
      "step 150: loss = 0.0479\n",
      "step 200: loss = 0.0484\n",
      "step 250: loss = 0.0474\n",
      "step 300: loss = 0.0490\n",
      "step 350: loss = 0.0505\n",
      "step 400: loss = 0.0479\n",
      "step 450: loss = 0.0486\n",
      "test_loss: 0.04777552604675293\n",
      "step 0: loss = 0.0501\n",
      "step 50: loss = 0.0461\n",
      "step 100: loss = 0.0468\n",
      "step 150: loss = 0.0461\n",
      "step 200: loss = 0.0466\n",
      "step 250: loss = 0.0470\n",
      "step 300: loss = 0.0476\n",
      "step 350: loss = 0.0467\n",
      "step 400: loss = 0.0479\n",
      "step 450: loss = 0.0488\n",
      "test_loss: 0.04776891231536865\n",
      "step 0: loss = 0.0488\n",
      "step 50: loss = 0.0484\n",
      "step 100: loss = 0.0493\n",
      "step 150: loss = 0.0487\n",
      "step 200: loss = 0.0468\n",
      "step 250: loss = 0.0494\n",
      "step 300: loss = 0.0463\n",
      "step 350: loss = 0.0522\n",
      "step 400: loss = 0.0476\n",
      "step 450: loss = 0.0487\n",
      "test_loss: 0.04738879203796387\n",
      "step 0: loss = 0.0464\n",
      "step 50: loss = 0.0484\n",
      "step 100: loss = 0.0471\n",
      "step 150: loss = 0.0454\n",
      "step 200: loss = 0.0473\n",
      "step 250: loss = 0.0512\n",
      "step 300: loss = 0.0482\n",
      "step 350: loss = 0.0500\n",
      "step 400: loss = 0.0477\n",
      "step 450: loss = 0.0477\n",
      "test_loss: 0.047777419090270994\n",
      "step 0: loss = 0.0457\n",
      "step 50: loss = 0.0459\n",
      "step 100: loss = 0.0457\n",
      "step 150: loss = 0.0495\n",
      "step 200: loss = 0.0455\n",
      "step 250: loss = 0.0453\n",
      "step 300: loss = 0.0444\n",
      "step 350: loss = 0.0464\n",
      "step 400: loss = 0.0476\n",
      "step 450: loss = 0.0479\n",
      "test_loss: 0.0473927640914917\n",
      "step 0: loss = 0.0469\n",
      "step 50: loss = 0.0478\n",
      "step 100: loss = 0.0478\n",
      "step 150: loss = 0.0476\n",
      "step 200: loss = 0.0464\n",
      "step 250: loss = 0.0486\n",
      "step 300: loss = 0.0508\n",
      "step 350: loss = 0.0477\n",
      "step 400: loss = 0.0465\n",
      "step 450: loss = 0.0473\n",
      "test_loss: 0.047703604698181155\n",
      "step 0: loss = 0.0466\n",
      "step 50: loss = 0.0468\n",
      "step 100: loss = 0.0480\n",
      "step 150: loss = 0.0460\n",
      "step 200: loss = 0.0481\n",
      "step 250: loss = 0.0476\n",
      "step 300: loss = 0.0478\n",
      "step 350: loss = 0.0478\n",
      "step 400: loss = 0.0506\n",
      "step 450: loss = 0.0491\n",
      "test_loss: 0.047926445007324216\n",
      "step 0: loss = 0.0476\n",
      "step 50: loss = 0.0478\n",
      "step 100: loss = 0.0466\n",
      "step 150: loss = 0.0503\n",
      "step 200: loss = 0.0479\n",
      "step 250: loss = 0.0498\n",
      "step 300: loss = 0.0489\n",
      "step 350: loss = 0.0469\n",
      "step 400: loss = 0.0451\n",
      "step 450: loss = 0.0495\n",
      "test_loss: 0.04776912212371826\n",
      "step 0: loss = 0.0439\n",
      "step 50: loss = 0.0467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 100: loss = 0.0471\n",
      "step 150: loss = 0.0475\n",
      "step 200: loss = 0.0469\n",
      "step 250: loss = 0.0460\n",
      "step 300: loss = 0.0484\n",
      "step 350: loss = 0.0490\n",
      "step 400: loss = 0.0455\n",
      "step 450: loss = 0.0494\n",
      "test_loss: 0.047554821968078614\n",
      "step 0: loss = 0.0458\n",
      "step 50: loss = 0.0481\n",
      "step 100: loss = 0.0474\n",
      "step 150: loss = 0.0458\n",
      "step 200: loss = 0.0461\n",
      "step 250: loss = 0.0464\n",
      "step 300: loss = 0.0492\n",
      "step 350: loss = 0.0496\n",
      "step 400: loss = 0.0470\n",
      "step 450: loss = 0.0475\n",
      "test_loss: 0.047812151908874514\n",
      "step 0: loss = 0.0475\n",
      "step 50: loss = 0.0448\n",
      "step 100: loss = 0.0490\n",
      "step 150: loss = 0.0491\n",
      "step 200: loss = 0.0454\n",
      "step 250: loss = 0.0501\n",
      "step 300: loss = 0.0460\n",
      "step 350: loss = 0.0455\n",
      "step 400: loss = 0.0471\n",
      "step 450: loss = 0.0453\n",
      "test_loss: 0.047446908950805666\n",
      "step 0: loss = 0.0499\n",
      "step 50: loss = 0.0489\n",
      "step 100: loss = 0.0493\n",
      "step 150: loss = 0.0471\n",
      "step 200: loss = 0.0465\n",
      "step 250: loss = 0.0467\n",
      "step 300: loss = 0.0496\n",
      "step 350: loss = 0.0490\n",
      "step 400: loss = 0.0486\n",
      "step 450: loss = 0.0481\n",
      "test_loss: 0.047655706405639646\n",
      "step 0: loss = 0.0473\n",
      "step 50: loss = 0.0471\n",
      "step 100: loss = 0.0486\n",
      "step 150: loss = 0.0470\n",
      "step 200: loss = 0.0470\n",
      "step 250: loss = 0.0436\n",
      "step 300: loss = 0.0466\n",
      "step 350: loss = 0.0462\n",
      "step 400: loss = 0.0484\n",
      "step 450: loss = 0.0478\n",
      "test_loss: 0.04776406288146973\n",
      "step 0: loss = 0.0501\n",
      "step 50: loss = 0.0494\n",
      "step 100: loss = 0.0472\n",
      "step 150: loss = 0.0472\n",
      "step 200: loss = 0.0461\n",
      "step 250: loss = 0.0466\n",
      "step 300: loss = 0.0489\n",
      "step 350: loss = 0.0495\n",
      "step 400: loss = 0.0490\n",
      "step 450: loss = 0.0454\n",
      "test_loss: 0.04776077747344971\n",
      "step 0: loss = 0.0480\n",
      "step 50: loss = 0.0490\n",
      "step 100: loss = 0.0473\n",
      "step 150: loss = 0.0478\n",
      "step 200: loss = 0.0466\n",
      "step 250: loss = 0.0458\n",
      "step 300: loss = 0.0452\n",
      "step 350: loss = 0.0453\n",
      "step 400: loss = 0.0473\n",
      "step 450: loss = 0.0488\n",
      "test_loss: 0.047349467277526855\n",
      "step 0: loss = 0.0464\n",
      "step 50: loss = 0.0467\n",
      "step 100: loss = 0.0479\n",
      "step 150: loss = 0.0458\n",
      "step 200: loss = 0.0460\n",
      "step 250: loss = 0.0499\n",
      "step 300: loss = 0.0452\n",
      "step 350: loss = 0.0477\n",
      "step 400: loss = 0.0473\n",
      "step 450: loss = 0.0441\n",
      "test_loss: 0.047396764755249024\n",
      "step 0: loss = 0.0486\n",
      "step 50: loss = 0.0485\n",
      "step 100: loss = 0.0464\n",
      "step 150: loss = 0.0485\n",
      "step 200: loss = 0.0483\n",
      "step 250: loss = 0.0488\n",
      "step 300: loss = 0.0442\n",
      "step 350: loss = 0.0502\n",
      "step 400: loss = 0.0491\n",
      "step 450: loss = 0.0468\n",
      "test_loss: 0.047533149719238284\n",
      "step 0: loss = 0.0484\n",
      "step 50: loss = 0.0474\n",
      "step 100: loss = 0.0477\n",
      "step 150: loss = 0.0475\n",
      "step 200: loss = 0.0480\n",
      "step 250: loss = 0.0467\n",
      "step 300: loss = 0.0480\n",
      "step 350: loss = 0.0485\n",
      "step 400: loss = 0.0461\n",
      "step 450: loss = 0.0468\n",
      "test_loss: 0.047457127571105956\n",
      "step 0: loss = 0.0473\n",
      "step 50: loss = 0.0489\n",
      "step 100: loss = 0.0469\n",
      "step 150: loss = 0.0454\n",
      "step 200: loss = 0.0470\n",
      "step 250: loss = 0.0480\n",
      "step 300: loss = 0.0463\n",
      "step 350: loss = 0.0465\n",
      "step 400: loss = 0.0482\n",
      "step 450: loss = 0.0475\n",
      "test_loss: 0.047575798034667965\n",
      "step 0: loss = 0.0440\n",
      "step 50: loss = 0.0441\n",
      "step 100: loss = 0.0448\n",
      "step 150: loss = 0.0475\n",
      "step 200: loss = 0.0471\n",
      "step 250: loss = 0.0478\n",
      "step 300: loss = 0.0460\n",
      "step 350: loss = 0.0454\n",
      "step 400: loss = 0.0479\n",
      "step 450: loss = 0.0468\n",
      "test_loss: 0.047532296180725096\n",
      "step 0: loss = 0.0443\n",
      "step 50: loss = 0.0498\n",
      "step 100: loss = 0.0480\n",
      "step 150: loss = 0.0476\n",
      "step 200: loss = 0.0474\n",
      "step 250: loss = 0.0473\n",
      "step 300: loss = 0.0482\n",
      "step 350: loss = 0.0484\n",
      "step 400: loss = 0.0491\n",
      "step 450: loss = 0.0493\n",
      "test_loss: 0.047223567962646484\n",
      "step 0: loss = 0.0465\n",
      "step 50: loss = 0.0477\n",
      "step 100: loss = 0.0469\n",
      "step 150: loss = 0.0489\n",
      "step 200: loss = 0.0467\n",
      "step 250: loss = 0.0460\n",
      "step 300: loss = 0.0476\n",
      "step 350: loss = 0.0471\n",
      "step 400: loss = 0.0486\n",
      "step 450: loss = 0.0460\n",
      "test_loss: 0.04766981601715088\n",
      "step 0: loss = 0.0484\n",
      "step 50: loss = 0.0474\n",
      "step 100: loss = 0.0511\n",
      "step 150: loss = 0.0485\n",
      "step 200: loss = 0.0477\n",
      "step 250: loss = 0.0477\n",
      "step 300: loss = 0.0460\n",
      "step 350: loss = 0.0473\n",
      "step 400: loss = 0.0473\n",
      "step 450: loss = 0.0452\n",
      "test_loss: 0.04781110286712646\n",
      "step 0: loss = 0.0494\n",
      "step 50: loss = 0.0490\n",
      "step 100: loss = 0.0467\n",
      "step 150: loss = 0.0467\n",
      "step 200: loss = 0.0467\n",
      "step 250: loss = 0.0480\n",
      "step 300: loss = 0.0449\n",
      "step 350: loss = 0.0469\n",
      "step 400: loss = 0.0482\n",
      "step 450: loss = 0.0485\n",
      "test_loss: 0.04736190795898437\n",
      "step 0: loss = 0.0463\n",
      "step 50: loss = 0.0459\n",
      "step 100: loss = 0.0462\n",
      "step 150: loss = 0.0478\n",
      "step 200: loss = 0.0482\n",
      "step 250: loss = 0.0468\n",
      "step 300: loss = 0.0482\n",
      "step 350: loss = 0.0460\n",
      "step 400: loss = 0.0495\n",
      "step 450: loss = 0.0466\n",
      "test_loss: 0.04734313011169434\n",
      "step 0: loss = 0.0498\n",
      "step 50: loss = 0.0515\n",
      "step 100: loss = 0.0455\n",
      "step 150: loss = 0.0493\n",
      "step 200: loss = 0.0457\n",
      "step 250: loss = 0.0486\n",
      "step 300: loss = 0.0461\n",
      "step 350: loss = 0.0455\n",
      "step 400: loss = 0.0464\n",
      "step 450: loss = 0.0475\n",
      "test_loss: 0.04769140720367432\n",
      "step 0: loss = 0.0498\n",
      "step 50: loss = 0.0481\n",
      "step 100: loss = 0.0473\n",
      "step 150: loss = 0.0472\n",
      "step 200: loss = 0.0489\n",
      "step 250: loss = 0.0462\n",
      "step 300: loss = 0.0475\n",
      "step 350: loss = 0.0449\n",
      "step 400: loss = 0.0474\n",
      "step 450: loss = 0.0448\n",
      "test_loss: 0.04746767044067383\n",
      "step 0: loss = 0.0467\n",
      "step 50: loss = 0.0497\n",
      "step 100: loss = 0.0481\n",
      "step 150: loss = 0.0464\n",
      "step 200: loss = 0.0461\n",
      "step 250: loss = 0.0487\n",
      "step 300: loss = 0.0481\n",
      "step 350: loss = 0.0454\n",
      "step 400: loss = 0.0460\n",
      "step 450: loss = 0.0466\n",
      "test_loss: 0.04726974487304687\n",
      "step 0: loss = 0.0477\n",
      "step 50: loss = 0.0486\n",
      "step 100: loss = 0.0458\n",
      "step 150: loss = 0.0478\n",
      "step 200: loss = 0.0468\n",
      "step 250: loss = 0.0505\n",
      "step 300: loss = 0.0468\n",
      "step 350: loss = 0.0466\n",
      "step 400: loss = 0.0491\n",
      "step 450: loss = 0.0457\n",
      "test_loss: 0.04733930587768555\n",
      "step 0: loss = 0.0481\n",
      "step 50: loss = 0.0467\n",
      "step 100: loss = 0.0442\n",
      "step 150: loss = 0.0462\n",
      "step 200: loss = 0.0455\n",
      "step 250: loss = 0.0479\n",
      "step 300: loss = 0.0480\n",
      "step 350: loss = 0.0501\n",
      "step 400: loss = 0.0458\n",
      "step 450: loss = 0.0478\n",
      "test_loss: 0.04721744060516357\n",
      "step 0: loss = 0.0473\n",
      "step 50: loss = 0.0476\n",
      "step 100: loss = 0.0487\n",
      "step 150: loss = 0.0478\n",
      "step 200: loss = 0.0462\n",
      "step 250: loss = 0.0496\n",
      "step 300: loss = 0.0486\n",
      "step 350: loss = 0.0450\n",
      "step 400: loss = 0.0469\n",
      "step 450: loss = 0.0468\n",
      "test_loss: 0.04723003387451172\n",
      "step 0: loss = 0.0474\n",
      "step 50: loss = 0.0486\n",
      "step 100: loss = 0.0455\n",
      "step 150: loss = 0.0494\n",
      "step 200: loss = 0.0476\n",
      "step 250: loss = 0.0484\n",
      "step 300: loss = 0.0478\n",
      "step 350: loss = 0.0486\n",
      "step 400: loss = 0.0449\n",
      "step 450: loss = 0.0479\n",
      "test_loss: 0.047297215461730956\n",
      "step 0: loss = 0.0469\n",
      "step 50: loss = 0.0479\n",
      "step 100: loss = 0.0489\n",
      "step 150: loss = 0.0457\n",
      "step 200: loss = 0.0498\n",
      "step 250: loss = 0.0459\n",
      "step 300: loss = 0.0459\n",
      "step 350: loss = 0.0512\n",
      "step 400: loss = 0.0503\n",
      "step 450: loss = 0.0461\n",
      "test_loss: 0.04763514518737793\n",
      "step 0: loss = 0.0459\n",
      "step 50: loss = 0.0465\n",
      "step 100: loss = 0.0461\n",
      "step 150: loss = 0.0468\n",
      "step 200: loss = 0.0483\n",
      "step 250: loss = 0.0472\n",
      "step 300: loss = 0.0472\n",
      "step 350: loss = 0.0479\n",
      "step 400: loss = 0.0450\n",
      "step 450: loss = 0.0476\n",
      "test_loss: 0.04723756313323975\n",
      "step 0: loss = 0.0443\n",
      "step 50: loss = 0.0480\n",
      "step 100: loss = 0.0457\n",
      "step 150: loss = 0.0486\n",
      "step 200: loss = 0.0464\n",
      "step 250: loss = 0.0453\n",
      "step 300: loss = 0.0467\n",
      "step 350: loss = 0.0484\n",
      "step 400: loss = 0.0500\n",
      "step 450: loss = 0.0499\n",
      "test_loss: 0.04746640682220459\n",
      "step 0: loss = 0.0475\n",
      "step 50: loss = 0.0464\n",
      "step 100: loss = 0.0469\n",
      "step 150: loss = 0.0469\n",
      "step 200: loss = 0.0472\n",
      "step 250: loss = 0.0471\n",
      "step 300: loss = 0.0483\n",
      "step 350: loss = 0.0475\n",
      "step 400: loss = 0.0451\n",
      "step 450: loss = 0.0468\n",
      "test_loss: 0.047669997215270994\n",
      "step 0: loss = 0.0473\n",
      "step 50: loss = 0.0464\n",
      "step 100: loss = 0.0461\n",
      "step 150: loss = 0.0460\n",
      "step 200: loss = 0.0459\n",
      "step 250: loss = 0.0470\n",
      "step 300: loss = 0.0478\n",
      "step 350: loss = 0.0481\n",
      "step 400: loss = 0.0465\n",
      "step 450: loss = 0.0467\n",
      "test_loss: 0.047285213470458984\n",
      "step 0: loss = 0.0455\n",
      "step 50: loss = 0.0488\n",
      "step 100: loss = 0.0455\n",
      "step 150: loss = 0.0467\n",
      "step 200: loss = 0.0461\n",
      "step 250: loss = 0.0461\n",
      "step 300: loss = 0.0470\n",
      "step 350: loss = 0.0471\n",
      "step 400: loss = 0.0464\n",
      "step 450: loss = 0.0461\n",
      "test_loss: 0.047474632263183596\n",
      "step 0: loss = 0.0480\n",
      "step 50: loss = 0.0461\n",
      "step 100: loss = 0.0484\n",
      "step 150: loss = 0.0487\n",
      "step 200: loss = 0.0481\n",
      "step 250: loss = 0.0464\n",
      "step 300: loss = 0.0495\n",
      "step 350: loss = 0.0460\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 400: loss = 0.0505\n",
      "step 450: loss = 0.0479\n",
      "test_loss: 0.047130393981933597\n",
      "step 0: loss = 0.0461\n",
      "step 50: loss = 0.0450\n",
      "step 100: loss = 0.0470\n",
      "step 150: loss = 0.0472\n",
      "step 200: loss = 0.0456\n",
      "step 250: loss = 0.0471\n",
      "step 300: loss = 0.0448\n",
      "step 350: loss = 0.0462\n",
      "step 400: loss = 0.0503\n",
      "step 450: loss = 0.0479\n",
      "test_loss: 0.04742093563079834\n",
      "step 0: loss = 0.0477\n",
      "step 50: loss = 0.0476\n",
      "step 100: loss = 0.0451\n",
      "step 150: loss = 0.0468\n",
      "step 200: loss = 0.0451\n",
      "step 250: loss = 0.0464\n",
      "step 300: loss = 0.0467\n",
      "step 350: loss = 0.0483\n",
      "step 400: loss = 0.0447\n",
      "step 450: loss = 0.0476\n",
      "test_loss: 0.047083349227905275\n",
      "step 0: loss = 0.0481\n",
      "step 50: loss = 0.0476\n",
      "step 100: loss = 0.0458\n",
      "step 150: loss = 0.0471\n",
      "step 200: loss = 0.0488\n",
      "step 250: loss = 0.0475\n",
      "step 300: loss = 0.0464\n",
      "step 350: loss = 0.0478\n",
      "step 400: loss = 0.0480\n",
      "step 450: loss = 0.0489\n",
      "test_loss: 0.047315669059753415\n",
      "step 0: loss = 0.0443\n",
      "step 50: loss = 0.0471\n",
      "step 100: loss = 0.0477\n",
      "step 150: loss = 0.0491\n",
      "step 200: loss = 0.0471\n",
      "step 250: loss = 0.0471\n",
      "step 300: loss = 0.0484\n",
      "step 350: loss = 0.0465\n",
      "step 400: loss = 0.0489\n",
      "step 450: loss = 0.0467\n",
      "test_loss: 0.0476105785369873\n",
      "step 0: loss = 0.0485\n",
      "step 50: loss = 0.0455\n",
      "step 100: loss = 0.0493\n",
      "step 150: loss = 0.0470\n",
      "step 200: loss = 0.0464\n",
      "step 250: loss = 0.0469\n",
      "step 300: loss = 0.0467\n",
      "step 350: loss = 0.0463\n",
      "step 400: loss = 0.0478\n",
      "step 450: loss = 0.0477\n",
      "test_loss: 0.04712965488433838\n",
      "step 0: loss = 0.0454\n",
      "step 50: loss = 0.0468\n",
      "step 100: loss = 0.0456\n",
      "step 150: loss = 0.0480\n",
      "step 200: loss = 0.0466\n",
      "step 250: loss = 0.0477\n",
      "step 300: loss = 0.0469\n",
      "step 350: loss = 0.0463\n",
      "step 400: loss = 0.0477\n",
      "step 450: loss = 0.0467\n",
      "test_loss: 0.04737315654754639\n",
      "step 0: loss = 0.0476\n",
      "step 50: loss = 0.0472\n",
      "step 100: loss = 0.0480\n",
      "step 150: loss = 0.0464\n",
      "step 200: loss = 0.0474\n",
      "step 250: loss = 0.0475\n",
      "step 300: loss = 0.0458\n",
      "step 350: loss = 0.0478\n",
      "step 400: loss = 0.0481\n",
      "step 450: loss = 0.0493\n",
      "test_loss: 0.04760315895080566\n",
      "step 0: loss = 0.0479\n",
      "step 50: loss = 0.0467\n",
      "step 100: loss = 0.0459\n",
      "step 150: loss = 0.0485\n",
      "step 200: loss = 0.0482\n",
      "step 250: loss = 0.0478\n",
      "step 300: loss = 0.0458\n",
      "step 350: loss = 0.0473\n",
      "step 400: loss = 0.0465\n",
      "step 450: loss = 0.0486\n",
      "test_loss: 0.04714931964874267\n",
      "step 0: loss = 0.0456\n",
      "step 50: loss = 0.0462\n",
      "step 100: loss = 0.0454\n",
      "step 150: loss = 0.0461\n",
      "step 200: loss = 0.0490\n",
      "step 250: loss = 0.0445\n",
      "step 300: loss = 0.0487\n",
      "step 350: loss = 0.0460\n",
      "step 400: loss = 0.0474\n",
      "step 450: loss = 0.0475\n",
      "test_loss: 0.04741769313812256\n",
      "step 0: loss = 0.0473\n",
      "step 50: loss = 0.0473\n",
      "step 100: loss = 0.0457\n",
      "step 150: loss = 0.0477\n",
      "step 200: loss = 0.0482\n",
      "step 250: loss = 0.0479\n",
      "step 300: loss = 0.0474\n",
      "step 350: loss = 0.0453\n",
      "step 400: loss = 0.0452\n",
      "step 450: loss = 0.0461\n",
      "test_loss: 0.04747440814971924\n",
      "step 0: loss = 0.0460\n",
      "step 50: loss = 0.0473\n",
      "step 100: loss = 0.0475\n",
      "step 150: loss = 0.0492\n",
      "step 200: loss = 0.0483\n",
      "step 250: loss = 0.0494\n",
      "step 300: loss = 0.0459\n",
      "step 350: loss = 0.0480\n",
      "step 400: loss = 0.0460\n",
      "step 450: loss = 0.0459\n",
      "test_loss: 0.04754300594329834\n",
      "step 0: loss = 0.0482\n",
      "step 50: loss = 0.0480\n",
      "step 100: loss = 0.0466\n",
      "step 150: loss = 0.0464\n",
      "step 200: loss = 0.0452\n",
      "step 250: loss = 0.0490\n",
      "step 300: loss = 0.0470\n",
      "step 350: loss = 0.0450\n",
      "step 400: loss = 0.0441\n",
      "step 450: loss = 0.0464\n",
      "test_loss: 0.047201104164123535\n",
      "step 0: loss = 0.0452\n",
      "step 50: loss = 0.0467\n",
      "step 100: loss = 0.0490\n",
      "step 150: loss = 0.0484\n",
      "step 200: loss = 0.0469\n",
      "step 250: loss = 0.0457\n",
      "step 300: loss = 0.0480\n",
      "step 350: loss = 0.0453\n",
      "step 400: loss = 0.0454\n",
      "step 450: loss = 0.0468\n",
      "test_loss: 0.047052016258239744\n",
      "step 0: loss = 0.0450\n",
      "step 50: loss = 0.0459\n",
      "step 100: loss = 0.0474\n",
      "step 150: loss = 0.0455\n",
      "step 200: loss = 0.0472\n",
      "step 250: loss = 0.0469\n",
      "step 300: loss = 0.0467\n",
      "step 350: loss = 0.0455\n",
      "step 400: loss = 0.0482\n",
      "step 450: loss = 0.0466\n",
      "test_loss: 0.047521018981933595\n",
      "step 0: loss = 0.0498\n",
      "step 50: loss = 0.0485\n",
      "step 100: loss = 0.0485\n",
      "step 150: loss = 0.0474\n",
      "step 200: loss = 0.0484\n",
      "step 250: loss = 0.0481\n",
      "step 300: loss = 0.0472\n",
      "step 350: loss = 0.0469\n",
      "step 400: loss = 0.0457\n",
      "step 450: loss = 0.0469\n",
      "test_loss: 0.04708127021789551\n",
      "step 0: loss = 0.0450\n",
      "step 50: loss = 0.0475\n",
      "step 100: loss = 0.0485\n",
      "step 150: loss = 0.0458\n",
      "step 200: loss = 0.0465\n",
      "step 250: loss = 0.0464\n",
      "step 300: loss = 0.0468\n",
      "step 350: loss = 0.0440\n",
      "step 400: loss = 0.0466\n",
      "step 450: loss = 0.0488\n",
      "test_loss: 0.04730708599090576\n",
      "step 0: loss = 0.0469\n",
      "step 50: loss = 0.0455\n",
      "step 100: loss = 0.0492\n",
      "step 150: loss = 0.0467\n",
      "step 200: loss = 0.0474\n",
      "step 250: loss = 0.0488\n",
      "step 300: loss = 0.0481\n",
      "step 350: loss = 0.0443\n",
      "step 400: loss = 0.0476\n",
      "step 450: loss = 0.0480\n",
      "test_loss: 0.047265844345092775\n",
      "step 0: loss = 0.0486\n",
      "step 50: loss = 0.0471\n",
      "step 100: loss = 0.0466\n",
      "step 150: loss = 0.0488\n",
      "step 200: loss = 0.0473\n",
      "step 250: loss = 0.0467\n",
      "step 300: loss = 0.0474\n",
      "step 350: loss = 0.0480\n",
      "step 400: loss = 0.0478\n",
      "step 450: loss = 0.0494\n",
      "test_loss: 0.047131223678588866\n",
      "step 0: loss = 0.0458\n",
      "step 50: loss = 0.0473\n",
      "step 100: loss = 0.0496\n",
      "step 150: loss = 0.0486\n",
      "step 200: loss = 0.0455\n",
      "step 250: loss = 0.0459\n",
      "step 300: loss = 0.0484\n",
      "step 350: loss = 0.0443\n",
      "step 400: loss = 0.0464\n",
      "step 450: loss = 0.0458\n",
      "test_loss: 0.04718104362487793\n",
      "step 0: loss = 0.0486\n",
      "step 50: loss = 0.0454\n",
      "step 100: loss = 0.0471\n",
      "step 150: loss = 0.0481\n",
      "step 200: loss = 0.0477\n",
      "step 250: loss = 0.0492\n",
      "step 300: loss = 0.0476\n",
      "step 350: loss = 0.0470\n",
      "step 400: loss = 0.0495\n",
      "step 450: loss = 0.0458\n",
      "test_loss: 0.04743885040283203\n",
      "step 0: loss = 0.0437\n",
      "step 50: loss = 0.0439\n",
      "step 100: loss = 0.0491\n",
      "step 150: loss = 0.0470\n",
      "step 200: loss = 0.0476\n",
      "step 250: loss = 0.0473\n",
      "step 300: loss = 0.0452\n",
      "step 350: loss = 0.0457\n",
      "step 400: loss = 0.0439\n",
      "step 450: loss = 0.0475\n",
      "test_loss: 0.047349529266357424\n",
      "step 0: loss = 0.0485\n",
      "step 50: loss = 0.0466\n",
      "step 100: loss = 0.0456\n",
      "step 150: loss = 0.0446\n",
      "step 200: loss = 0.0471\n",
      "step 250: loss = 0.0489\n",
      "step 300: loss = 0.0499\n",
      "step 350: loss = 0.0461\n",
      "step 400: loss = 0.0452\n",
      "step 450: loss = 0.0476\n",
      "test_loss: 0.04747098445892334\n",
      "step 0: loss = 0.0493\n",
      "step 50: loss = 0.0448\n",
      "step 100: loss = 0.0443\n",
      "step 150: loss = 0.0464\n",
      "step 200: loss = 0.0487\n",
      "step 250: loss = 0.0465\n",
      "step 300: loss = 0.0477\n",
      "step 350: loss = 0.0447\n",
      "step 400: loss = 0.0502\n",
      "step 450: loss = 0.0448\n",
      "test_loss: 0.047040252685546874\n",
      "step 0: loss = 0.0451\n",
      "step 50: loss = 0.0465\n",
      "step 100: loss = 0.0466\n",
      "step 150: loss = 0.0482\n",
      "step 200: loss = 0.0473\n",
      "step 250: loss = 0.0469\n",
      "step 300: loss = 0.0477\n",
      "step 350: loss = 0.0488\n",
      "step 400: loss = 0.0473\n",
      "step 450: loss = 0.0506\n",
      "test_loss: 0.04717901706695557\n",
      "step 0: loss = 0.0477\n",
      "step 50: loss = 0.0479\n",
      "step 100: loss = 0.0470\n",
      "step 150: loss = 0.0445\n",
      "step 200: loss = 0.0479\n",
      "step 250: loss = 0.0483\n",
      "step 300: loss = 0.0477\n",
      "step 350: loss = 0.0460\n",
      "step 400: loss = 0.0449\n",
      "step 450: loss = 0.0465\n",
      "test_loss: 0.04720044612884521\n",
      "step 0: loss = 0.0462\n",
      "step 50: loss = 0.0451\n",
      "step 100: loss = 0.0472\n",
      "step 150: loss = 0.0468\n",
      "step 200: loss = 0.0466\n",
      "step 250: loss = 0.0452\n",
      "step 300: loss = 0.0465\n",
      "step 350: loss = 0.0455\n",
      "step 400: loss = 0.0490\n",
      "step 450: loss = 0.0486\n",
      "test_loss: 0.046875348091125486\n",
      "step 0: loss = 0.0485\n",
      "step 50: loss = 0.0499\n",
      "step 100: loss = 0.0450\n",
      "step 150: loss = 0.0466\n",
      "step 200: loss = 0.0467\n",
      "step 250: loss = 0.0473\n",
      "step 300: loss = 0.0457\n",
      "step 350: loss = 0.0447\n",
      "step 400: loss = 0.0466\n",
      "step 450: loss = 0.0451\n",
      "test_loss: 0.04722296714782715\n",
      "step 0: loss = 0.0490\n",
      "step 50: loss = 0.0483\n",
      "step 100: loss = 0.0467\n",
      "step 150: loss = 0.0455\n",
      "step 200: loss = 0.0477\n",
      "step 250: loss = 0.0459\n",
      "step 300: loss = 0.0472\n",
      "step 350: loss = 0.0456\n",
      "step 400: loss = 0.0475\n",
      "step 450: loss = 0.0465\n",
      "test_loss: 0.04721066951751709\n",
      "step 0: loss = 0.0463\n",
      "step 50: loss = 0.0479\n",
      "step 100: loss = 0.0471\n",
      "step 150: loss = 0.0464\n",
      "step 200: loss = 0.0485\n",
      "step 250: loss = 0.0466\n",
      "step 300: loss = 0.0484\n",
      "step 350: loss = 0.0489\n",
      "step 400: loss = 0.0463\n",
      "step 450: loss = 0.0471\n",
      "test_loss: 0.04736368179321289\n",
      "step 0: loss = 0.0455\n",
      "step 50: loss = 0.0451\n",
      "step 100: loss = 0.0486\n",
      "step 150: loss = 0.0453\n",
      "step 200: loss = 0.0453\n",
      "step 250: loss = 0.0444\n",
      "step 300: loss = 0.0478\n",
      "step 350: loss = 0.0488\n",
      "step 400: loss = 0.0477\n",
      "step 450: loss = 0.0468\n",
      "test_loss: 0.046898417472839356\n",
      "step 0: loss = 0.0467\n",
      "step 50: loss = 0.0464\n",
      "step 100: loss = 0.0454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 150: loss = 0.0451\n",
      "step 200: loss = 0.0494\n",
      "step 250: loss = 0.0462\n",
      "step 300: loss = 0.0490\n",
      "step 350: loss = 0.0496\n",
      "step 400: loss = 0.0477\n",
      "step 450: loss = 0.0469\n",
      "test_loss: 0.0473290491104126\n",
      "step 0: loss = 0.0462\n",
      "step 50: loss = 0.0458\n",
      "step 100: loss = 0.0471\n",
      "step 150: loss = 0.0480\n",
      "step 200: loss = 0.0462\n",
      "step 250: loss = 0.0482\n",
      "step 300: loss = 0.0459\n",
      "step 350: loss = 0.0443\n",
      "step 400: loss = 0.0461\n",
      "step 450: loss = 0.0477\n",
      "test_loss: 0.04672904968261719\n",
      "step 0: loss = 0.0464\n",
      "step 50: loss = 0.0480\n",
      "step 100: loss = 0.0468\n",
      "step 150: loss = 0.0456\n",
      "step 200: loss = 0.0458\n",
      "step 250: loss = 0.0472\n",
      "step 300: loss = 0.0465\n",
      "step 350: loss = 0.0459\n",
      "step 400: loss = 0.0459\n",
      "step 450: loss = 0.0468\n",
      "test_loss: 0.04687005043029785\n",
      "step 0: loss = 0.0455\n",
      "step 50: loss = 0.0483\n",
      "step 100: loss = 0.0444\n",
      "step 150: loss = 0.0464\n",
      "step 200: loss = 0.0447\n",
      "step 250: loss = 0.0459\n",
      "step 300: loss = 0.0463\n",
      "step 350: loss = 0.0502\n",
      "step 400: loss = 0.0470\n",
      "step 450: loss = 0.0487\n",
      "test_loss: 0.04742574691772461\n",
      "step 0: loss = 0.0480\n",
      "step 50: loss = 0.0461\n",
      "step 100: loss = 0.0484\n",
      "step 150: loss = 0.0477\n",
      "step 200: loss = 0.0490\n",
      "step 250: loss = 0.0486\n",
      "step 300: loss = 0.0484\n",
      "step 350: loss = 0.0481\n",
      "step 400: loss = 0.0478\n",
      "step 450: loss = 0.0472\n",
      "test_loss: 0.0471816349029541\n",
      "step 0: loss = 0.0453\n",
      "step 50: loss = 0.0462\n",
      "step 100: loss = 0.0502\n",
      "step 150: loss = 0.0459\n",
      "step 200: loss = 0.0472\n",
      "step 250: loss = 0.0455\n",
      "step 300: loss = 0.0467\n",
      "step 350: loss = 0.0452\n",
      "step 400: loss = 0.0467\n",
      "step 450: loss = 0.0480\n",
      "test_loss: 0.047063045501708985\n",
      "step 0: loss = 0.0461\n",
      "step 50: loss = 0.0453\n",
      "step 100: loss = 0.0475\n",
      "step 150: loss = 0.0463\n",
      "step 200: loss = 0.0444\n",
      "step 250: loss = 0.0460\n",
      "step 300: loss = 0.0450\n",
      "step 350: loss = 0.0500\n",
      "step 400: loss = 0.0494\n",
      "step 450: loss = 0.0464\n",
      "test_loss: 0.04701399803161621\n",
      "step 0: loss = 0.0467\n",
      "step 50: loss = 0.0474\n",
      "step 100: loss = 0.0476\n",
      "step 150: loss = 0.0451\n",
      "step 200: loss = 0.0493\n",
      "step 250: loss = 0.0464\n",
      "step 300: loss = 0.0470\n",
      "step 350: loss = 0.0462\n",
      "step 400: loss = 0.0457\n",
      "step 450: loss = 0.0464\n",
      "test_loss: 0.047050070762634275\n",
      "step 0: loss = 0.0479\n",
      "step 50: loss = 0.0439\n",
      "step 100: loss = 0.0481\n",
      "step 150: loss = 0.0478\n",
      "step 200: loss = 0.0487\n",
      "step 250: loss = 0.0455\n",
      "step 300: loss = 0.0465\n",
      "step 350: loss = 0.0463\n",
      "step 400: loss = 0.0486\n",
      "step 450: loss = 0.0443\n",
      "test_loss: 0.04702548027038574\n",
      "step 0: loss = 0.0484\n",
      "step 50: loss = 0.0484\n",
      "step 100: loss = 0.0479\n",
      "step 150: loss = 0.0491\n",
      "step 200: loss = 0.0467\n",
      "step 250: loss = 0.0474\n",
      "step 300: loss = 0.0450\n",
      "step 350: loss = 0.0461\n",
      "step 400: loss = 0.0437\n",
      "step 450: loss = 0.0458\n",
      "test_loss: 0.04699123859405518\n",
      "step 0: loss = 0.0469\n",
      "step 50: loss = 0.0478\n",
      "step 100: loss = 0.0470\n",
      "step 150: loss = 0.0465\n",
      "step 200: loss = 0.0465\n",
      "step 250: loss = 0.0472\n",
      "step 300: loss = 0.0498\n",
      "step 350: loss = 0.0480\n",
      "step 400: loss = 0.0472\n",
      "step 450: loss = 0.0464\n",
      "test_loss: 0.047318615913391114\n",
      "step 0: loss = 0.0478\n",
      "step 50: loss = 0.0472\n",
      "step 100: loss = 0.0466\n",
      "step 150: loss = 0.0463\n",
      "step 200: loss = 0.0464\n",
      "step 250: loss = 0.0475\n",
      "step 300: loss = 0.0491\n",
      "step 350: loss = 0.0491\n",
      "step 400: loss = 0.0483\n",
      "step 450: loss = 0.0469\n",
      "test_loss: 0.04699462890625\n",
      "step 0: loss = 0.0469\n",
      "step 50: loss = 0.0471\n",
      "step 100: loss = 0.0488\n",
      "step 150: loss = 0.0467\n",
      "step 200: loss = 0.0463\n",
      "step 250: loss = 0.0457\n",
      "step 300: loss = 0.0477\n",
      "step 350: loss = 0.0469\n",
      "step 400: loss = 0.0456\n",
      "step 450: loss = 0.0474\n",
      "test_loss: 0.04708276271820069\n",
      "step 0: loss = 0.0451\n",
      "step 50: loss = 0.0452\n",
      "step 100: loss = 0.0471\n",
      "step 150: loss = 0.0468\n",
      "step 200: loss = 0.0441\n",
      "step 250: loss = 0.0461\n",
      "step 300: loss = 0.0492\n",
      "step 350: loss = 0.0473\n",
      "step 400: loss = 0.0454\n",
      "step 450: loss = 0.0457\n",
      "test_loss: 0.047206602096557616\n",
      "step 0: loss = 0.0494\n",
      "step 50: loss = 0.0452\n",
      "step 100: loss = 0.0486\n",
      "step 150: loss = 0.0462\n",
      "step 200: loss = 0.0458\n",
      "step 250: loss = 0.0476\n",
      "step 300: loss = 0.0442\n",
      "step 350: loss = 0.0471\n",
      "step 400: loss = 0.0478\n",
      "step 450: loss = 0.0456\n",
      "test_loss: 0.04673472404479981\n",
      "step 0: loss = 0.0462\n",
      "step 50: loss = 0.0472\n",
      "step 100: loss = 0.0460\n",
      "step 150: loss = 0.0467\n",
      "step 200: loss = 0.0463\n",
      "step 250: loss = 0.0469\n",
      "step 300: loss = 0.0464\n",
      "step 350: loss = 0.0495\n",
      "step 400: loss = 0.0462\n",
      "step 450: loss = 0.0464\n",
      "test_loss: 0.04703670024871826\n",
      "step 0: loss = 0.0466\n",
      "step 50: loss = 0.0476\n",
      "step 100: loss = 0.0471\n",
      "step 150: loss = 0.0490\n",
      "step 200: loss = 0.0461\n",
      "step 250: loss = 0.0476\n",
      "step 300: loss = 0.0447\n",
      "step 350: loss = 0.0467\n",
      "step 400: loss = 0.0465\n",
      "step 450: loss = 0.0496\n",
      "test_loss: 0.04690991401672363\n",
      "step 0: loss = 0.0469\n",
      "step 50: loss = 0.0450\n",
      "step 100: loss = 0.0468\n",
      "step 150: loss = 0.0459\n",
      "step 200: loss = 0.0457\n",
      "step 250: loss = 0.0453\n",
      "step 300: loss = 0.0464\n",
      "step 350: loss = 0.0501\n",
      "step 400: loss = 0.0460\n",
      "step 450: loss = 0.0475\n",
      "test_loss: 0.046892156600952146\n",
      "step 0: loss = 0.0465\n",
      "step 50: loss = 0.0471\n",
      "step 100: loss = 0.0472\n",
      "step 150: loss = 0.0455\n",
      "step 200: loss = 0.0460\n",
      "step 250: loss = 0.0485\n",
      "step 300: loss = 0.0461\n",
      "step 350: loss = 0.0459\n",
      "step 400: loss = 0.0432\n",
      "step 450: loss = 0.0467\n",
      "test_loss: 0.04688961982727051\n",
      "step 0: loss = 0.0495\n",
      "step 50: loss = 0.0475\n",
      "step 100: loss = 0.0444\n",
      "step 150: loss = 0.0480\n",
      "step 200: loss = 0.0464\n",
      "step 250: loss = 0.0463\n",
      "step 300: loss = 0.0488\n",
      "step 350: loss = 0.0475\n",
      "step 400: loss = 0.0457\n",
      "step 450: loss = 0.0455\n",
      "test_loss: 0.04691995143890381\n",
      "step 0: loss = 0.0477\n",
      "step 50: loss = 0.0492\n",
      "step 100: loss = 0.0463\n",
      "step 150: loss = 0.0425\n",
      "step 200: loss = 0.0463\n",
      "step 250: loss = 0.0463\n",
      "step 300: loss = 0.0495\n",
      "step 350: loss = 0.0482\n",
      "step 400: loss = 0.0494\n",
      "step 450: loss = 0.0465\n",
      "test_loss: 0.04671019077301025\n",
      "step 0: loss = 0.0476\n",
      "step 50: loss = 0.0464\n",
      "step 100: loss = 0.0462\n",
      "step 150: loss = 0.0472\n",
      "step 200: loss = 0.0458\n",
      "step 250: loss = 0.0472\n",
      "step 300: loss = 0.0464\n",
      "step 350: loss = 0.0482\n",
      "step 400: loss = 0.0451\n",
      "step 450: loss = 0.0459\n",
      "test_loss: 0.04689576148986816\n",
      "step 0: loss = 0.0455\n",
      "step 50: loss = 0.0469\n",
      "step 100: loss = 0.0490\n",
      "step 150: loss = 0.0472\n",
      "step 200: loss = 0.0466\n",
      "step 250: loss = 0.0466\n",
      "step 300: loss = 0.0470\n",
      "step 350: loss = 0.0467\n",
      "step 400: loss = 0.0478\n",
      "step 450: loss = 0.0481\n",
      "test_loss: 0.046943259239196775\n",
      "step 0: loss = 0.0483\n",
      "step 50: loss = 0.0470\n",
      "step 100: loss = 0.0476\n",
      "step 150: loss = 0.0472\n",
      "step 200: loss = 0.0471\n",
      "step 250: loss = 0.0458\n",
      "step 300: loss = 0.0446\n",
      "step 350: loss = 0.0443\n",
      "step 400: loss = 0.0451\n",
      "step 450: loss = 0.0466\n",
      "test_loss: 0.04678147792816162\n",
      "step 0: loss = 0.0477\n",
      "step 50: loss = 0.0452\n",
      "step 100: loss = 0.0464\n",
      "step 150: loss = 0.0458\n",
      "step 200: loss = 0.0459\n",
      "step 250: loss = 0.0452\n",
      "step 300: loss = 0.0470\n",
      "step 350: loss = 0.0466\n",
      "step 400: loss = 0.0440\n",
      "step 450: loss = 0.0451\n",
      "test_loss: 0.04664347171783447\n",
      "step 0: loss = 0.0480\n",
      "step 50: loss = 0.0486\n",
      "step 100: loss = 0.0488\n",
      "step 150: loss = 0.0464\n",
      "step 200: loss = 0.0483\n",
      "step 250: loss = 0.0462\n",
      "step 300: loss = 0.0477\n",
      "step 350: loss = 0.0471\n",
      "step 400: loss = 0.0502\n",
      "step 450: loss = 0.0467\n",
      "test_loss: 0.04658383369445801\n",
      "step 0: loss = 0.0457\n",
      "step 50: loss = 0.0486\n",
      "step 100: loss = 0.0475\n",
      "step 150: loss = 0.0436\n",
      "step 200: loss = 0.0464\n",
      "step 250: loss = 0.0453\n",
      "step 300: loss = 0.0450\n",
      "step 350: loss = 0.0463\n",
      "step 400: loss = 0.0460\n",
      "step 450: loss = 0.0467\n",
      "test_loss: 0.04698680400848389\n",
      "step 0: loss = 0.0463\n",
      "step 50: loss = 0.0458\n",
      "step 100: loss = 0.0453\n",
      "step 150: loss = 0.0482\n",
      "step 200: loss = 0.0450\n",
      "step 250: loss = 0.0457\n",
      "step 300: loss = 0.0458\n",
      "step 350: loss = 0.0467\n",
      "step 400: loss = 0.0459\n",
      "step 450: loss = 0.0464\n",
      "test_loss: 0.04683581829071045\n",
      "step 0: loss = 0.0470\n",
      "step 50: loss = 0.0456\n",
      "step 100: loss = 0.0463\n",
      "step 150: loss = 0.0479\n",
      "step 200: loss = 0.0466\n",
      "step 250: loss = 0.0465\n",
      "step 300: loss = 0.0459\n",
      "step 350: loss = 0.0469\n",
      "step 400: loss = 0.0472\n",
      "step 450: loss = 0.0453\n",
      "test_loss: 0.04669713020324707\n",
      "step 0: loss = 0.0461\n",
      "step 50: loss = 0.0496\n",
      "step 100: loss = 0.0499\n",
      "step 150: loss = 0.0475\n",
      "step 200: loss = 0.0456\n",
      "step 250: loss = 0.0481\n",
      "step 300: loss = 0.0472\n",
      "step 350: loss = 0.0482\n",
      "step 400: loss = 0.0477\n",
      "step 450: loss = 0.0464\n",
      "test_loss: 0.04692590713500976\n",
      "step 0: loss = 0.0459\n",
      "step 50: loss = 0.0455\n",
      "step 100: loss = 0.0448\n",
      "step 150: loss = 0.0475\n",
      "step 200: loss = 0.0489\n",
      "step 250: loss = 0.0471\n",
      "step 300: loss = 0.0454\n",
      "step 350: loss = 0.0470\n",
      "step 400: loss = 0.0459\n",
      "step 450: loss = 0.0488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 0.046771059036254885\n",
      "step 0: loss = 0.0454\n",
      "step 50: loss = 0.0459\n",
      "step 100: loss = 0.0487\n",
      "step 150: loss = 0.0477\n",
      "step 200: loss = 0.0462\n",
      "step 250: loss = 0.0474\n",
      "step 300: loss = 0.0456\n",
      "step 350: loss = 0.0466\n",
      "step 400: loss = 0.0464\n",
      "step 450: loss = 0.0472\n",
      "test_loss: 0.046689324378967285\n",
      "step 0: loss = 0.0478\n",
      "step 50: loss = 0.0464\n",
      "step 100: loss = 0.0447\n",
      "step 150: loss = 0.0488\n",
      "step 200: loss = 0.0453\n",
      "step 250: loss = 0.0462\n",
      "step 300: loss = 0.0491\n",
      "step 350: loss = 0.0459\n",
      "step 400: loss = 0.0458\n",
      "step 450: loss = 0.0480\n",
      "test_loss: 0.04673484325408936\n",
      "step 0: loss = 0.0440\n",
      "step 50: loss = 0.0446\n",
      "step 100: loss = 0.0434\n",
      "step 150: loss = 0.0476\n",
      "step 200: loss = 0.0436\n",
      "step 250: loss = 0.0456\n",
      "step 300: loss = 0.0475\n",
      "step 350: loss = 0.0461\n",
      "step 400: loss = 0.0453\n",
      "step 450: loss = 0.0452\n",
      "test_loss: 0.04691484451293945\n",
      "step 0: loss = 0.0456\n",
      "step 50: loss = 0.0456\n",
      "step 100: loss = 0.0465\n",
      "step 150: loss = 0.0463\n",
      "step 200: loss = 0.0460\n",
      "step 250: loss = 0.0472\n",
      "step 300: loss = 0.0435\n",
      "step 350: loss = 0.0475\n",
      "step 400: loss = 0.0452\n",
      "step 450: loss = 0.0474\n",
      "test_loss: 0.04650847911834717\n",
      "step 0: loss = 0.0474\n",
      "step 50: loss = 0.0461\n",
      "step 100: loss = 0.0476\n",
      "step 150: loss = 0.0455\n",
      "step 200: loss = 0.0437\n",
      "step 250: loss = 0.0455\n",
      "step 300: loss = 0.0467\n",
      "step 350: loss = 0.0478\n",
      "step 400: loss = 0.0468\n",
      "step 450: loss = 0.0467\n",
      "test_loss: 0.04684823989868164\n",
      "step 0: loss = 0.0466\n",
      "step 50: loss = 0.0455\n",
      "step 100: loss = 0.0465\n",
      "step 150: loss = 0.0464\n",
      "step 200: loss = 0.0488\n",
      "step 250: loss = 0.0457\n",
      "step 300: loss = 0.0474\n",
      "step 350: loss = 0.0477\n",
      "step 400: loss = 0.0466\n",
      "step 450: loss = 0.0472\n",
      "test_loss: 0.046717348098754885\n",
      "step 0: loss = 0.0472\n",
      "step 50: loss = 0.0461\n",
      "step 100: loss = 0.0458\n",
      "step 150: loss = 0.0472\n",
      "step 200: loss = 0.0483\n",
      "step 250: loss = 0.0458\n",
      "step 300: loss = 0.0471\n",
      "step 350: loss = 0.0458\n",
      "step 400: loss = 0.0467\n",
      "step 450: loss = 0.0454\n",
      "test_loss: 0.04697054386138916\n",
      "step 0: loss = 0.0460\n",
      "step 50: loss = 0.0482\n",
      "step 100: loss = 0.0451\n",
      "step 150: loss = 0.0463\n",
      "step 200: loss = 0.0472\n",
      "step 250: loss = 0.0465\n",
      "step 300: loss = 0.0500\n",
      "step 350: loss = 0.0465\n",
      "step 400: loss = 0.0478\n",
      "step 450: loss = 0.0455\n",
      "test_loss: 0.046668987274169925\n",
      "step 0: loss = 0.0462\n",
      "step 50: loss = 0.0492\n",
      "step 100: loss = 0.0463\n",
      "step 150: loss = 0.0464\n",
      "step 200: loss = 0.0453\n",
      "step 250: loss = 0.0477\n",
      "step 300: loss = 0.0460\n",
      "step 350: loss = 0.0446\n",
      "step 400: loss = 0.0436\n",
      "step 450: loss = 0.0468\n",
      "test_loss: 0.04682815074920654\n",
      "step 0: loss = 0.0470\n",
      "step 50: loss = 0.0434\n",
      "step 100: loss = 0.0457\n",
      "step 150: loss = 0.0480\n",
      "step 200: loss = 0.0468\n",
      "step 250: loss = 0.0477\n",
      "step 300: loss = 0.0478\n",
      "step 350: loss = 0.0426\n",
      "step 400: loss = 0.0459\n",
      "step 450: loss = 0.0459\n",
      "test_loss: 0.04678647994995117\n",
      "step 0: loss = 0.0480\n",
      "step 50: loss = 0.0461\n",
      "step 100: loss = 0.0475\n",
      "step 150: loss = 0.0462\n",
      "step 200: loss = 0.0447\n",
      "step 250: loss = 0.0442\n",
      "step 300: loss = 0.0470\n",
      "step 350: loss = 0.0463\n",
      "step 400: loss = 0.0477\n",
      "step 450: loss = 0.0469\n",
      "test_loss: 0.04684774398803711\n",
      "step 0: loss = 0.0469\n",
      "step 50: loss = 0.0484\n",
      "step 100: loss = 0.0447\n",
      "step 150: loss = 0.0466\n",
      "step 200: loss = 0.0477\n",
      "step 250: loss = 0.0442\n",
      "step 300: loss = 0.0449\n",
      "step 350: loss = 0.0454\n",
      "step 400: loss = 0.0475\n",
      "step 450: loss = 0.0462\n",
      "test_loss: 0.046563396453857424\n",
      "step 0: loss = 0.0472\n",
      "step 50: loss = 0.0473\n",
      "step 100: loss = 0.0468\n",
      "step 150: loss = 0.0446\n",
      "step 200: loss = 0.0464\n",
      "step 250: loss = 0.0486\n",
      "step 300: loss = 0.0465\n",
      "step 350: loss = 0.0484\n",
      "step 400: loss = 0.0472\n",
      "step 450: loss = 0.0475\n",
      "test_loss: 0.04630740642547607\n",
      "step 0: loss = 0.0465\n",
      "step 50: loss = 0.0473\n",
      "step 100: loss = 0.0452\n",
      "step 150: loss = 0.0446\n",
      "step 200: loss = 0.0472\n",
      "step 250: loss = 0.0443\n",
      "step 300: loss = 0.0430\n",
      "step 350: loss = 0.0488\n",
      "step 400: loss = 0.0462\n",
      "step 450: loss = 0.0455\n",
      "test_loss: 0.04667411804199219\n",
      "step 0: loss = 0.0483\n",
      "step 50: loss = 0.0453\n",
      "step 100: loss = 0.0449\n",
      "step 150: loss = 0.0449\n",
      "step 200: loss = 0.0453\n",
      "step 250: loss = 0.0451\n",
      "step 300: loss = 0.0466\n",
      "step 350: loss = 0.0463\n",
      "step 400: loss = 0.0457\n",
      "step 450: loss = 0.0442\n",
      "test_loss: 0.046380138397216795\n",
      "step 0: loss = 0.0460\n",
      "step 50: loss = 0.0463\n",
      "step 100: loss = 0.0436\n",
      "step 150: loss = 0.0452\n",
      "step 200: loss = 0.0441\n",
      "step 250: loss = 0.0473\n",
      "step 300: loss = 0.0471\n",
      "step 350: loss = 0.0469\n",
      "step 400: loss = 0.0481\n",
      "step 450: loss = 0.0460\n",
      "test_loss: 0.046283998489379884\n",
      "step 0: loss = 0.0445\n",
      "step 50: loss = 0.0459\n",
      "step 100: loss = 0.0468\n",
      "step 150: loss = 0.0433\n",
      "step 200: loss = 0.0453\n",
      "step 250: loss = 0.0479\n",
      "step 300: loss = 0.0452\n",
      "step 350: loss = 0.0458\n",
      "step 400: loss = 0.0458\n",
      "step 450: loss = 0.0460\n",
      "test_loss: 0.04618727207183838\n",
      "step 0: loss = 0.0452\n",
      "step 50: loss = 0.0451\n",
      "step 100: loss = 0.0469\n",
      "step 150: loss = 0.0470\n",
      "step 200: loss = 0.0428\n",
      "step 250: loss = 0.0458\n",
      "step 300: loss = 0.0484\n",
      "step 350: loss = 0.0500\n",
      "step 400: loss = 0.0464\n",
      "step 450: loss = 0.0444\n",
      "test_loss: 0.04637801647186279\n",
      "step 0: loss = 0.0433\n",
      "step 50: loss = 0.0448\n",
      "step 100: loss = 0.0469\n",
      "step 150: loss = 0.0485\n",
      "step 200: loss = 0.0469\n",
      "step 250: loss = 0.0450\n",
      "step 300: loss = 0.0450\n",
      "step 350: loss = 0.0448\n",
      "step 400: loss = 0.0445\n",
      "step 450: loss = 0.0444\n",
      "test_loss: 0.04612282276153565\n",
      "step 0: loss = 0.0444\n",
      "step 50: loss = 0.0437\n",
      "step 100: loss = 0.0483\n",
      "step 150: loss = 0.0453\n",
      "step 200: loss = 0.0454\n",
      "step 250: loss = 0.0463\n",
      "step 300: loss = 0.0441\n",
      "step 350: loss = 0.0468\n",
      "step 400: loss = 0.0447\n",
      "step 450: loss = 0.0462\n",
      "test_loss: 0.046212387084960935\n",
      "step 0: loss = 0.0447\n",
      "step 50: loss = 0.0462\n",
      "step 100: loss = 0.0479\n",
      "step 150: loss = 0.0439\n",
      "step 200: loss = 0.0480\n",
      "step 250: loss = 0.0469\n",
      "step 300: loss = 0.0459\n",
      "step 350: loss = 0.0461\n",
      "step 400: loss = 0.0456\n",
      "step 450: loss = 0.0451\n",
      "test_loss: 0.046066503524780276\n",
      "step 0: loss = 0.0455\n",
      "step 50: loss = 0.0468\n",
      "step 100: loss = 0.0472\n",
      "step 150: loss = 0.0474\n",
      "step 200: loss = 0.0435\n",
      "step 250: loss = 0.0452\n",
      "step 300: loss = 0.0454\n",
      "step 350: loss = 0.0445\n",
      "step 400: loss = 0.0462\n",
      "step 450: loss = 0.0472\n",
      "test_loss: 0.04597387790679932\n",
      "step 0: loss = 0.0450\n",
      "step 50: loss = 0.0462\n",
      "step 100: loss = 0.0492\n",
      "step 150: loss = 0.0459\n",
      "step 200: loss = 0.0452\n",
      "step 250: loss = 0.0457\n",
      "step 300: loss = 0.0455\n",
      "step 350: loss = 0.0458\n",
      "step 400: loss = 0.0467\n",
      "step 450: loss = 0.0448\n",
      "test_loss: 0.046011033058166506\n",
      "step 0: loss = 0.0459\n",
      "step 50: loss = 0.0464\n",
      "step 100: loss = 0.0449\n",
      "step 150: loss = 0.0454\n",
      "step 200: loss = 0.0451\n",
      "step 250: loss = 0.0466\n",
      "step 300: loss = 0.0461\n",
      "step 350: loss = 0.0461\n",
      "step 400: loss = 0.0456\n",
      "step 450: loss = 0.0460\n",
      "test_loss: 0.04578959465026856\n",
      "step 0: loss = 0.0458\n",
      "step 50: loss = 0.0446\n",
      "step 100: loss = 0.0446\n",
      "step 150: loss = 0.0458\n",
      "step 200: loss = 0.0472\n",
      "step 250: loss = 0.0465\n",
      "step 300: loss = 0.0432\n",
      "step 350: loss = 0.0491\n",
      "step 400: loss = 0.0457\n",
      "step 450: loss = 0.0466\n",
      "test_loss: 0.045857934951782225\n",
      "step 0: loss = 0.0468\n",
      "step 50: loss = 0.0479\n",
      "step 100: loss = 0.0453\n",
      "step 150: loss = 0.0448\n",
      "step 200: loss = 0.0477\n",
      "step 250: loss = 0.0453\n",
      "step 300: loss = 0.0449\n",
      "step 350: loss = 0.0476\n",
      "step 400: loss = 0.0441\n",
      "step 450: loss = 0.0458\n",
      "test_loss: 0.04597973823547363\n",
      "step 0: loss = 0.0475\n",
      "step 50: loss = 0.0467\n",
      "step 100: loss = 0.0464\n",
      "step 150: loss = 0.0444\n",
      "step 200: loss = 0.0453\n",
      "step 250: loss = 0.0453\n",
      "step 300: loss = 0.0452\n",
      "step 350: loss = 0.0463\n",
      "step 400: loss = 0.0437\n",
      "step 450: loss = 0.0437\n",
      "test_loss: 0.04570250511169434\n",
      "step 0: loss = 0.0452\n",
      "step 50: loss = 0.0448\n",
      "step 100: loss = 0.0446\n",
      "step 150: loss = 0.0442\n",
      "step 200: loss = 0.0465\n",
      "step 250: loss = 0.0466\n",
      "step 300: loss = 0.0457\n",
      "step 350: loss = 0.0460\n",
      "step 400: loss = 0.0476\n",
      "step 450: loss = 0.0462\n",
      "test_loss: 0.04598850727081299\n",
      "step 0: loss = 0.0453\n",
      "step 50: loss = 0.0451\n",
      "step 100: loss = 0.0448\n",
      "step 150: loss = 0.0482\n",
      "step 200: loss = 0.0449\n",
      "step 250: loss = 0.0469\n",
      "step 300: loss = 0.0472\n",
      "step 350: loss = 0.0467\n",
      "step 400: loss = 0.0474\n",
      "step 450: loss = 0.0469\n",
      "test_loss: 0.04584061622619629\n",
      "step 0: loss = 0.0471\n",
      "step 50: loss = 0.0449\n",
      "step 100: loss = 0.0459\n",
      "step 150: loss = 0.0458\n",
      "step 200: loss = 0.0453\n",
      "step 250: loss = 0.0471\n",
      "step 300: loss = 0.0455\n",
      "step 350: loss = 0.0450\n",
      "step 400: loss = 0.0435\n",
      "step 450: loss = 0.0463\n",
      "test_loss: 0.04574880123138428\n",
      "step 0: loss = 0.0461\n",
      "step 50: loss = 0.0439\n",
      "step 100: loss = 0.0444\n",
      "step 150: loss = 0.0471\n",
      "step 200: loss = 0.0493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 250: loss = 0.0444\n",
      "step 300: loss = 0.0447\n",
      "step 350: loss = 0.0456\n",
      "step 400: loss = 0.0446\n",
      "step 450: loss = 0.0453\n",
      "test_loss: 0.04558298587799072\n",
      "step 0: loss = 0.0461\n",
      "step 50: loss = 0.0447\n",
      "step 100: loss = 0.0431\n",
      "step 150: loss = 0.0446\n",
      "step 200: loss = 0.0464\n",
      "step 250: loss = 0.0439\n",
      "step 300: loss = 0.0447\n",
      "step 350: loss = 0.0436\n",
      "step 400: loss = 0.0442\n",
      "step 450: loss = 0.0453\n",
      "test_loss: 0.045584826469421386\n",
      "step 0: loss = 0.0456\n",
      "step 50: loss = 0.0474\n",
      "step 100: loss = 0.0454\n",
      "step 150: loss = 0.0450\n",
      "step 200: loss = 0.0477\n",
      "step 250: loss = 0.0416\n",
      "step 300: loss = 0.0429\n",
      "step 350: loss = 0.0449\n",
      "step 400: loss = 0.0460\n",
      "step 450: loss = 0.0455\n",
      "test_loss: 0.04558544158935547\n",
      "step 0: loss = 0.0460\n",
      "step 50: loss = 0.0450\n",
      "step 100: loss = 0.0441\n",
      "step 150: loss = 0.0466\n",
      "step 200: loss = 0.0450\n",
      "step 250: loss = 0.0458\n",
      "step 300: loss = 0.0450\n",
      "step 350: loss = 0.0472\n",
      "step 400: loss = 0.0442\n",
      "step 450: loss = 0.0451\n",
      "test_loss: 0.04577095031738281\n",
      "step 0: loss = 0.0466\n",
      "step 50: loss = 0.0479\n",
      "step 100: loss = 0.0445\n",
      "step 150: loss = 0.0493\n",
      "step 200: loss = 0.0455\n",
      "step 250: loss = 0.0464\n",
      "step 300: loss = 0.0463\n",
      "step 350: loss = 0.0438\n",
      "step 400: loss = 0.0458\n",
      "step 450: loss = 0.0469\n",
      "test_loss: 0.045467767715454105\n",
      "step 0: loss = 0.0437\n",
      "step 50: loss = 0.0479\n",
      "step 100: loss = 0.0442\n",
      "step 150: loss = 0.0485\n",
      "step 200: loss = 0.0456\n",
      "step 250: loss = 0.0435\n",
      "step 300: loss = 0.0454\n",
      "step 350: loss = 0.0432\n",
      "step 400: loss = 0.0459\n",
      "step 450: loss = 0.0454\n",
      "test_loss: 0.04566018581390381\n",
      "step 0: loss = 0.0455\n",
      "step 50: loss = 0.0477\n",
      "step 100: loss = 0.0468\n",
      "step 150: loss = 0.0447\n",
      "step 200: loss = 0.0458\n",
      "step 250: loss = 0.0459\n",
      "step 300: loss = 0.0460\n",
      "step 350: loss = 0.0455\n",
      "step 400: loss = 0.0471\n",
      "step 450: loss = 0.0461\n",
      "test_loss: 0.04564446926116943\n",
      "step 0: loss = 0.0444\n",
      "step 50: loss = 0.0460\n",
      "step 100: loss = 0.0427\n",
      "step 150: loss = 0.0450\n",
      "step 200: loss = 0.0438\n",
      "step 250: loss = 0.0471\n",
      "step 300: loss = 0.0476\n",
      "step 350: loss = 0.0455\n",
      "step 400: loss = 0.0479\n",
      "step 450: loss = 0.0482\n",
      "test_loss: 0.04570023536682129\n",
      "step 0: loss = 0.0450\n",
      "step 50: loss = 0.0475\n",
      "step 100: loss = 0.0450\n",
      "step 150: loss = 0.0468\n",
      "step 200: loss = 0.0457\n",
      "step 250: loss = 0.0455\n",
      "step 300: loss = 0.0464\n",
      "step 350: loss = 0.0441\n",
      "step 400: loss = 0.0452\n",
      "step 450: loss = 0.0467\n",
      "test_loss: 0.04559257984161377\n",
      "step 0: loss = 0.0454\n",
      "step 50: loss = 0.0461\n",
      "step 100: loss = 0.0469\n",
      "step 150: loss = 0.0451\n",
      "step 200: loss = 0.0447\n",
      "step 250: loss = 0.0471\n",
      "step 300: loss = 0.0459\n",
      "step 350: loss = 0.0459\n",
      "step 400: loss = 0.0456\n",
      "step 450: loss = 0.0441\n",
      "test_loss: 0.045528111457824705\n",
      "step 0: loss = 0.0460\n",
      "step 50: loss = 0.0439\n",
      "step 100: loss = 0.0440\n",
      "step 150: loss = 0.0438\n",
      "step 200: loss = 0.0467\n",
      "step 250: loss = 0.0461\n",
      "step 300: loss = 0.0454\n",
      "step 350: loss = 0.0457\n",
      "step 400: loss = 0.0457\n",
      "step 450: loss = 0.0443\n",
      "test_loss: 0.045396828651428224\n",
      "step 0: loss = 0.0457\n",
      "step 50: loss = 0.0468\n",
      "step 100: loss = 0.0465\n",
      "step 150: loss = 0.0452\n",
      "step 200: loss = 0.0447\n",
      "step 250: loss = 0.0462\n",
      "step 300: loss = 0.0465\n",
      "step 350: loss = 0.0437\n",
      "step 400: loss = 0.0458\n",
      "step 450: loss = 0.0445\n",
      "test_loss: 0.04563562870025635\n",
      "step 0: loss = 0.0428\n",
      "step 50: loss = 0.0468\n",
      "step 100: loss = 0.0484\n",
      "step 150: loss = 0.0432\n",
      "step 200: loss = 0.0434\n",
      "step 250: loss = 0.0458\n",
      "step 300: loss = 0.0469\n",
      "step 350: loss = 0.0466\n",
      "step 400: loss = 0.0433\n",
      "step 450: loss = 0.0433\n",
      "test_loss: 0.04530133724212646\n",
      "step 0: loss = 0.0414\n",
      "step 50: loss = 0.0444\n",
      "step 100: loss = 0.0448\n",
      "step 150: loss = 0.0433\n",
      "step 200: loss = 0.0436\n",
      "step 250: loss = 0.0468\n",
      "step 300: loss = 0.0474\n",
      "step 350: loss = 0.0441\n",
      "step 400: loss = 0.0438\n",
      "step 450: loss = 0.0456\n",
      "test_loss: 0.04496880531311035\n",
      "step 0: loss = 0.0448\n",
      "step 50: loss = 0.0464\n",
      "step 100: loss = 0.0456\n",
      "step 150: loss = 0.0447\n",
      "step 200: loss = 0.0454\n",
      "step 250: loss = 0.0446\n",
      "step 300: loss = 0.0456\n",
      "step 350: loss = 0.0476\n",
      "step 400: loss = 0.0473\n",
      "step 450: loss = 0.0452\n",
      "test_loss: 0.04513514041900635\n",
      "step 0: loss = 0.0465\n",
      "step 50: loss = 0.0440\n",
      "step 100: loss = 0.0453\n",
      "step 150: loss = 0.0430\n",
      "step 200: loss = 0.0469\n",
      "step 250: loss = 0.0443\n",
      "step 300: loss = 0.0446\n",
      "step 350: loss = 0.0444\n",
      "step 400: loss = 0.0457\n",
      "step 450: loss = 0.0450\n",
      "test_loss: 0.04527540683746338\n",
      "step 0: loss = 0.0444\n",
      "step 50: loss = 0.0466\n",
      "step 100: loss = 0.0455\n",
      "step 150: loss = 0.0454\n",
      "step 200: loss = 0.0431\n",
      "step 250: loss = 0.0451\n",
      "step 300: loss = 0.0455\n",
      "step 350: loss = 0.0445\n",
      "step 400: loss = 0.0447\n",
      "step 450: loss = 0.0419\n",
      "test_loss: 0.04509866714477539\n",
      "step 0: loss = 0.0460\n",
      "step 50: loss = 0.0446\n",
      "step 100: loss = 0.0470\n",
      "step 150: loss = 0.0441\n",
      "step 200: loss = 0.0472\n",
      "step 250: loss = 0.0478\n",
      "step 300: loss = 0.0449\n",
      "step 350: loss = 0.0447\n",
      "step 400: loss = 0.0432\n",
      "step 450: loss = 0.0451\n",
      "test_loss: 0.045151715278625486\n",
      "step 0: loss = 0.0458\n",
      "step 50: loss = 0.0429\n",
      "step 100: loss = 0.0457\n",
      "step 150: loss = 0.0463\n",
      "step 200: loss = 0.0424\n",
      "step 250: loss = 0.0443\n",
      "step 300: loss = 0.0458\n",
      "step 350: loss = 0.0442\n",
      "step 400: loss = 0.0428\n",
      "step 450: loss = 0.0459\n",
      "test_loss: 0.044970245361328126\n",
      "step 0: loss = 0.0440\n",
      "step 50: loss = 0.0450\n",
      "step 100: loss = 0.0440\n",
      "step 150: loss = 0.0437\n",
      "step 200: loss = 0.0441\n",
      "step 250: loss = 0.0441\n",
      "step 300: loss = 0.0451\n",
      "step 350: loss = 0.0469\n",
      "step 400: loss = 0.0440\n",
      "step 450: loss = 0.0451\n",
      "test_loss: 0.044883589744567874\n",
      "step 0: loss = 0.0454\n",
      "step 50: loss = 0.0455\n",
      "step 100: loss = 0.0456\n",
      "step 150: loss = 0.0444\n",
      "step 200: loss = 0.0453\n",
      "step 250: loss = 0.0443\n",
      "step 300: loss = 0.0446\n",
      "step 350: loss = 0.0458\n",
      "step 400: loss = 0.0451\n",
      "step 450: loss = 0.0451\n",
      "test_loss: 0.04468830108642578\n",
      "step 0: loss = 0.0459\n",
      "step 50: loss = 0.0452\n",
      "step 100: loss = 0.0448\n",
      "step 150: loss = 0.0432\n",
      "step 200: loss = 0.0433\n",
      "step 250: loss = 0.0436\n",
      "step 300: loss = 0.0426\n",
      "step 350: loss = 0.0426\n",
      "step 400: loss = 0.0459\n",
      "step 450: loss = 0.0448\n",
      "test_loss: 0.04483108997344971\n",
      "step 0: loss = 0.0444\n",
      "step 50: loss = 0.0438\n",
      "step 100: loss = 0.0454\n",
      "step 150: loss = 0.0453\n",
      "step 200: loss = 0.0438\n",
      "step 250: loss = 0.0445\n",
      "step 300: loss = 0.0465\n",
      "step 350: loss = 0.0461\n",
      "step 400: loss = 0.0453\n",
      "step 450: loss = 0.0447\n",
      "test_loss: 0.04473302841186524\n",
      "step 0: loss = 0.0467\n",
      "step 50: loss = 0.0450\n",
      "step 100: loss = 0.0455\n",
      "step 150: loss = 0.0457\n",
      "step 200: loss = 0.0443\n",
      "step 250: loss = 0.0439\n",
      "step 300: loss = 0.0439\n",
      "step 350: loss = 0.0453\n",
      "step 400: loss = 0.0449\n",
      "step 450: loss = 0.0438\n",
      "test_loss: 0.04481074810028076\n",
      "step 0: loss = 0.0425\n",
      "step 50: loss = 0.0450\n",
      "step 100: loss = 0.0461\n",
      "step 150: loss = 0.0455\n",
      "step 200: loss = 0.0457\n",
      "step 250: loss = 0.0443\n",
      "step 300: loss = 0.0467\n",
      "step 350: loss = 0.0427\n",
      "step 400: loss = 0.0463\n",
      "step 450: loss = 0.0450\n",
      "test_loss: 0.0449659538269043\n",
      "step 0: loss = 0.0455\n",
      "step 50: loss = 0.0442\n",
      "step 100: loss = 0.0448\n",
      "step 150: loss = 0.0447\n",
      "step 200: loss = 0.0445\n",
      "step 250: loss = 0.0450\n",
      "step 300: loss = 0.0447\n",
      "step 350: loss = 0.0445\n",
      "step 400: loss = 0.0452\n",
      "step 450: loss = 0.0443\n",
      "test_loss: 0.044666481018066403\n",
      "step 0: loss = 0.0430\n",
      "step 50: loss = 0.0445\n",
      "step 100: loss = 0.0436\n",
      "step 150: loss = 0.0431\n",
      "step 200: loss = 0.0464\n",
      "step 250: loss = 0.0440\n",
      "step 300: loss = 0.0412\n",
      "step 350: loss = 0.0431\n",
      "step 400: loss = 0.0481\n",
      "step 450: loss = 0.0442\n",
      "test_loss: 0.045112924575805666\n",
      "step 0: loss = 0.0454\n",
      "step 50: loss = 0.0422\n",
      "step 100: loss = 0.0418\n",
      "step 150: loss = 0.0433\n",
      "step 200: loss = 0.0436\n",
      "step 250: loss = 0.0459\n",
      "step 300: loss = 0.0439\n",
      "step 350: loss = 0.0444\n",
      "step 400: loss = 0.0469\n",
      "step 450: loss = 0.0452\n",
      "test_loss: 0.04481386661529541\n",
      "step 0: loss = 0.0440\n",
      "step 50: loss = 0.0454\n",
      "step 100: loss = 0.0440\n",
      "step 150: loss = 0.0415\n",
      "step 200: loss = 0.0476\n",
      "step 250: loss = 0.0444\n",
      "step 300: loss = 0.0458\n",
      "step 350: loss = 0.0462\n",
      "step 400: loss = 0.0446\n",
      "step 450: loss = 0.0457\n",
      "test_loss: 0.0446302604675293\n",
      "step 0: loss = 0.0472\n",
      "step 50: loss = 0.0468\n",
      "step 100: loss = 0.0444\n",
      "step 150: loss = 0.0425\n",
      "step 200: loss = 0.0448\n",
      "step 250: loss = 0.0436\n",
      "step 300: loss = 0.0457\n",
      "step 350: loss = 0.0451\n",
      "step 400: loss = 0.0428\n",
      "step 450: loss = 0.0436\n",
      "test_loss: 0.04459469318389893\n",
      "step 0: loss = 0.0436\n",
      "step 50: loss = 0.0443\n",
      "step 100: loss = 0.0424\n",
      "step 150: loss = 0.0439\n",
      "step 200: loss = 0.0461\n",
      "step 250: loss = 0.0451\n",
      "step 300: loss = 0.0441\n",
      "step 350: loss = 0.0422\n",
      "step 400: loss = 0.0426\n",
      "step 450: loss = 0.0441\n",
      "test_loss: 0.04464462757110596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: loss = 0.0437\n",
      "step 50: loss = 0.0429\n",
      "step 100: loss = 0.0435\n",
      "step 150: loss = 0.0451\n",
      "step 200: loss = 0.0427\n",
      "step 250: loss = 0.0441\n",
      "step 300: loss = 0.0438\n",
      "step 350: loss = 0.0443\n",
      "step 400: loss = 0.0449\n",
      "step 450: loss = 0.0423\n",
      "test_loss: 0.044403085708618166\n",
      "step 0: loss = 0.0433\n",
      "step 50: loss = 0.0454\n",
      "step 100: loss = 0.0432\n",
      "step 150: loss = 0.0443\n",
      "step 200: loss = 0.0445\n",
      "step 250: loss = 0.0439\n",
      "step 300: loss = 0.0429\n",
      "step 350: loss = 0.0453\n",
      "step 400: loss = 0.0443\n",
      "step 450: loss = 0.0448\n",
      "test_loss: 0.044417109489440915\n",
      "step 0: loss = 0.0419\n",
      "step 50: loss = 0.0447\n",
      "step 100: loss = 0.0417\n",
      "step 150: loss = 0.0446\n",
      "step 200: loss = 0.0436\n",
      "step 250: loss = 0.0468\n",
      "step 300: loss = 0.0465\n",
      "step 350: loss = 0.0413\n",
      "step 400: loss = 0.0449\n",
      "step 450: loss = 0.0456\n",
      "test_loss: 0.04464056015014648\n",
      "step 0: loss = 0.0441\n",
      "step 50: loss = 0.0421\n",
      "step 100: loss = 0.0436\n",
      "step 150: loss = 0.0420\n",
      "step 200: loss = 0.0464\n",
      "step 250: loss = 0.0422\n",
      "step 300: loss = 0.0454\n",
      "step 350: loss = 0.0447\n",
      "step 400: loss = 0.0450\n",
      "step 450: loss = 0.0451\n",
      "test_loss: 0.044522700309753416\n",
      "step 0: loss = 0.0445\n",
      "step 50: loss = 0.0455\n",
      "step 100: loss = 0.0429\n",
      "step 150: loss = 0.0437\n",
      "step 200: loss = 0.0443\n",
      "step 250: loss = 0.0445\n",
      "step 300: loss = 0.0469\n",
      "step 350: loss = 0.0461\n",
      "step 400: loss = 0.0432\n",
      "step 450: loss = 0.0439\n",
      "test_loss: 0.044732894897460934\n",
      "step 0: loss = 0.0428\n",
      "step 50: loss = 0.0456\n",
      "step 100: loss = 0.0449\n",
      "step 150: loss = 0.0426\n",
      "step 200: loss = 0.0452\n",
      "step 250: loss = 0.0443\n",
      "step 300: loss = 0.0451\n",
      "step 350: loss = 0.0458\n",
      "step 400: loss = 0.0456\n",
      "step 450: loss = 0.0450\n",
      "test_loss: 0.04423677921295166\n",
      "step 0: loss = 0.0411\n",
      "step 50: loss = 0.0454\n",
      "step 100: loss = 0.0452\n",
      "step 150: loss = 0.0448\n",
      "step 200: loss = 0.0447\n",
      "step 250: loss = 0.0459\n",
      "step 300: loss = 0.0440\n",
      "step 350: loss = 0.0445\n",
      "step 400: loss = 0.0446\n",
      "step 450: loss = 0.0455\n",
      "test_loss: 0.04447499752044678\n",
      "step 0: loss = 0.0454\n",
      "step 50: loss = 0.0449\n",
      "step 100: loss = 0.0461\n",
      "step 150: loss = 0.0433\n",
      "step 200: loss = 0.0443\n",
      "step 250: loss = 0.0446\n",
      "step 300: loss = 0.0420\n",
      "step 350: loss = 0.0455\n",
      "step 400: loss = 0.0445\n",
      "step 450: loss = 0.0436\n",
      "test_loss: 0.04475894451141357\n",
      "step 0: loss = 0.0461\n",
      "step 50: loss = 0.0433\n",
      "step 100: loss = 0.0427\n",
      "step 150: loss = 0.0455\n",
      "step 200: loss = 0.0455\n",
      "step 250: loss = 0.0420\n",
      "step 300: loss = 0.0444\n",
      "step 350: loss = 0.0434\n",
      "step 400: loss = 0.0440\n",
      "step 450: loss = 0.0436\n",
      "test_loss: 0.04422966957092285\n",
      "step 0: loss = 0.0452\n",
      "step 50: loss = 0.0421\n",
      "step 100: loss = 0.0460\n",
      "step 150: loss = 0.0431\n",
      "step 200: loss = 0.0466\n",
      "step 250: loss = 0.0436\n",
      "step 300: loss = 0.0449\n",
      "step 350: loss = 0.0450\n",
      "step 400: loss = 0.0439\n",
      "step 450: loss = 0.0454\n",
      "test_loss: 0.04450277328491211\n",
      "step 0: loss = 0.0457\n",
      "step 50: loss = 0.0473\n",
      "step 100: loss = 0.0452\n",
      "step 150: loss = 0.0434\n",
      "step 200: loss = 0.0447\n",
      "step 250: loss = 0.0447\n",
      "step 300: loss = 0.0424\n",
      "step 350: loss = 0.0439\n",
      "step 400: loss = 0.0423\n",
      "step 450: loss = 0.0448\n",
      "test_loss: 0.0446182918548584\n",
      "step 0: loss = 0.0426\n",
      "step 50: loss = 0.0445\n",
      "step 100: loss = 0.0450\n",
      "step 150: loss = 0.0428\n",
      "step 200: loss = 0.0434\n",
      "step 250: loss = 0.0464\n",
      "step 300: loss = 0.0421\n",
      "step 350: loss = 0.0427\n",
      "step 400: loss = 0.0442\n",
      "step 450: loss = 0.0428\n",
      "test_loss: 0.04473562240600586\n",
      "step 0: loss = 0.0434\n",
      "step 50: loss = 0.0438\n",
      "step 100: loss = 0.0433\n",
      "step 150: loss = 0.0442\n",
      "step 200: loss = 0.0423\n",
      "step 250: loss = 0.0444\n",
      "step 300: loss = 0.0452\n",
      "step 350: loss = 0.0473\n",
      "step 400: loss = 0.0464\n",
      "step 450: loss = 0.0445\n",
      "test_loss: 0.04446291923522949\n",
      "step 0: loss = 0.0438\n",
      "step 50: loss = 0.0455\n",
      "step 100: loss = 0.0437\n",
      "step 150: loss = 0.0437\n",
      "step 200: loss = 0.0429\n",
      "step 250: loss = 0.0438\n",
      "step 300: loss = 0.0438\n",
      "step 350: loss = 0.0463\n",
      "step 400: loss = 0.0432\n",
      "step 450: loss = 0.0433\n",
      "test_loss: 0.04452678203582763\n",
      "step 0: loss = 0.0430\n",
      "step 50: loss = 0.0462\n",
      "step 100: loss = 0.0433\n",
      "step 150: loss = 0.0433\n",
      "step 200: loss = 0.0459\n",
      "step 250: loss = 0.0437\n",
      "step 300: loss = 0.0464\n",
      "step 350: loss = 0.0444\n",
      "step 400: loss = 0.0451\n",
      "step 450: loss = 0.0435\n",
      "test_loss: 0.04445613861083984\n",
      "step 0: loss = 0.0448\n",
      "step 50: loss = 0.0447\n",
      "step 100: loss = 0.0450\n",
      "step 150: loss = 0.0411\n",
      "step 200: loss = 0.0446\n",
      "step 250: loss = 0.0457\n",
      "step 300: loss = 0.0445\n",
      "step 350: loss = 0.0435\n",
      "step 400: loss = 0.0439\n",
      "step 450: loss = 0.0451\n",
      "test_loss: 0.04440185546875\n",
      "step 0: loss = 0.0445\n",
      "step 50: loss = 0.0438\n",
      "step 100: loss = 0.0448\n",
      "step 150: loss = 0.0441\n",
      "step 200: loss = 0.0433\n",
      "step 250: loss = 0.0431\n",
      "step 300: loss = 0.0453\n",
      "step 350: loss = 0.0442\n",
      "step 400: loss = 0.0455\n",
      "step 450: loss = 0.0440\n",
      "test_loss: 0.044626102447509766\n",
      "step 0: loss = 0.0452\n",
      "step 50: loss = 0.0461\n",
      "step 100: loss = 0.0443\n",
      "step 150: loss = 0.0424\n",
      "step 200: loss = 0.0460\n",
      "step 250: loss = 0.0447\n",
      "step 300: loss = 0.0458\n",
      "step 350: loss = 0.0426\n",
      "step 400: loss = 0.0432\n",
      "step 450: loss = 0.0453\n",
      "test_loss: 0.04428219795227051\n",
      "step 0: loss = 0.0440\n",
      "step 50: loss = 0.0450\n",
      "step 100: loss = 0.0444\n",
      "step 150: loss = 0.0458\n",
      "step 200: loss = 0.0440\n",
      "step 250: loss = 0.0438\n",
      "step 300: loss = 0.0433\n",
      "step 350: loss = 0.0428\n",
      "step 400: loss = 0.0413\n",
      "step 450: loss = 0.0445\n",
      "test_loss: 0.044311423301696774\n",
      "step 0: loss = 0.0429\n",
      "step 50: loss = 0.0448\n",
      "step 100: loss = 0.0435\n",
      "step 150: loss = 0.0433\n",
      "step 200: loss = 0.0423\n",
      "step 250: loss = 0.0457\n",
      "step 300: loss = 0.0479\n",
      "step 350: loss = 0.0442\n",
      "step 400: loss = 0.0456\n",
      "step 450: loss = 0.0456\n",
      "test_loss: 0.0443203592300415\n",
      "step 0: loss = 0.0442\n",
      "step 50: loss = 0.0443\n",
      "step 100: loss = 0.0441\n",
      "step 150: loss = 0.0439\n",
      "step 200: loss = 0.0452\n",
      "step 250: loss = 0.0446\n",
      "step 300: loss = 0.0419\n",
      "step 350: loss = 0.0449\n",
      "step 400: loss = 0.0439\n",
      "step 450: loss = 0.0431\n",
      "test_loss: 0.0443794584274292\n",
      "step 0: loss = 0.0443\n",
      "step 50: loss = 0.0421\n",
      "step 100: loss = 0.0425\n",
      "step 150: loss = 0.0433\n",
      "step 200: loss = 0.0408\n",
      "step 250: loss = 0.0434\n",
      "step 300: loss = 0.0472\n",
      "step 350: loss = 0.0458\n",
      "step 400: loss = 0.0470\n",
      "step 450: loss = 0.0441\n",
      "test_loss: 0.044213061332702634\n",
      "step 0: loss = 0.0419\n",
      "step 50: loss = 0.0444\n",
      "step 100: loss = 0.0415\n",
      "step 150: loss = 0.0448\n",
      "step 200: loss = 0.0465\n",
      "step 250: loss = 0.0429\n",
      "step 300: loss = 0.0450\n",
      "step 350: loss = 0.0437\n",
      "step 400: loss = 0.0428\n",
      "step 450: loss = 0.0454\n",
      "test_loss: 0.04437456130981445\n",
      "step 0: loss = 0.0454\n",
      "step 50: loss = 0.0440\n",
      "step 100: loss = 0.0451\n",
      "step 150: loss = 0.0450\n",
      "step 200: loss = 0.0426\n",
      "step 250: loss = 0.0429\n",
      "step 300: loss = 0.0428\n",
      "step 350: loss = 0.0445\n",
      "step 400: loss = 0.0436\n",
      "step 450: loss = 0.0446\n",
      "test_loss: 0.0442731237411499\n",
      "step 0: loss = 0.0427\n",
      "step 50: loss = 0.0440\n",
      "step 100: loss = 0.0443\n",
      "step 150: loss = 0.0445\n",
      "step 200: loss = 0.0460\n",
      "step 250: loss = 0.0444\n",
      "step 300: loss = 0.0413\n",
      "step 350: loss = 0.0457\n",
      "step 400: loss = 0.0446\n",
      "step 450: loss = 0.0426\n",
      "test_loss: 0.04422656536102295\n",
      "step 0: loss = 0.0429\n",
      "step 50: loss = 0.0445\n",
      "step 100: loss = 0.0434\n",
      "step 150: loss = 0.0429\n",
      "step 200: loss = 0.0456\n",
      "step 250: loss = 0.0431\n",
      "step 300: loss = 0.0444\n",
      "step 350: loss = 0.0467\n",
      "step 400: loss = 0.0437\n",
      "step 450: loss = 0.0453\n",
      "test_loss: 0.044153385162353516\n",
      "step 0: loss = 0.0452\n",
      "step 50: loss = 0.0429\n",
      "step 100: loss = 0.0452\n",
      "step 150: loss = 0.0424\n",
      "step 200: loss = 0.0448\n",
      "step 250: loss = 0.0412\n",
      "step 300: loss = 0.0443\n",
      "step 350: loss = 0.0454\n",
      "step 400: loss = 0.0431\n",
      "step 450: loss = 0.0433\n",
      "test_loss: 0.04458117961883545\n",
      "step 0: loss = 0.0442\n",
      "step 50: loss = 0.0457\n",
      "step 100: loss = 0.0452\n",
      "step 150: loss = 0.0433\n",
      "step 200: loss = 0.0424\n",
      "step 250: loss = 0.0485\n",
      "step 300: loss = 0.0459\n",
      "step 350: loss = 0.0448\n",
      "step 400: loss = 0.0437\n",
      "step 450: loss = 0.0426\n",
      "test_loss: 0.044027881622314455\n",
      "step 0: loss = 0.0427\n",
      "step 50: loss = 0.0452\n",
      "step 100: loss = 0.0428\n",
      "step 150: loss = 0.0465\n",
      "step 200: loss = 0.0438\n",
      "step 250: loss = 0.0458\n",
      "step 300: loss = 0.0457\n",
      "step 350: loss = 0.0415\n",
      "step 400: loss = 0.0434\n",
      "step 450: loss = 0.0455\n",
      "test_loss: 0.04386834621429443\n",
      "step 0: loss = 0.0441\n",
      "step 50: loss = 0.0453\n",
      "step 100: loss = 0.0444\n",
      "step 150: loss = 0.0429\n",
      "step 200: loss = 0.0436\n",
      "step 250: loss = 0.0444\n",
      "step 300: loss = 0.0437\n",
      "step 350: loss = 0.0433\n",
      "step 400: loss = 0.0428\n",
      "step 450: loss = 0.0427\n",
      "test_loss: 0.04408259391784668\n",
      "step 0: loss = 0.0448\n",
      "step 50: loss = 0.0437\n",
      "step 100: loss = 0.0427\n",
      "step 150: loss = 0.0447\n",
      "step 200: loss = 0.0453\n",
      "step 250: loss = 0.0432\n",
      "step 300: loss = 0.0450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350: loss = 0.0435\n",
      "step 400: loss = 0.0436\n",
      "step 450: loss = 0.0438\n",
      "test_loss: 0.04374153137207031\n",
      "step 0: loss = 0.0443\n",
      "step 50: loss = 0.0430\n",
      "step 100: loss = 0.0420\n",
      "step 150: loss = 0.0439\n",
      "step 200: loss = 0.0424\n",
      "step 250: loss = 0.0462\n",
      "step 300: loss = 0.0431\n",
      "step 350: loss = 0.0426\n",
      "step 400: loss = 0.0452\n",
      "step 450: loss = 0.0427\n",
      "test_loss: 0.0438374137878418\n",
      "step 0: loss = 0.0431\n",
      "step 50: loss = 0.0452\n",
      "step 100: loss = 0.0448\n",
      "step 150: loss = 0.0435\n",
      "step 200: loss = 0.0424\n",
      "step 250: loss = 0.0447\n",
      "step 300: loss = 0.0429\n",
      "step 350: loss = 0.0432\n",
      "step 400: loss = 0.0460\n",
      "step 450: loss = 0.0442\n",
      "test_loss: 0.043320412635803225\n",
      "step 0: loss = 0.0430\n",
      "step 50: loss = 0.0422\n",
      "step 100: loss = 0.0434\n",
      "step 150: loss = 0.0422\n",
      "step 200: loss = 0.0436\n",
      "step 250: loss = 0.0446\n",
      "step 300: loss = 0.0433\n",
      "step 350: loss = 0.0456\n",
      "step 400: loss = 0.0419\n",
      "step 450: loss = 0.0431\n",
      "test_loss: 0.04339375019073486\n",
      "step 0: loss = 0.0407\n",
      "step 50: loss = 0.0442\n",
      "step 100: loss = 0.0435\n",
      "step 150: loss = 0.0437\n",
      "step 200: loss = 0.0420\n",
      "step 250: loss = 0.0438\n",
      "step 300: loss = 0.0431\n",
      "step 350: loss = 0.0430\n",
      "step 400: loss = 0.0441\n",
      "step 450: loss = 0.0437\n",
      "test_loss: 0.043120226860046386\n",
      "step 0: loss = 0.0446\n",
      "step 50: loss = 0.0424\n",
      "step 100: loss = 0.0423\n",
      "step 150: loss = 0.0425\n",
      "step 200: loss = 0.0421\n",
      "step 250: loss = 0.0426\n",
      "step 300: loss = 0.0426\n",
      "step 350: loss = 0.0436\n",
      "step 400: loss = 0.0458\n",
      "step 450: loss = 0.0449\n",
      "test_loss: 0.04286863327026367\n",
      "step 0: loss = 0.0405\n",
      "step 50: loss = 0.0423\n",
      "step 100: loss = 0.0445\n",
      "step 150: loss = 0.0432\n",
      "step 200: loss = 0.0419\n",
      "step 250: loss = 0.0416\n",
      "step 300: loss = 0.0423\n",
      "step 350: loss = 0.0427\n",
      "step 400: loss = 0.0452\n",
      "step 450: loss = 0.0418\n",
      "test_loss: 0.04255105018615723\n",
      "step 0: loss = 0.0440\n",
      "step 50: loss = 0.0419\n",
      "step 100: loss = 0.0430\n",
      "step 150: loss = 0.0395\n",
      "step 200: loss = 0.0412\n",
      "step 250: loss = 0.0419\n",
      "step 300: loss = 0.0416\n",
      "step 350: loss = 0.0408\n",
      "step 400: loss = 0.0403\n",
      "step 450: loss = 0.0423\n",
      "test_loss: 0.042612032890319826\n",
      "step 0: loss = 0.0425\n",
      "step 50: loss = 0.0430\n",
      "step 100: loss = 0.0426\n",
      "step 150: loss = 0.0422\n",
      "step 200: loss = 0.0424\n",
      "step 250: loss = 0.0414\n",
      "step 300: loss = 0.0412\n",
      "step 350: loss = 0.0422\n",
      "step 400: loss = 0.0420\n",
      "step 450: loss = 0.0409\n",
      "test_loss: 0.042322611808776854\n",
      "step 0: loss = 0.0394\n",
      "step 50: loss = 0.0444\n",
      "step 100: loss = 0.0419\n",
      "step 150: loss = 0.0420\n",
      "step 200: loss = 0.0411\n",
      "step 250: loss = 0.0410\n",
      "step 300: loss = 0.0396\n",
      "step 350: loss = 0.0430\n",
      "step 400: loss = 0.0416\n",
      "step 450: loss = 0.0404\n",
      "test_loss: 0.04216428279876709\n",
      "step 0: loss = 0.0426\n",
      "step 50: loss = 0.0436\n",
      "step 100: loss = 0.0435\n",
      "step 150: loss = 0.0403\n",
      "step 200: loss = 0.0439\n",
      "step 250: loss = 0.0404\n",
      "step 300: loss = 0.0400\n",
      "step 350: loss = 0.0436\n",
      "step 400: loss = 0.0400\n",
      "step 450: loss = 0.0422\n",
      "test_loss: 0.04181952953338623\n",
      "step 0: loss = 0.0422\n",
      "step 50: loss = 0.0439\n",
      "step 100: loss = 0.0412\n",
      "step 150: loss = 0.0419\n",
      "step 200: loss = 0.0423\n",
      "step 250: loss = 0.0409\n",
      "step 300: loss = 0.0400\n",
      "step 350: loss = 0.0417\n",
      "step 400: loss = 0.0423\n",
      "step 450: loss = 0.0424\n",
      "test_loss: 0.04223854064941406\n",
      "step 0: loss = 0.0403\n",
      "step 50: loss = 0.0420\n",
      "step 100: loss = 0.0411\n",
      "step 150: loss = 0.0388\n",
      "step 200: loss = 0.0438\n",
      "step 250: loss = 0.0412\n",
      "step 300: loss = 0.0397\n",
      "step 350: loss = 0.0409\n",
      "step 400: loss = 0.0422\n",
      "step 450: loss = 0.0412\n",
      "test_loss: 0.04209207057952881\n",
      "step 0: loss = 0.0402\n",
      "step 50: loss = 0.0417\n",
      "step 100: loss = 0.0419\n",
      "step 150: loss = 0.0432\n",
      "step 200: loss = 0.0403\n",
      "step 250: loss = 0.0413\n",
      "step 300: loss = 0.0413\n",
      "step 350: loss = 0.0440\n",
      "step 400: loss = 0.0419\n",
      "step 450: loss = 0.0406\n",
      "test_loss: 0.04172774314880371\n",
      "step 0: loss = 0.0393\n",
      "step 50: loss = 0.0411\n",
      "step 100: loss = 0.0406\n",
      "step 150: loss = 0.0414\n",
      "step 200: loss = 0.0415\n",
      "step 250: loss = 0.0418\n",
      "step 300: loss = 0.0395\n",
      "step 350: loss = 0.0401\n",
      "step 400: loss = 0.0414\n",
      "step 450: loss = 0.0391\n",
      "test_loss: 0.04190976619720459\n",
      "step 0: loss = 0.0427\n",
      "step 50: loss = 0.0418\n",
      "step 100: loss = 0.0436\n",
      "step 150: loss = 0.0412\n",
      "step 200: loss = 0.0400\n",
      "step 250: loss = 0.0414\n",
      "step 300: loss = 0.0427\n",
      "step 350: loss = 0.0679\n",
      "step 400: loss = 0.0410\n",
      "step 450: loss = 0.0421\n",
      "test_loss: 0.04191249370574951\n",
      "step 0: loss = 0.0411\n",
      "step 50: loss = 0.0420\n",
      "step 100: loss = 0.0420\n",
      "step 150: loss = 0.0427\n",
      "step 200: loss = 0.0423\n",
      "step 250: loss = 0.0454\n",
      "step 300: loss = 0.0400\n",
      "step 350: loss = 0.0395\n",
      "step 400: loss = 0.0403\n",
      "step 450: loss = 0.0402\n",
      "test_loss: 0.04169179439544678\n",
      "step 0: loss = 0.0414\n",
      "step 50: loss = 0.0433\n",
      "step 100: loss = 0.0431\n",
      "step 150: loss = 0.0397\n",
      "step 200: loss = 0.0428\n",
      "step 250: loss = 0.0416\n",
      "step 300: loss = 0.0423\n",
      "step 350: loss = 0.0416\n",
      "step 400: loss = 0.0399\n",
      "step 450: loss = 0.0384\n",
      "test_loss: 0.04167385101318359\n",
      "step 0: loss = 0.0428\n",
      "step 50: loss = 0.0421\n",
      "step 100: loss = 0.0408\n",
      "step 150: loss = 0.0408\n",
      "step 200: loss = 0.0420\n",
      "step 250: loss = 0.0393\n",
      "step 300: loss = 0.0420\n",
      "step 350: loss = 0.0416\n",
      "step 400: loss = 0.0414\n",
      "step 450: loss = 0.0432\n",
      "test_loss: 0.04160758018493652\n",
      "step 0: loss = 0.0428\n",
      "step 50: loss = 0.0408\n",
      "step 100: loss = 0.0428\n",
      "step 150: loss = 0.0408\n",
      "step 200: loss = 0.0396\n",
      "step 250: loss = 0.0401\n",
      "step 300: loss = 0.0427\n",
      "step 350: loss = 0.0413\n",
      "step 400: loss = 0.0406\n",
      "step 450: loss = 0.0403\n",
      "test_loss: 0.04168335914611816\n",
      "step 0: loss = 0.0414\n",
      "step 50: loss = 0.0405\n",
      "step 100: loss = 0.0398\n",
      "step 150: loss = 0.0415\n",
      "step 200: loss = 0.0404\n",
      "step 250: loss = 0.0403\n",
      "step 300: loss = 0.0419\n",
      "step 350: loss = 0.0409\n",
      "step 400: loss = 0.0433\n",
      "step 450: loss = 0.0427\n",
      "test_loss: 0.041885819435119626\n",
      "step 0: loss = 0.0417\n",
      "step 50: loss = 0.0424\n",
      "step 100: loss = 0.0413\n",
      "step 150: loss = 0.0418\n",
      "step 200: loss = 0.0432\n",
      "step 250: loss = 0.0407\n",
      "step 300: loss = 0.0416\n",
      "step 350: loss = 0.0393\n",
      "step 400: loss = 0.0413\n",
      "step 450: loss = 0.0420\n",
      "test_loss: 0.0416558837890625\n",
      "step 0: loss = 0.0404\n",
      "step 50: loss = 0.0405\n",
      "step 100: loss = 0.0434\n",
      "step 150: loss = 0.0414\n",
      "step 200: loss = 0.0422\n",
      "step 250: loss = 0.0407\n",
      "step 300: loss = 0.0408\n",
      "step 350: loss = 0.0410\n",
      "step 400: loss = 0.0401\n",
      "step 450: loss = 0.0420\n",
      "test_loss: 0.041589908599853516\n",
      "step 0: loss = 0.0420\n",
      "step 50: loss = 0.0411\n",
      "step 100: loss = 0.0396\n",
      "step 150: loss = 0.0433\n",
      "step 200: loss = 0.0405\n",
      "step 250: loss = 0.0412\n",
      "step 300: loss = 0.0406\n",
      "step 350: loss = 0.0413\n",
      "step 400: loss = 0.0407\n",
      "step 450: loss = 0.0412\n",
      "test_loss: 0.04168521881103516\n",
      "step 0: loss = 0.0416\n",
      "step 50: loss = 0.0414\n",
      "step 100: loss = 0.0394\n",
      "step 150: loss = 0.0430\n",
      "step 200: loss = 0.0412\n",
      "step 250: loss = 0.0388\n",
      "step 300: loss = 0.0404\n",
      "step 350: loss = 0.0408\n",
      "step 400: loss = 0.0392\n",
      "step 450: loss = 0.0411\n",
      "test_loss: 0.04155563354492187\n",
      "step 0: loss = 0.0425\n",
      "step 50: loss = 0.0407\n",
      "step 100: loss = 0.0390\n",
      "step 150: loss = 0.0427\n",
      "step 200: loss = 0.0426\n",
      "step 250: loss = 0.0437\n",
      "step 300: loss = 0.0402\n",
      "step 350: loss = 0.0398\n",
      "step 400: loss = 0.0438\n",
      "step 450: loss = 0.0413\n",
      "test_loss: 0.04156881332397461\n",
      "step 0: loss = 0.0405\n",
      "step 50: loss = 0.0411\n",
      "step 100: loss = 0.0397\n",
      "step 150: loss = 0.0398\n",
      "step 200: loss = 0.0409\n",
      "step 250: loss = 0.0408\n",
      "step 300: loss = 0.0431\n",
      "step 350: loss = 0.0420\n",
      "step 400: loss = 0.0408\n",
      "step 450: loss = 0.0387\n",
      "test_loss: 0.04145353317260742\n",
      "step 0: loss = 0.0395\n",
      "step 50: loss = 0.0415\n",
      "step 100: loss = 0.0415\n",
      "step 150: loss = 0.0412\n",
      "step 200: loss = 0.0390\n",
      "step 250: loss = 0.0407\n",
      "step 300: loss = 0.0420\n",
      "step 350: loss = 0.0414\n",
      "step 400: loss = 0.0405\n",
      "step 450: loss = 0.0429\n",
      "test_loss: 0.041702594757080075\n",
      "step 0: loss = 0.0409\n",
      "step 50: loss = 0.0414\n",
      "step 100: loss = 0.0428\n",
      "step 150: loss = 0.0436\n",
      "step 200: loss = 0.0409\n",
      "step 250: loss = 0.0394\n",
      "step 300: loss = 0.0402\n",
      "step 350: loss = 0.0405\n",
      "step 400: loss = 0.0439\n",
      "step 450: loss = 0.0414\n",
      "test_loss: 0.04180080890655517\n",
      "step 0: loss = 0.0414\n",
      "step 50: loss = 0.0407\n",
      "step 100: loss = 0.0416\n",
      "step 150: loss = 0.0408\n",
      "step 200: loss = 0.0418\n",
      "step 250: loss = 0.0398\n",
      "step 300: loss = 0.0386\n",
      "step 350: loss = 0.0437\n",
      "step 400: loss = 0.0413\n",
      "step 450: loss = 0.0422\n",
      "test_loss: 0.041465411186218264\n",
      "step 0: loss = 0.0429\n",
      "step 50: loss = 0.0425\n",
      "step 100: loss = 0.0412\n",
      "step 150: loss = 0.0404\n",
      "step 200: loss = 0.0411\n",
      "step 250: loss = 0.0413\n",
      "step 300: loss = 0.0403\n",
      "step 350: loss = 0.0414\n",
      "step 400: loss = 0.0417\n",
      "step 450: loss = 0.0421\n",
      "test_loss: 0.04141299247741699\n",
      "step 0: loss = 0.0401\n",
      "step 50: loss = 0.0420\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 100: loss = 0.0394\n",
      "step 150: loss = 0.0417\n",
      "step 200: loss = 0.0412\n",
      "step 250: loss = 0.0399\n",
      "step 300: loss = 0.0436\n",
      "step 350: loss = 0.0427\n",
      "step 400: loss = 0.0408\n",
      "step 450: loss = 0.0410\n",
      "test_loss: 0.0415607738494873\n",
      "step 0: loss = 0.0427\n",
      "step 50: loss = 0.0403\n",
      "step 100: loss = 0.0423\n",
      "step 150: loss = 0.0418\n",
      "step 200: loss = 0.0429\n",
      "step 250: loss = 0.0422\n",
      "step 300: loss = 0.0415\n",
      "step 350: loss = 0.0418\n",
      "step 400: loss = 0.0417\n",
      "step 450: loss = 0.0429\n",
      "test_loss: 0.041389236450195314\n",
      "step 0: loss = 0.0401\n",
      "step 50: loss = 0.0413\n",
      "step 100: loss = 0.0413\n",
      "step 150: loss = 0.0425\n",
      "step 200: loss = 0.0428\n",
      "step 250: loss = 0.0419\n",
      "step 300: loss = 0.0404\n",
      "step 350: loss = 0.0415\n",
      "step 400: loss = 0.0409\n",
      "step 450: loss = 0.0415\n",
      "test_loss: 0.041487507820129395\n",
      "step 0: loss = 0.0407\n",
      "step 50: loss = 0.0419\n",
      "step 100: loss = 0.0412\n",
      "step 150: loss = 0.0416\n",
      "step 200: loss = 0.0392\n",
      "step 250: loss = 0.0418\n",
      "step 300: loss = 0.0409\n",
      "step 350: loss = 0.0420\n",
      "step 400: loss = 0.0411\n",
      "step 450: loss = 0.0411\n",
      "test_loss: 0.0414508056640625\n",
      "step 0: loss = 0.0409\n",
      "step 50: loss = 0.0416\n",
      "step 100: loss = 0.0436\n",
      "step 150: loss = 0.0420\n",
      "step 200: loss = 0.0405\n",
      "step 250: loss = 0.0409\n",
      "step 300: loss = 0.0409\n",
      "step 350: loss = 0.0415\n",
      "step 400: loss = 0.0409\n",
      "step 450: loss = 0.0406\n",
      "test_loss: 0.0415241813659668\n",
      "step 0: loss = 0.0412\n",
      "step 50: loss = 0.0413\n",
      "step 100: loss = 0.0423\n",
      "step 150: loss = 0.0400\n",
      "step 200: loss = 0.0410\n",
      "step 250: loss = 0.0417\n",
      "step 300: loss = 0.0413\n",
      "step 350: loss = 0.0419\n",
      "step 400: loss = 0.0408\n",
      "step 450: loss = 0.0436\n",
      "test_loss: 0.041606020927429196\n",
      "step 0: loss = 0.0417\n",
      "step 50: loss = 0.0404\n",
      "step 100: loss = 0.0425\n",
      "step 150: loss = 0.0417\n",
      "step 200: loss = 0.0437\n",
      "step 250: loss = 0.0414\n",
      "step 300: loss = 0.0426\n",
      "step 350: loss = 0.0407\n",
      "step 400: loss = 0.0393\n",
      "step 450: loss = 0.0407\n",
      "test_loss: 0.041563100814819336\n",
      "step 0: loss = 0.0386\n",
      "step 50: loss = 0.0387\n",
      "step 100: loss = 0.0414\n",
      "step 150: loss = 0.0405\n",
      "step 200: loss = 0.0412\n",
      "step 250: loss = 0.0412\n",
      "step 300: loss = 0.0411\n",
      "step 350: loss = 0.0422\n",
      "step 400: loss = 0.0427\n",
      "step 450: loss = 0.0411\n",
      "test_loss: 0.0416018009185791\n",
      "step 0: loss = 0.0412\n",
      "step 50: loss = 0.0404\n",
      "step 100: loss = 0.0421\n",
      "step 150: loss = 0.0415\n",
      "step 200: loss = 0.0391\n",
      "step 250: loss = 0.0406\n",
      "step 300: loss = 0.0399\n",
      "step 350: loss = 0.0406\n",
      "step 400: loss = 0.0396\n",
      "step 450: loss = 0.0387\n",
      "test_loss: 0.041452188491821286\n",
      "step 0: loss = 0.0403\n",
      "step 50: loss = 0.0410\n",
      "step 100: loss = 0.0412\n",
      "step 150: loss = 0.0421\n",
      "step 200: loss = 0.0422\n",
      "step 250: loss = 0.0407\n",
      "step 300: loss = 0.0442\n",
      "step 350: loss = 0.0397\n",
      "step 400: loss = 0.0402\n",
      "step 450: loss = 0.0408\n",
      "test_loss: 0.04152372360229492\n",
      "step 0: loss = 0.0415\n",
      "step 50: loss = 0.0409\n",
      "step 100: loss = 0.0424\n",
      "step 150: loss = 0.0425\n",
      "step 200: loss = 0.0416\n",
      "step 250: loss = 0.0411\n",
      "step 300: loss = 0.0419\n",
      "step 350: loss = 0.0424\n",
      "step 400: loss = 0.0436\n",
      "step 450: loss = 0.0426\n",
      "test_loss: 0.041530098915100094\n",
      "step 0: loss = 0.0396\n",
      "step 50: loss = 0.0422\n",
      "step 100: loss = 0.0415\n",
      "step 150: loss = 0.0419\n",
      "step 200: loss = 0.0417\n",
      "step 250: loss = 0.0410\n",
      "step 300: loss = 0.0415\n",
      "step 350: loss = 0.0398\n",
      "step 400: loss = 0.0409\n",
      "step 450: loss = 0.0417\n",
      "test_loss: 0.04165573120117187\n",
      "step 0: loss = 0.0390\n",
      "step 50: loss = 0.0400\n",
      "step 100: loss = 0.0415\n",
      "step 150: loss = 0.0393\n",
      "step 200: loss = 0.0421\n",
      "step 250: loss = 0.0403\n",
      "step 300: loss = 0.0405\n",
      "step 350: loss = 0.0406\n",
      "step 400: loss = 0.0421\n",
      "step 450: loss = 0.0439\n",
      "test_loss: 0.04158906936645508\n",
      "step 0: loss = 0.0422\n",
      "step 50: loss = 0.0423\n",
      "step 100: loss = 0.0437\n",
      "step 150: loss = 0.0420\n",
      "step 200: loss = 0.0415\n",
      "step 250: loss = 0.0416\n",
      "step 300: loss = 0.0404\n",
      "step 350: loss = 0.0411\n",
      "step 400: loss = 0.0419\n",
      "step 450: loss = 0.0391\n",
      "test_loss: 0.041577115058898925\n",
      "step 0: loss = 0.0405\n",
      "step 50: loss = 0.0426\n",
      "step 100: loss = 0.0388\n",
      "step 150: loss = 0.0423\n",
      "step 200: loss = 0.0412\n",
      "step 250: loss = 0.0401\n",
      "step 300: loss = 0.0426\n",
      "step 350: loss = 0.0419\n",
      "step 400: loss = 0.0405\n",
      "step 450: loss = 0.0420\n",
      "test_loss: 0.04139384746551514\n",
      "step 0: loss = 0.0399\n",
      "step 50: loss = 0.0400\n",
      "step 100: loss = 0.0419\n",
      "step 150: loss = 0.0403\n",
      "step 200: loss = 0.0407\n",
      "step 250: loss = 0.0410\n",
      "step 300: loss = 0.0414\n",
      "step 350: loss = 0.0418\n",
      "step 400: loss = 0.0387\n",
      "step 450: loss = 0.0401\n",
      "test_loss: 0.04164759635925293\n",
      "step 0: loss = 0.0421\n",
      "step 50: loss = 0.0418\n",
      "step 100: loss = 0.0423\n",
      "step 150: loss = 0.0431\n",
      "step 200: loss = 0.0420\n",
      "step 250: loss = 0.0415\n",
      "step 300: loss = 0.0407\n",
      "step 350: loss = 0.0416\n",
      "step 400: loss = 0.0408\n",
      "step 450: loss = 0.0429\n",
      "test_loss: 0.04117863178253174\n",
      "step 0: loss = 0.0406\n",
      "step 50: loss = 0.0433\n",
      "step 100: loss = 0.0429\n",
      "step 150: loss = 0.0444\n",
      "step 200: loss = 0.0430\n",
      "step 250: loss = 0.0412\n",
      "step 300: loss = 0.0422\n",
      "step 350: loss = 0.0414\n",
      "step 400: loss = 0.0411\n",
      "step 450: loss = 0.0392\n",
      "test_loss: 0.04134937763214111\n",
      "step 0: loss = 0.0415\n",
      "step 50: loss = 0.0394\n",
      "step 100: loss = 0.0417\n",
      "step 150: loss = 0.0407\n",
      "step 200: loss = 0.0416\n",
      "step 250: loss = 0.0444\n",
      "step 300: loss = 0.0407\n",
      "step 350: loss = 0.0398\n",
      "step 400: loss = 0.0410\n",
      "step 450: loss = 0.0411\n",
      "test_loss: 0.04175427913665772\n",
      "step 0: loss = 0.0413\n",
      "step 50: loss = 0.0395\n",
      "step 100: loss = 0.0420\n",
      "step 150: loss = 0.0402\n",
      "step 200: loss = 0.0412\n",
      "step 250: loss = 0.0410\n",
      "step 300: loss = 0.0425\n",
      "step 350: loss = 0.0416\n",
      "step 400: loss = 0.0411\n",
      "step 450: loss = 0.0429\n",
      "test_loss: 0.041693434715271\n",
      "step 0: loss = 0.0430\n",
      "step 50: loss = 0.0404\n",
      "step 100: loss = 0.0408\n",
      "step 150: loss = 0.0418\n",
      "step 200: loss = 0.0413\n",
      "step 250: loss = 0.0421\n",
      "step 300: loss = 0.0407\n",
      "step 350: loss = 0.0406\n",
      "step 400: loss = 0.0409\n",
      "step 450: loss = 0.0388\n",
      "test_loss: 0.04141643047332764\n",
      "step 0: loss = 0.0413\n",
      "step 50: loss = 0.0415\n",
      "step 100: loss = 0.0416\n",
      "step 150: loss = 0.0412\n",
      "step 200: loss = 0.0406\n",
      "step 250: loss = 0.0398\n",
      "step 300: loss = 0.0412\n",
      "step 350: loss = 0.0409\n",
      "step 400: loss = 0.0422\n",
      "step 450: loss = 0.0422\n",
      "test_loss: 0.04143087387084961\n",
      "step 0: loss = 0.0409\n",
      "step 50: loss = 0.0402\n",
      "step 100: loss = 0.0438\n",
      "step 150: loss = 0.0433\n",
      "step 200: loss = 0.0387\n",
      "step 250: loss = 0.0426\n",
      "step 300: loss = 0.0414\n",
      "step 350: loss = 0.0427\n",
      "step 400: loss = 0.0421\n",
      "step 450: loss = 0.0408\n",
      "test_loss: 0.0411392879486084\n",
      "step 0: loss = 0.0402\n",
      "step 50: loss = 0.0407\n",
      "step 100: loss = 0.0428\n",
      "step 150: loss = 0.0412\n",
      "step 200: loss = 0.0398\n",
      "step 250: loss = 0.0389\n",
      "step 300: loss = 0.0420\n",
      "step 350: loss = 0.0402\n",
      "step 400: loss = 0.0411\n",
      "step 450: loss = 0.0412\n",
      "test_loss: 0.04138542652130127\n",
      "step 0: loss = 0.0429\n",
      "step 50: loss = 0.0411\n",
      "step 100: loss = 0.0413\n",
      "step 150: loss = 0.0423\n",
      "step 200: loss = 0.0394\n",
      "step 250: loss = 0.0418\n",
      "step 300: loss = 0.0375\n",
      "step 350: loss = 0.0413\n",
      "step 400: loss = 0.0393\n",
      "step 450: loss = 0.0421\n",
      "test_loss: 0.04148639678955078\n",
      "step 0: loss = 0.0412\n",
      "step 50: loss = 0.0434\n",
      "step 100: loss = 0.0413\n",
      "step 150: loss = 0.0404\n",
      "step 200: loss = 0.0423\n",
      "step 250: loss = 0.0408\n",
      "step 300: loss = 0.0404\n",
      "step 350: loss = 0.0399\n",
      "step 400: loss = 0.0398\n",
      "step 450: loss = 0.0416\n",
      "test_loss: 0.041263003349304196\n",
      "step 0: loss = 0.0434\n",
      "step 50: loss = 0.0432\n",
      "step 100: loss = 0.0431\n",
      "step 150: loss = 0.0393\n",
      "step 200: loss = 0.0398\n",
      "step 250: loss = 0.0414\n",
      "step 300: loss = 0.0417\n",
      "step 350: loss = 0.0407\n",
      "step 400: loss = 0.0398\n",
      "step 450: loss = 0.0410\n",
      "test_loss: 0.04161259651184082\n",
      "step 0: loss = 0.0414\n",
      "step 50: loss = 0.0400\n",
      "step 100: loss = 0.0418\n",
      "step 150: loss = 0.0398\n",
      "step 200: loss = 0.0415\n",
      "step 250: loss = 0.0425\n",
      "step 300: loss = 0.0424\n",
      "step 350: loss = 0.0417\n",
      "step 400: loss = 0.0405\n",
      "step 450: loss = 0.0396\n",
      "test_loss: 0.04107571125030517\n",
      "step 0: loss = 0.0412\n",
      "step 50: loss = 0.0398\n",
      "step 100: loss = 0.0404\n",
      "step 150: loss = 0.0402\n",
      "step 200: loss = 0.0416\n",
      "step 250: loss = 0.0414\n",
      "step 300: loss = 0.0413\n",
      "step 350: loss = 0.0416\n",
      "step 400: loss = 0.0415\n",
      "step 450: loss = 0.0400\n",
      "test_loss: 0.042050771713256836\n",
      "step 0: loss = 0.0427\n",
      "step 50: loss = 0.0422\n",
      "step 100: loss = 0.0405\n",
      "step 150: loss = 0.0424\n",
      "step 200: loss = 0.0397\n",
      "step 250: loss = 0.0412\n",
      "step 300: loss = 0.0402\n",
      "step 350: loss = 0.0408\n",
      "step 400: loss = 0.0410\n",
      "step 450: loss = 0.0413\n",
      "test_loss: 0.04148201942443848\n",
      "step 0: loss = 0.0393\n",
      "step 50: loss = 0.0423\n",
      "step 100: loss = 0.0429\n",
      "step 150: loss = 0.0414\n",
      "step 200: loss = 0.0410\n",
      "step 250: loss = 0.0416\n",
      "step 300: loss = 0.0403\n",
      "step 350: loss = 0.0400\n",
      "step 400: loss = 0.0397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 450: loss = 0.0421\n",
      "test_loss: 0.04138040065765381\n",
      "step 0: loss = 0.0401\n",
      "step 50: loss = 0.0405\n",
      "step 100: loss = 0.0385\n",
      "step 150: loss = 0.0420\n",
      "step 200: loss = 0.0416\n",
      "step 250: loss = 0.0427\n",
      "step 300: loss = 0.0400\n",
      "step 350: loss = 0.0425\n",
      "step 400: loss = 0.0440\n",
      "step 450: loss = 0.0418\n",
      "test_loss: 0.041513471603393554\n",
      "step 0: loss = 0.0393\n",
      "step 50: loss = 0.0411\n",
      "step 100: loss = 0.0394\n",
      "step 150: loss = 0.0424\n",
      "step 200: loss = 0.0406\n",
      "step 250: loss = 0.0417\n",
      "step 300: loss = 0.0425\n",
      "step 350: loss = 0.0399\n",
      "step 400: loss = 0.0417\n",
      "step 450: loss = 0.0419\n",
      "test_loss: 0.041382861137390134\n",
      "step 0: loss = 0.0411\n",
      "step 50: loss = 0.0415\n",
      "step 100: loss = 0.0404\n",
      "step 150: loss = 0.0413\n",
      "step 200: loss = 0.0408\n",
      "step 250: loss = 0.0404\n",
      "step 300: loss = 0.0403\n",
      "step 350: loss = 0.0417\n",
      "step 400: loss = 0.0400\n",
      "step 450: loss = 0.0410\n",
      "test_loss: 0.041289634704589843\n",
      "step 0: loss = 0.0402\n",
      "step 50: loss = 0.0402\n",
      "step 100: loss = 0.0412\n",
      "step 150: loss = 0.0407\n",
      "step 200: loss = 0.0431\n",
      "step 250: loss = 0.0410\n",
      "step 300: loss = 0.0416\n",
      "step 350: loss = 0.0430\n",
      "step 400: loss = 0.0411\n",
      "step 450: loss = 0.0418\n",
      "test_loss: 0.04156199932098389\n",
      "step 0: loss = 0.0425\n",
      "step 50: loss = 0.0383\n",
      "step 100: loss = 0.0403\n",
      "step 150: loss = 0.0399\n",
      "step 200: loss = 0.0433\n",
      "step 250: loss = 0.0409\n",
      "step 300: loss = 0.0417\n",
      "step 350: loss = 0.0419\n",
      "step 400: loss = 0.0428\n",
      "step 450: loss = 0.0422\n",
      "test_loss: 0.04171113967895508\n",
      "step 0: loss = 0.0404\n",
      "step 50: loss = 0.0406\n",
      "step 100: loss = 0.0400\n",
      "step 150: loss = 0.0408\n",
      "step 200: loss = 0.0429\n",
      "step 250: loss = 0.0425\n",
      "step 300: loss = 0.0425\n",
      "step 350: loss = 0.0427\n",
      "step 400: loss = 0.0400\n",
      "step 450: loss = 0.0412\n",
      "test_loss: 0.04159034729003906\n",
      "step 0: loss = 0.0394\n",
      "step 50: loss = 0.0415\n",
      "step 100: loss = 0.0409\n",
      "step 150: loss = 0.0413\n",
      "step 200: loss = 0.0418\n",
      "step 250: loss = 0.0417\n",
      "step 300: loss = 0.0399\n",
      "step 350: loss = 0.0412\n",
      "step 400: loss = 0.0418\n",
      "step 450: loss = 0.0440\n",
      "test_loss: 0.041366496086120606\n",
      "step 0: loss = 0.0423\n",
      "step 50: loss = 0.0414\n",
      "step 100: loss = 0.0410\n",
      "step 150: loss = 0.0427\n",
      "step 200: loss = 0.0401\n",
      "step 250: loss = 0.0407\n",
      "step 300: loss = 0.0426\n",
      "step 350: loss = 0.0396\n",
      "step 400: loss = 0.0407\n",
      "step 450: loss = 0.0428\n",
      "test_loss: 0.04162300109863281\n",
      "step 0: loss = 0.0415\n",
      "step 50: loss = 0.0410\n",
      "step 100: loss = 0.0407\n",
      "step 150: loss = 0.0412\n",
      "step 200: loss = 0.0400\n",
      "step 250: loss = 0.0414\n",
      "step 300: loss = 0.0428\n",
      "step 350: loss = 0.0417\n",
      "step 400: loss = 0.0407\n",
      "step 450: loss = 0.0419\n",
      "test_loss: 0.041235651969909665\n",
      "step 0: loss = 0.0405\n",
      "step 50: loss = 0.0432\n",
      "step 100: loss = 0.0418\n",
      "step 150: loss = 0.0421\n",
      "step 200: loss = 0.0400\n",
      "step 250: loss = 0.0442\n",
      "step 300: loss = 0.0405\n",
      "step 350: loss = 0.0419\n",
      "step 400: loss = 0.0417\n",
      "step 450: loss = 0.0413\n",
      "test_loss: 0.04148834228515625\n",
      "step 0: loss = 0.0425\n",
      "step 50: loss = 0.0422\n",
      "step 100: loss = 0.0411\n",
      "step 150: loss = 0.0400\n",
      "step 200: loss = 0.0393\n",
      "step 250: loss = 0.0415\n",
      "step 300: loss = 0.0416\n",
      "step 350: loss = 0.0433\n",
      "step 400: loss = 0.0374\n",
      "step 450: loss = 0.0407\n",
      "test_loss: 0.04131807804107666\n",
      "step 0: loss = 0.0397\n",
      "step 50: loss = 0.0404\n",
      "step 100: loss = 0.0416\n",
      "step 150: loss = 0.0427\n",
      "step 200: loss = 0.0402\n",
      "step 250: loss = 0.0397\n",
      "step 300: loss = 0.0397\n",
      "step 350: loss = 0.0408\n",
      "step 400: loss = 0.0407\n",
      "step 450: loss = 0.0428\n",
      "test_loss: 0.04121365070343017\n",
      "step 0: loss = 0.0425\n",
      "step 50: loss = 0.0427\n",
      "step 100: loss = 0.0407\n",
      "step 150: loss = 0.0401\n",
      "step 200: loss = 0.0438\n",
      "step 250: loss = 0.0423\n",
      "step 300: loss = 0.0389\n",
      "step 350: loss = 0.0409\n",
      "step 400: loss = 0.0429\n",
      "step 450: loss = 0.0424\n",
      "test_loss: 0.04148056030273437\n",
      "step 0: loss = 0.0390\n",
      "step 50: loss = 0.0394\n",
      "step 100: loss = 0.0427\n",
      "step 150: loss = 0.0406\n",
      "step 200: loss = 0.0433\n",
      "step 250: loss = 0.0426\n",
      "step 300: loss = 0.0398\n",
      "step 350: loss = 0.0410\n",
      "step 400: loss = 0.0391\n",
      "step 450: loss = 0.0417\n",
      "test_loss: 0.04154961585998535\n",
      "step 0: loss = 0.0378\n",
      "step 50: loss = 0.0420\n",
      "step 100: loss = 0.0405\n",
      "step 150: loss = 0.0407\n",
      "step 200: loss = 0.0416\n",
      "step 250: loss = 0.0428\n",
      "step 300: loss = 0.0437\n",
      "step 350: loss = 0.0393\n",
      "step 400: loss = 0.0400\n",
      "step 450: loss = 0.0413\n",
      "test_loss: 0.0413740873336792\n",
      "step 0: loss = 0.0407\n",
      "step 50: loss = 0.0420\n",
      "step 100: loss = 0.0400\n",
      "step 150: loss = 0.0410\n",
      "step 200: loss = 0.0661\n",
      "step 250: loss = 0.0421\n",
      "step 300: loss = 0.0399\n",
      "step 350: loss = 0.0393\n",
      "step 400: loss = 0.0411\n",
      "step 450: loss = 0.0407\n",
      "test_loss: 0.04149179935455322\n",
      "step 0: loss = 0.0422\n",
      "step 50: loss = 0.0410\n",
      "step 100: loss = 0.0429\n",
      "step 150: loss = 0.0416\n",
      "step 200: loss = 0.0407\n",
      "step 250: loss = 0.0400\n",
      "step 300: loss = 0.0407\n",
      "step 350: loss = 0.0407\n",
      "step 400: loss = 0.0406\n",
      "step 450: loss = 0.0417\n",
      "test_loss: 0.041270418167114256\n",
      "step 0: loss = 0.0410\n",
      "step 50: loss = 0.0402\n",
      "step 100: loss = 0.0397\n",
      "step 150: loss = 0.0409\n",
      "step 200: loss = 0.0405\n",
      "step 250: loss = 0.0427\n",
      "step 300: loss = 0.0402\n",
      "step 350: loss = 0.0407\n",
      "step 400: loss = 0.0419\n",
      "step 450: loss = 0.0418\n",
      "test_loss: 0.04150479793548584\n",
      "step 0: loss = 0.0388\n",
      "step 50: loss = 0.0423\n",
      "step 100: loss = 0.0413\n",
      "step 150: loss = 0.0415\n",
      "step 200: loss = 0.0415\n",
      "step 250: loss = 0.0420\n",
      "step 300: loss = 0.0410\n",
      "step 350: loss = 0.0394\n",
      "step 400: loss = 0.0401\n",
      "step 450: loss = 0.0398\n",
      "test_loss: 0.04120326519012451\n",
      "step 0: loss = 0.0402\n",
      "step 50: loss = 0.0400\n",
      "step 100: loss = 0.0412\n",
      "step 150: loss = 0.0445\n",
      "step 200: loss = 0.0385\n",
      "step 250: loss = 0.0396\n",
      "step 300: loss = 0.0419\n",
      "step 350: loss = 0.0441\n",
      "step 400: loss = 0.0435\n",
      "step 450: loss = 0.0397\n",
      "test_loss: 0.0413092565536499\n",
      "step 0: loss = 0.0419\n",
      "step 50: loss = 0.0396\n",
      "step 100: loss = 0.0409\n",
      "step 150: loss = 0.0408\n",
      "step 200: loss = 0.0430\n",
      "step 250: loss = 0.0424\n",
      "step 300: loss = 0.0410\n",
      "step 350: loss = 0.0424\n",
      "step 400: loss = 0.0420\n",
      "step 450: loss = 0.0397\n",
      "test_loss: 0.04148177146911621\n",
      "step 0: loss = 0.0413\n",
      "step 50: loss = 0.0414\n",
      "step 100: loss = 0.0420\n",
      "step 150: loss = 0.0430\n",
      "step 200: loss = 0.0446\n",
      "step 250: loss = 0.0413\n",
      "step 300: loss = 0.0394\n",
      "step 350: loss = 0.0403\n",
      "step 400: loss = 0.0430\n",
      "step 450: loss = 0.0427\n",
      "test_loss: 0.041519784927368165\n",
      "step 0: loss = 0.0428\n",
      "step 50: loss = 0.0416\n",
      "step 100: loss = 0.0427\n",
      "step 150: loss = 0.0421\n",
      "step 200: loss = 0.0421\n",
      "step 250: loss = 0.0402\n",
      "step 300: loss = 0.0412\n",
      "step 350: loss = 0.0426\n",
      "step 400: loss = 0.0420\n",
      "step 450: loss = 0.0378\n",
      "test_loss: 0.041066770553588865\n",
      "step 0: loss = 0.0406\n",
      "step 50: loss = 0.0417\n",
      "step 100: loss = 0.0422\n",
      "step 150: loss = 0.0420\n",
      "step 200: loss = 0.0393\n",
      "step 250: loss = 0.0405\n",
      "step 300: loss = 0.0404\n",
      "step 350: loss = 0.0427\n",
      "step 400: loss = 0.0412\n",
      "step 450: loss = 0.0412\n",
      "test_loss: 0.04141542911529541\n",
      "step 0: loss = 0.0407\n",
      "step 50: loss = 0.0417\n",
      "step 100: loss = 0.0421\n",
      "step 150: loss = 0.0419\n",
      "step 200: loss = 0.0403\n",
      "step 250: loss = 0.0396\n",
      "step 300: loss = 0.0436\n",
      "step 350: loss = 0.0391\n",
      "step 400: loss = 0.0401\n",
      "step 450: loss = 0.0425\n",
      "test_loss: 0.04105579853057861\n",
      "step 0: loss = 0.0401\n",
      "step 50: loss = 0.0415\n",
      "step 100: loss = 0.0394\n",
      "step 150: loss = 0.0398\n",
      "step 200: loss = 0.0397\n",
      "step 250: loss = 0.0406\n",
      "step 300: loss = 0.0397\n",
      "step 350: loss = 0.0417\n",
      "step 400: loss = 0.0384\n",
      "step 450: loss = 0.0407\n",
      "test_loss: 0.04130646228790283\n",
      "step 0: loss = 0.0401\n",
      "step 50: loss = 0.0420\n",
      "step 100: loss = 0.0419\n",
      "step 150: loss = 0.0420\n",
      "step 200: loss = 0.0432\n",
      "step 250: loss = 0.0409\n",
      "step 300: loss = 0.0418\n",
      "step 350: loss = 0.0434\n",
      "step 400: loss = 0.0420\n",
      "step 450: loss = 0.0397\n",
      "test_loss: 0.04162075519561768\n",
      "step 0: loss = 0.0420\n",
      "step 50: loss = 0.0396\n",
      "step 100: loss = 0.0406\n",
      "step 150: loss = 0.0425\n",
      "step 200: loss = 0.0412\n",
      "step 250: loss = 0.0393\n",
      "step 300: loss = 0.0404\n",
      "step 350: loss = 0.0383\n",
      "step 400: loss = 0.0400\n",
      "step 450: loss = 0.0417\n",
      "test_loss: 0.04134229183197022\n",
      "step 0: loss = 0.0393\n",
      "step 50: loss = 0.0409\n",
      "step 100: loss = 0.0425\n",
      "step 150: loss = 0.0403\n",
      "step 200: loss = 0.0424\n",
      "step 250: loss = 0.0407\n",
      "step 300: loss = 0.0410\n",
      "step 350: loss = 0.0402\n",
      "step 400: loss = 0.0418\n",
      "step 450: loss = 0.0420\n",
      "test_loss: 0.04120073318481445\n",
      "step 0: loss = 0.0402\n",
      "step 50: loss = 0.0412\n",
      "step 100: loss = 0.0425\n",
      "step 150: loss = 0.0395\n",
      "step 200: loss = 0.0391\n",
      "step 250: loss = 0.0414\n",
      "step 300: loss = 0.0402\n",
      "step 350: loss = 0.0419\n",
      "step 400: loss = 0.0412\n",
      "step 450: loss = 0.0409\n",
      "test_loss: 0.041239476203918456\n",
      "step 0: loss = 0.0432\n",
      "step 50: loss = 0.0384\n",
      "step 100: loss = 0.0396\n",
      "step 150: loss = 0.0397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 200: loss = 0.0405\n",
      "step 250: loss = 0.0409\n",
      "step 300: loss = 0.0386\n",
      "step 350: loss = 0.0405\n",
      "step 400: loss = 0.0403\n",
      "step 450: loss = 0.0418\n",
      "test_loss: 0.04126838684082031\n",
      "step 0: loss = 0.0399\n",
      "step 50: loss = 0.0397\n",
      "step 100: loss = 0.0408\n",
      "step 150: loss = 0.0424\n",
      "step 200: loss = 0.0432\n",
      "step 250: loss = 0.0417\n",
      "step 300: loss = 0.0416\n",
      "step 350: loss = 0.0420\n",
      "step 400: loss = 0.0407\n",
      "step 450: loss = 0.0409\n",
      "test_loss: 0.04109864234924317\n",
      "step 0: loss = 0.0383\n",
      "step 50: loss = 0.0412\n",
      "step 100: loss = 0.0401\n",
      "step 150: loss = 0.0418\n",
      "step 200: loss = 0.0418\n",
      "step 250: loss = 0.0404\n",
      "step 300: loss = 0.0420\n",
      "step 350: loss = 0.0412\n",
      "step 400: loss = 0.0411\n",
      "step 450: loss = 0.0417\n",
      "test_loss: 0.04140695095062256\n",
      "step 0: loss = 0.0391\n",
      "step 50: loss = 0.0420\n",
      "step 100: loss = 0.0399\n",
      "step 150: loss = 0.0384\n",
      "step 200: loss = 0.0438\n",
      "step 250: loss = 0.0416\n",
      "step 300: loss = 0.0428\n",
      "step 350: loss = 0.0435\n",
      "step 400: loss = 0.0386\n",
      "step 450: loss = 0.0396\n",
      "test_loss: 0.041314854621887206\n",
      "step 0: loss = 0.0419\n",
      "step 50: loss = 0.0401\n",
      "step 100: loss = 0.0391\n",
      "step 150: loss = 0.0395\n",
      "step 200: loss = 0.0415\n",
      "step 250: loss = 0.0398\n",
      "step 300: loss = 0.0420\n",
      "step 350: loss = 0.0429\n",
      "step 400: loss = 0.0427\n",
      "step 450: loss = 0.0399\n",
      "test_loss: 0.04152484893798828\n",
      "step 0: loss = 0.0409\n",
      "step 50: loss = 0.0434\n",
      "step 100: loss = 0.0412\n",
      "step 150: loss = 0.0407\n",
      "step 200: loss = 0.0428\n",
      "step 250: loss = 0.0411\n",
      "step 300: loss = 0.0410\n",
      "step 350: loss = 0.0414\n",
      "step 400: loss = 0.0399\n",
      "step 450: loss = 0.0429\n",
      "test_loss: 0.04124998092651367\n",
      "step 0: loss = 0.0424\n",
      "step 50: loss = 0.0432\n",
      "step 100: loss = 0.0407\n",
      "step 150: loss = 0.0400\n",
      "step 200: loss = 0.0421\n",
      "step 250: loss = 0.0393\n",
      "step 300: loss = 0.0413\n",
      "step 350: loss = 0.0416\n",
      "step 400: loss = 0.0408\n",
      "step 450: loss = 0.0415\n",
      "test_loss: 0.04142946243286133\n",
      "step 0: loss = 0.0413\n",
      "step 50: loss = 0.0404\n",
      "step 100: loss = 0.0413\n",
      "step 150: loss = 0.0402\n",
      "step 200: loss = 0.0399\n",
      "step 250: loss = 0.0403\n",
      "step 300: loss = 0.0409\n",
      "step 350: loss = 0.0397\n",
      "step 400: loss = 0.0400\n",
      "step 450: loss = 0.0392\n",
      "test_loss: 0.04122845649719238\n",
      "step 0: loss = 0.0420\n",
      "step 50: loss = 0.0409\n",
      "step 100: loss = 0.0409\n",
      "step 150: loss = 0.0407\n",
      "step 200: loss = 0.0397\n",
      "step 250: loss = 0.0398\n",
      "step 300: loss = 0.0412\n",
      "step 350: loss = 0.0398\n",
      "step 400: loss = 0.0432\n",
      "step 450: loss = 0.0433\n",
      "test_loss: 0.04132997512817383\n",
      "step 0: loss = 0.0414\n",
      "step 50: loss = 0.0403\n",
      "step 100: loss = 0.0423\n",
      "step 150: loss = 0.0409\n",
      "step 200: loss = 0.0398\n",
      "step 250: loss = 0.0412\n",
      "step 300: loss = 0.0386\n",
      "step 350: loss = 0.0394\n",
      "step 400: loss = 0.0407\n",
      "step 450: loss = 0.0395\n",
      "test_loss: 0.04134833335876465\n",
      "step 0: loss = 0.0398\n",
      "step 50: loss = 0.0409\n",
      "step 100: loss = 0.0421\n",
      "step 150: loss = 0.0400\n",
      "step 200: loss = 0.0419\n",
      "step 250: loss = 0.0402\n",
      "step 300: loss = 0.0406\n",
      "step 350: loss = 0.0411\n",
      "step 400: loss = 0.0412\n",
      "step 450: loss = 0.0422\n",
      "test_loss: 0.04120602607727051\n",
      "step 0: loss = 0.0407\n",
      "step 50: loss = 0.0424\n",
      "step 100: loss = 0.0395\n",
      "step 150: loss = 0.0420\n",
      "step 200: loss = 0.0418\n",
      "step 250: loss = 0.0398\n",
      "step 300: loss = 0.0421\n",
      "step 350: loss = 0.0417\n",
      "step 400: loss = 0.0402\n",
      "step 450: loss = 0.0388\n",
      "test_loss: 0.04113576889038086\n",
      "step 0: loss = 0.0441\n",
      "step 50: loss = 0.0399\n",
      "step 100: loss = 0.0404\n",
      "step 150: loss = 0.0407\n",
      "step 200: loss = 0.0416\n",
      "step 250: loss = 0.0403\n",
      "step 300: loss = 0.0422\n",
      "step 350: loss = 0.0430\n",
      "step 400: loss = 0.0391\n",
      "step 450: loss = 0.0410\n",
      "test_loss: 0.04114792823791504\n",
      "step 0: loss = 0.0418\n",
      "step 50: loss = 0.0400\n",
      "step 100: loss = 0.0430\n",
      "step 150: loss = 0.0425\n",
      "step 200: loss = 0.0398\n",
      "step 250: loss = 0.0420\n",
      "step 300: loss = 0.0420\n",
      "step 350: loss = 0.0378\n",
      "step 400: loss = 0.0403\n",
      "step 450: loss = 0.0420\n",
      "test_loss: 0.041386871337890624\n",
      "step 0: loss = 0.0410\n",
      "step 50: loss = 0.0407\n",
      "step 100: loss = 0.0406\n",
      "step 150: loss = 0.0427\n",
      "step 200: loss = 0.0430\n",
      "step 250: loss = 0.0402\n",
      "step 300: loss = 0.0411\n",
      "step 350: loss = 0.0424\n",
      "step 400: loss = 0.0404\n",
      "step 450: loss = 0.0421\n",
      "test_loss: 0.04116220474243164\n",
      "step 0: loss = 0.0392\n",
      "step 50: loss = 0.0436\n",
      "step 100: loss = 0.0398\n",
      "step 150: loss = 0.0404\n",
      "step 200: loss = 0.0431\n",
      "step 250: loss = 0.0417\n",
      "step 300: loss = 0.0409\n",
      "step 350: loss = 0.0395\n",
      "step 400: loss = 0.0412\n",
      "step 450: loss = 0.0399\n",
      "test_loss: 0.04109198570251465\n",
      "step 0: loss = 0.0411\n",
      "step 50: loss = 0.0420\n",
      "step 100: loss = 0.0391\n",
      "step 150: loss = 0.0406\n",
      "step 200: loss = 0.0410\n",
      "step 250: loss = 0.0399\n",
      "step 300: loss = 0.0404\n",
      "step 350: loss = 0.0394\n",
      "step 400: loss = 0.0399\n",
      "step 450: loss = 0.0415\n",
      "test_loss: 0.0412617826461792\n",
      "step 0: loss = 0.0413\n",
      "step 50: loss = 0.0407\n",
      "step 100: loss = 0.0422\n",
      "step 150: loss = 0.0406\n",
      "step 200: loss = 0.0431\n",
      "step 250: loss = 0.0403\n",
      "step 300: loss = 0.0411\n",
      "step 350: loss = 0.0412\n",
      "step 400: loss = 0.0432\n",
      "step 450: loss = 0.0436\n",
      "test_loss: 0.04130520343780517\n",
      "step 0: loss = 0.0433\n",
      "step 50: loss = 0.0405\n",
      "step 100: loss = 0.0415\n",
      "step 150: loss = 0.0433\n",
      "step 200: loss = 0.0408\n",
      "step 250: loss = 0.0414\n",
      "step 300: loss = 0.0404\n",
      "step 350: loss = 0.0424\n",
      "step 400: loss = 0.0396\n",
      "step 450: loss = 0.0396\n",
      "test_loss: 0.04127517700195313\n",
      "step 0: loss = 0.0418\n",
      "step 50: loss = 0.0417\n",
      "step 100: loss = 0.0414\n",
      "step 150: loss = 0.0408\n",
      "step 200: loss = 0.0416\n",
      "step 250: loss = 0.0400\n",
      "step 300: loss = 0.0394\n",
      "step 350: loss = 0.0418\n",
      "step 400: loss = 0.0413\n",
      "step 450: loss = 0.0403\n",
      "test_loss: 0.04126533031463623\n",
      "step 0: loss = 0.0394\n",
      "step 50: loss = 0.0419\n",
      "step 100: loss = 0.0416\n",
      "step 150: loss = 0.0406\n",
      "step 200: loss = 0.0418\n",
      "step 250: loss = 0.0420\n",
      "step 300: loss = 0.0409\n",
      "step 350: loss = 0.0417\n",
      "step 400: loss = 0.0415\n",
      "step 450: loss = 0.0404\n",
      "test_loss: 0.04086868286132812\n",
      "step 0: loss = 0.0404\n",
      "step 50: loss = 0.0395\n",
      "step 100: loss = 0.0428\n",
      "step 150: loss = 0.0385\n",
      "step 200: loss = 0.0427\n",
      "step 250: loss = 0.0411\n",
      "step 300: loss = 0.0442\n",
      "step 350: loss = 0.0398\n",
      "step 400: loss = 0.0410\n",
      "step 450: loss = 0.0418\n",
      "test_loss: 0.04146417617797851\n",
      "step 0: loss = 0.0413\n",
      "step 50: loss = 0.0408\n",
      "step 100: loss = 0.0403\n",
      "step 150: loss = 0.0416\n",
      "step 200: loss = 0.0427\n",
      "step 250: loss = 0.0397\n",
      "step 300: loss = 0.0408\n",
      "step 350: loss = 0.0404\n",
      "step 400: loss = 0.0451\n",
      "step 450: loss = 0.0396\n",
      "test_loss: 0.04110738754272461\n",
      "step 0: loss = 0.0413\n",
      "step 50: loss = 0.0418\n",
      "step 100: loss = 0.0401\n",
      "step 150: loss = 0.0415\n",
      "step 200: loss = 0.0414\n",
      "step 250: loss = 0.0428\n",
      "step 300: loss = 0.0396\n",
      "step 350: loss = 0.0402\n",
      "step 400: loss = 0.0392\n",
      "step 450: loss = 0.0409\n",
      "test_loss: 0.04119259834289551\n",
      "step 0: loss = 0.0394\n",
      "step 50: loss = 0.0420\n",
      "step 100: loss = 0.0415\n",
      "step 150: loss = 0.0415\n",
      "step 200: loss = 0.0406\n",
      "step 250: loss = 0.0411\n",
      "step 300: loss = 0.0421\n",
      "step 350: loss = 0.0396\n",
      "step 400: loss = 0.0413\n",
      "step 450: loss = 0.0409\n",
      "test_loss: 0.04119990825653076\n",
      "step 0: loss = 0.0421\n",
      "step 50: loss = 0.0409\n",
      "step 100: loss = 0.0409\n",
      "step 150: loss = 0.0400\n",
      "step 200: loss = 0.0410\n",
      "step 250: loss = 0.0421\n",
      "step 300: loss = 0.0411\n",
      "step 350: loss = 0.0426\n",
      "step 400: loss = 0.0412\n",
      "step 450: loss = 0.0406\n",
      "test_loss: 0.04139622211456299\n",
      "step 0: loss = 0.0414\n",
      "step 50: loss = 0.0408\n",
      "step 100: loss = 0.0424\n",
      "step 150: loss = 0.0394\n",
      "step 200: loss = 0.0396\n",
      "step 250: loss = 0.0400\n",
      "step 300: loss = 0.0412\n",
      "step 350: loss = 0.0409\n",
      "step 400: loss = 0.0423\n",
      "step 450: loss = 0.0408\n",
      "test_loss: 0.04122157096862793\n",
      "step 0: loss = 0.0416\n",
      "step 50: loss = 0.0393\n",
      "step 100: loss = 0.0425\n",
      "step 150: loss = 0.0436\n",
      "step 200: loss = 0.0426\n",
      "step 250: loss = 0.0407\n",
      "step 300: loss = 0.0399\n",
      "step 350: loss = 0.0403\n",
      "step 400: loss = 0.0431\n",
      "step 450: loss = 0.0407\n",
      "test_loss: 0.04125532627105713\n",
      "step 0: loss = 0.0426\n",
      "step 50: loss = 0.0414\n",
      "step 100: loss = 0.0415\n",
      "step 150: loss = 0.0397\n",
      "step 200: loss = 0.0421\n",
      "step 250: loss = 0.0410\n",
      "step 300: loss = 0.0411\n",
      "step 350: loss = 0.0408\n",
      "step 400: loss = 0.0407\n",
      "step 450: loss = 0.0408\n",
      "test_loss: 0.04126672744750977\n",
      "step 0: loss = 0.0412\n",
      "step 50: loss = 0.0447\n",
      "step 100: loss = 0.0410\n",
      "step 150: loss = 0.0390\n",
      "step 200: loss = 0.0412\n",
      "step 250: loss = 0.0394\n",
      "step 300: loss = 0.0430\n",
      "step 350: loss = 0.0416\n",
      "step 400: loss = 0.0409\n",
      "step 450: loss = 0.0405\n",
      "test_loss: 0.04116371631622315\n",
      "step 0: loss = 0.0415\n",
      "step 50: loss = 0.0398\n",
      "step 100: loss = 0.0405\n",
      "step 150: loss = 0.0403\n",
      "step 200: loss = 0.0430\n",
      "step 250: loss = 0.0408\n",
      "step 300: loss = 0.0438\n",
      "step 350: loss = 0.0402\n",
      "step 400: loss = 0.0412\n",
      "step 450: loss = 0.0422\n",
      "test_loss: 0.04109982013702393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: loss = 0.0399\n",
      "step 50: loss = 0.0409\n",
      "step 100: loss = 0.0415\n",
      "step 150: loss = 0.0413\n",
      "step 200: loss = 0.0406\n",
      "step 250: loss = 0.0426\n",
      "step 300: loss = 0.0419\n",
      "step 350: loss = 0.0407\n",
      "step 400: loss = 0.0419\n",
      "step 450: loss = 0.0407\n",
      "test_loss: 0.04163354396820068\n",
      "step 0: loss = 0.0403\n",
      "step 50: loss = 0.0408\n",
      "step 100: loss = 0.0431\n",
      "step 150: loss = 0.0425\n",
      "step 200: loss = 0.0398\n",
      "step 250: loss = 0.0403\n",
      "step 300: loss = 0.0421\n",
      "step 350: loss = 0.0408\n",
      "step 400: loss = 0.0406\n",
      "step 450: loss = 0.0413\n",
      "test_loss: 0.04138059139251709\n",
      "step 0: loss = 0.0403\n",
      "step 50: loss = 0.0403\n",
      "step 100: loss = 0.0407\n",
      "step 150: loss = 0.0391\n",
      "step 200: loss = 0.0410\n",
      "step 250: loss = 0.0409\n",
      "step 300: loss = 0.0417\n",
      "step 350: loss = 0.0417\n",
      "step 400: loss = 0.0412\n",
      "step 450: loss = 0.0413\n",
      "test_loss: 0.0413386058807373\n",
      "step 0: loss = 0.0391\n",
      "step 50: loss = 0.0407\n",
      "step 100: loss = 0.0406\n",
      "step 150: loss = 0.0405\n",
      "step 200: loss = 0.0415\n",
      "step 250: loss = 0.0411\n",
      "step 300: loss = 0.0425\n",
      "step 350: loss = 0.0421\n",
      "step 400: loss = 0.0409\n",
      "step 450: loss = 0.0416\n",
      "test_loss: 0.041101117134094235\n",
      "step 0: loss = 0.0404\n",
      "step 50: loss = 0.0408\n",
      "step 100: loss = 0.0402\n",
      "step 150: loss = 0.0416\n",
      "step 200: loss = 0.0413\n",
      "step 250: loss = 0.0404\n",
      "step 300: loss = 0.0418\n",
      "step 350: loss = 0.0413\n",
      "step 400: loss = 0.0403\n",
      "step 450: loss = 0.0408\n",
      "test_loss: 0.04100428104400635\n",
      "step 0: loss = 0.0385\n",
      "step 50: loss = 0.0426\n",
      "step 100: loss = 0.0426\n",
      "step 150: loss = 0.0399\n",
      "step 200: loss = 0.0398\n",
      "step 250: loss = 0.0414\n",
      "step 300: loss = 0.0407\n",
      "step 350: loss = 0.0412\n",
      "step 400: loss = 0.0432\n",
      "step 450: loss = 0.0414\n",
      "test_loss: 0.04106898307800293\n",
      "step 0: loss = 0.0418\n",
      "step 50: loss = 0.0430\n",
      "step 100: loss = 0.0397\n",
      "step 150: loss = 0.0396\n",
      "step 200: loss = 0.0407\n",
      "step 250: loss = 0.0404\n",
      "step 300: loss = 0.0405\n",
      "step 350: loss = 0.0425\n",
      "step 400: loss = 0.0426\n",
      "step 450: loss = 0.0377\n",
      "test_loss: 0.04095195293426514\n",
      "step 0: loss = 0.0429\n",
      "step 50: loss = 0.0412\n",
      "step 100: loss = 0.0408\n",
      "step 150: loss = 0.0409\n",
      "step 200: loss = 0.0402\n",
      "step 250: loss = 0.0387\n",
      "step 300: loss = 0.0401\n",
      "step 350: loss = 0.0397\n",
      "step 400: loss = 0.0424\n",
      "step 450: loss = 0.0410\n",
      "test_loss: 0.04116137981414795\n",
      "step 0: loss = 0.0419\n",
      "step 50: loss = 0.0421\n",
      "step 100: loss = 0.0415\n",
      "step 150: loss = 0.0397\n",
      "step 200: loss = 0.0414\n",
      "step 250: loss = 0.0400\n",
      "step 300: loss = 0.0408\n",
      "step 350: loss = 0.0440\n",
      "step 400: loss = 0.0404\n",
      "step 450: loss = 0.0416\n",
      "test_loss: 0.04124344825744629\n",
      "step 0: loss = 0.0413\n",
      "step 50: loss = 0.0642\n",
      "step 100: loss = 0.0397\n",
      "step 150: loss = 0.0443\n",
      "step 200: loss = 0.0415\n",
      "step 250: loss = 0.0434\n",
      "step 300: loss = 0.0440\n",
      "step 350: loss = 0.0426\n",
      "step 400: loss = 0.0427\n",
      "step 450: loss = 0.0392\n",
      "test_loss: 0.04137446403503418\n",
      "step 0: loss = 0.0398\n",
      "step 50: loss = 0.0407\n",
      "step 100: loss = 0.0439\n",
      "step 150: loss = 0.0416\n",
      "step 200: loss = 0.0419\n",
      "step 250: loss = 0.0414\n",
      "step 300: loss = 0.0425\n",
      "step 350: loss = 0.0411\n",
      "step 400: loss = 0.0405\n",
      "step 450: loss = 0.0394\n",
      "test_loss: 0.041111235618591306\n",
      "step 0: loss = 0.0409\n",
      "step 50: loss = 0.0414\n",
      "step 100: loss = 0.0441\n",
      "step 150: loss = 0.0424\n",
      "step 200: loss = 0.0421\n",
      "step 250: loss = 0.0409\n",
      "step 300: loss = 0.0405\n",
      "step 350: loss = 0.0396\n",
      "step 400: loss = 0.0394\n",
      "step 450: loss = 0.0401\n",
      "test_loss: 0.041117582321166996\n",
      "step 0: loss = 0.0378\n",
      "step 50: loss = 0.0415\n",
      "step 100: loss = 0.0391\n",
      "step 150: loss = 0.0418\n",
      "step 200: loss = 0.0387\n",
      "step 250: loss = 0.0398\n",
      "step 300: loss = 0.0400\n",
      "step 350: loss = 0.0412\n",
      "step 400: loss = 0.0402\n",
      "step 450: loss = 0.0413\n",
      "test_loss: 0.04130515098571777\n",
      "step 0: loss = 0.0418\n",
      "step 50: loss = 0.0385\n",
      "step 100: loss = 0.0422\n",
      "step 150: loss = 0.0434\n",
      "step 200: loss = 0.0408\n",
      "step 250: loss = 0.0412\n",
      "step 300: loss = 0.0400\n",
      "step 350: loss = 0.0425\n",
      "step 400: loss = 0.0416\n",
      "step 450: loss = 0.0413\n",
      "test_loss: 0.04195646286010742\n",
      "step 0: loss = 0.0427\n",
      "step 50: loss = 0.0387\n",
      "step 100: loss = 0.0402\n",
      "step 150: loss = 0.0393\n",
      "step 200: loss = 0.0405\n",
      "step 250: loss = 0.0412\n",
      "step 300: loss = 0.0408\n",
      "step 350: loss = 0.0390\n",
      "step 400: loss = 0.0390\n",
      "step 450: loss = 0.0410\n",
      "test_loss: 0.041083450317382815\n",
      "step 0: loss = 0.0412\n",
      "step 50: loss = 0.0397\n",
      "step 100: loss = 0.0394\n",
      "step 150: loss = 0.0428\n",
      "step 200: loss = 0.0393\n",
      "step 250: loss = 0.0424\n",
      "step 300: loss = 0.0409\n",
      "step 350: loss = 0.0415\n",
      "step 400: loss = 0.0435\n",
      "step 450: loss = 0.0399\n",
      "test_loss: 0.041339573860168455\n",
      "step 0: loss = 0.0399\n",
      "step 50: loss = 0.0403\n",
      "step 100: loss = 0.0406\n",
      "step 150: loss = 0.0409\n",
      "step 200: loss = 0.0423\n",
      "step 250: loss = 0.0430\n",
      "step 300: loss = 0.0400\n",
      "step 350: loss = 0.0408\n",
      "step 400: loss = 0.0389\n",
      "step 450: loss = 0.0395\n",
      "test_loss: 0.04164098739624023\n",
      "step 0: loss = 0.0428\n",
      "step 50: loss = 0.0404\n",
      "step 100: loss = 0.0409\n",
      "step 150: loss = 0.0421\n",
      "step 200: loss = 0.0426\n",
      "step 250: loss = 0.0423\n",
      "step 300: loss = 0.0405\n",
      "step 350: loss = 0.0405\n",
      "step 400: loss = 0.0436\n",
      "step 450: loss = 0.0410\n",
      "test_loss: 0.041339178085327145\n",
      "step 0: loss = 0.0394\n",
      "step 50: loss = 0.0389\n",
      "step 100: loss = 0.0407\n",
      "step 150: loss = 0.0410\n",
      "step 200: loss = 0.0426\n",
      "step 250: loss = 0.0383\n",
      "step 300: loss = 0.0414\n",
      "step 350: loss = 0.0414\n",
      "step 400: loss = 0.0402\n",
      "step 450: loss = 0.0412\n",
      "test_loss: 0.04108433246612549\n",
      "step 0: loss = 0.0425\n",
      "step 50: loss = 0.0416\n",
      "step 100: loss = 0.0416\n",
      "step 150: loss = 0.0401\n",
      "step 200: loss = 0.0397\n",
      "step 250: loss = 0.0415\n",
      "step 300: loss = 0.0433\n",
      "step 350: loss = 0.0417\n",
      "step 400: loss = 0.0404\n",
      "step 450: loss = 0.0401\n",
      "test_loss: 0.041452341079711914\n",
      "step 0: loss = 0.0422\n",
      "step 50: loss = 0.0416\n",
      "step 100: loss = 0.0409\n",
      "step 150: loss = 0.0405\n",
      "step 200: loss = 0.0398\n",
      "step 250: loss = 0.0411\n",
      "step 300: loss = 0.0379\n",
      "step 350: loss = 0.0428\n",
      "step 400: loss = 0.0414\n",
      "step 450: loss = 0.0445\n",
      "test_loss: 0.041219539642333984\n",
      "step 0: loss = 0.0388\n",
      "step 50: loss = 0.0409\n",
      "step 100: loss = 0.0426\n",
      "step 150: loss = 0.0422\n",
      "step 200: loss = 0.0414\n",
      "step 250: loss = 0.0409\n",
      "step 300: loss = 0.0416\n",
      "step 350: loss = 0.0402\n",
      "step 400: loss = 0.0421\n",
      "step 450: loss = 0.0406\n",
      "test_loss: 0.0413716459274292\n",
      "step 0: loss = 0.0410\n",
      "step 50: loss = 0.0412\n",
      "step 100: loss = 0.0419\n",
      "step 150: loss = 0.0410\n",
      "step 200: loss = 0.0417\n",
      "step 250: loss = 0.0397\n",
      "step 300: loss = 0.0393\n",
      "step 350: loss = 0.0417\n",
      "step 400: loss = 0.0408\n",
      "step 450: loss = 0.0417\n",
      "test_loss: 0.04117207050323486\n",
      "step 0: loss = 0.0411\n",
      "step 50: loss = 0.0414\n",
      "step 100: loss = 0.0415\n",
      "step 150: loss = 0.0400\n",
      "step 200: loss = 0.0419\n",
      "step 250: loss = 0.0405\n",
      "step 300: loss = 0.0410\n",
      "step 350: loss = 0.0403\n",
      "step 400: loss = 0.0394\n",
      "step 450: loss = 0.0407\n",
      "test_loss: 0.041264338493347166\n",
      "step 0: loss = 0.0411\n",
      "step 50: loss = 0.0387\n",
      "step 100: loss = 0.0422\n",
      "step 150: loss = 0.0404\n",
      "step 200: loss = 0.0398\n",
      "step 250: loss = 0.0405\n",
      "step 300: loss = 0.0413\n",
      "step 350: loss = 0.0409\n",
      "step 400: loss = 0.0402\n",
      "step 450: loss = 0.0384\n",
      "test_loss: 0.0411240816116333\n",
      "step 0: loss = 0.0397\n",
      "step 50: loss = 0.0400\n",
      "step 100: loss = 0.0414\n",
      "step 150: loss = 0.0402\n",
      "step 200: loss = 0.0407\n",
      "step 250: loss = 0.0419\n",
      "step 300: loss = 0.0408\n",
      "step 350: loss = 0.0416\n",
      "step 400: loss = 0.0422\n",
      "step 450: loss = 0.0436\n",
      "test_loss: 0.04118942737579346\n",
      "step 0: loss = 0.0410\n",
      "step 50: loss = 0.0393\n",
      "step 100: loss = 0.0384\n",
      "step 150: loss = 0.0419\n",
      "step 200: loss = 0.0408\n",
      "step 250: loss = 0.0414\n",
      "step 300: loss = 0.0412\n",
      "step 350: loss = 0.0413\n",
      "step 400: loss = 0.0413\n",
      "step 450: loss = 0.0407\n",
      "test_loss: 0.04129666328430176\n",
      "step 0: loss = 0.0431\n",
      "step 50: loss = 0.0400\n",
      "step 100: loss = 0.0408\n",
      "step 150: loss = 0.0411\n",
      "step 200: loss = 0.0388\n",
      "step 250: loss = 0.0409\n",
      "step 300: loss = 0.0414\n",
      "step 350: loss = 0.0425\n",
      "step 400: loss = 0.0431\n",
      "step 450: loss = 0.0406\n",
      "test_loss: 0.04124476432800293\n",
      "step 0: loss = 0.0401\n",
      "step 50: loss = 0.0410\n",
      "step 100: loss = 0.0411\n",
      "step 150: loss = 0.0412\n",
      "step 200: loss = 0.0387\n",
      "step 250: loss = 0.0402\n",
      "step 300: loss = 0.0396\n",
      "step 350: loss = 0.0393\n",
      "step 400: loss = 0.0423\n",
      "step 450: loss = 0.0412\n",
      "test_loss: 0.04101426124572754\n",
      "step 0: loss = 0.0416\n",
      "step 50: loss = 0.0393\n",
      "step 100: loss = 0.0401\n",
      "step 150: loss = 0.0426\n",
      "step 200: loss = 0.0396\n",
      "step 250: loss = 0.0421\n",
      "step 300: loss = 0.0412\n",
      "step 350: loss = 0.0422\n",
      "step 400: loss = 0.0426\n",
      "step 450: loss = 0.0397\n",
      "test_loss: 0.041237349510192874\n",
      "step 0: loss = 0.0399\n",
      "step 50: loss = 0.0415\n",
      "step 100: loss = 0.0417\n",
      "step 150: loss = 0.0408\n",
      "step 200: loss = 0.0414\n",
      "step 250: loss = 0.0395\n",
      "step 300: loss = 0.0412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350: loss = 0.0429\n",
      "step 400: loss = 0.0401\n",
      "step 450: loss = 0.0417\n",
      "test_loss: 0.04145503044128418\n",
      "step 0: loss = 0.0415\n",
      "step 50: loss = 0.0404\n",
      "step 100: loss = 0.0420\n",
      "step 150: loss = 0.0400\n",
      "step 200: loss = 0.0417\n",
      "step 250: loss = 0.0404\n",
      "step 300: loss = 0.0429\n",
      "step 350: loss = 0.0398\n",
      "step 400: loss = 0.0421\n",
      "step 450: loss = 0.0385\n",
      "test_loss: 0.041299266815185545\n",
      "step 0: loss = 0.0413\n",
      "step 50: loss = 0.0417\n",
      "step 100: loss = 0.0407\n",
      "step 150: loss = 0.0401\n",
      "step 200: loss = 0.0413\n",
      "step 250: loss = 0.0397\n",
      "step 300: loss = 0.0393\n",
      "step 350: loss = 0.0410\n",
      "step 400: loss = 0.0419\n",
      "step 450: loss = 0.0417\n",
      "test_loss: 0.041299057006835935\n",
      "step 0: loss = 0.0418\n",
      "step 50: loss = 0.0428\n",
      "step 100: loss = 0.0406\n",
      "step 150: loss = 0.0418\n",
      "step 200: loss = 0.0414\n",
      "step 250: loss = 0.0405\n",
      "step 300: loss = 0.0412\n",
      "step 350: loss = 0.0405\n",
      "step 400: loss = 0.0398\n",
      "step 450: loss = 0.0402\n",
      "test_loss: 0.04134538173675537\n",
      "step 0: loss = 0.0431\n",
      "step 50: loss = 0.0405\n",
      "step 100: loss = 0.0389\n",
      "step 150: loss = 0.0391\n",
      "step 200: loss = 0.0417\n",
      "step 250: loss = 0.0419\n",
      "step 300: loss = 0.0403\n",
      "step 350: loss = 0.0406\n",
      "step 400: loss = 0.0411\n",
      "step 450: loss = 0.0411\n",
      "test_loss: 0.04098228454589844\n",
      "step 0: loss = 0.0434\n",
      "step 50: loss = 0.0404\n",
      "step 100: loss = 0.0415\n",
      "step 150: loss = 0.0411\n",
      "step 200: loss = 0.0405\n",
      "step 250: loss = 0.0387\n",
      "step 300: loss = 0.0414\n",
      "step 350: loss = 0.0420\n",
      "step 400: loss = 0.0396\n",
      "step 450: loss = 0.0396\n",
      "test_loss: 0.041287894248962405\n",
      "step 0: loss = 0.0410\n",
      "step 50: loss = 0.0407\n",
      "step 100: loss = 0.0417\n",
      "step 150: loss = 0.0420\n",
      "step 200: loss = 0.0415\n",
      "step 250: loss = 0.0408\n",
      "step 300: loss = 0.0406\n",
      "step 350: loss = 0.0425\n",
      "step 400: loss = 0.0422\n",
      "step 450: loss = 0.0413\n",
      "test_loss: 0.0414223575592041\n",
      "step 0: loss = 0.0403\n",
      "step 50: loss = 0.0419\n",
      "step 100: loss = 0.0407\n",
      "step 150: loss = 0.0391\n",
      "step 200: loss = 0.0378\n",
      "step 250: loss = 0.0415\n",
      "step 300: loss = 0.0388\n",
      "step 350: loss = 0.0422\n",
      "step 400: loss = 0.0409\n",
      "step 450: loss = 0.0400\n",
      "test_loss: 0.041447196006774906\n",
      "step 0: loss = 0.0411\n",
      "step 50: loss = 0.0402\n",
      "step 100: loss = 0.0438\n",
      "step 150: loss = 0.0420\n",
      "step 200: loss = 0.0400\n",
      "step 250: loss = 0.0395\n",
      "step 300: loss = 0.0421\n",
      "step 350: loss = 0.0417\n",
      "step 400: loss = 0.0418\n",
      "step 450: loss = 0.0390\n",
      "test_loss: 0.04112103462219238\n",
      "step 0: loss = 0.0412\n",
      "step 50: loss = 0.0412\n",
      "step 100: loss = 0.0412\n",
      "step 150: loss = 0.0423\n",
      "step 200: loss = 0.0394\n",
      "step 250: loss = 0.0410\n",
      "step 300: loss = 0.0417\n",
      "step 350: loss = 0.0402\n",
      "step 400: loss = 0.0408\n",
      "step 450: loss = 0.0399\n",
      "test_loss: 0.04109696865081787\n",
      "step 0: loss = 0.0384\n",
      "step 50: loss = 0.0444\n",
      "step 100: loss = 0.0402\n",
      "step 150: loss = 0.0419\n",
      "step 200: loss = 0.0420\n",
      "step 250: loss = 0.0416\n",
      "step 300: loss = 0.0426\n",
      "step 350: loss = 0.0419\n",
      "step 400: loss = 0.0414\n",
      "step 450: loss = 0.0413\n",
      "test_loss: 0.041269960403442385\n",
      "step 0: loss = 0.0396\n",
      "step 50: loss = 0.0408\n",
      "step 100: loss = 0.0419\n",
      "step 150: loss = 0.0393\n",
      "step 200: loss = 0.0411\n",
      "step 250: loss = 0.0397\n",
      "step 300: loss = 0.0423\n",
      "step 350: loss = 0.0415\n",
      "step 400: loss = 0.0415\n",
      "step 450: loss = 0.0421\n",
      "test_loss: 0.04102596759796143\n",
      "step 0: loss = 0.0419\n",
      "step 50: loss = 0.0403\n",
      "step 100: loss = 0.0420\n",
      "step 150: loss = 0.0648\n",
      "step 200: loss = 0.0415\n",
      "step 250: loss = 0.0406\n",
      "step 300: loss = 0.0413\n",
      "step 350: loss = 0.0391\n",
      "step 400: loss = 0.0388\n",
      "step 450: loss = 0.0428\n",
      "test_loss: 0.04133939743041992\n",
      "step 0: loss = 0.0406\n",
      "step 50: loss = 0.0428\n",
      "step 100: loss = 0.0420\n",
      "step 150: loss = 0.0423\n",
      "step 200: loss = 0.0413\n",
      "step 250: loss = 0.0417\n",
      "step 300: loss = 0.0382\n",
      "step 350: loss = 0.0400\n",
      "step 400: loss = 0.0407\n",
      "step 450: loss = 0.0387\n",
      "test_loss: 0.04117321014404297\n",
      "step 0: loss = 0.0396\n",
      "step 50: loss = 0.0390\n",
      "step 100: loss = 0.0410\n",
      "step 150: loss = 0.0420\n",
      "step 200: loss = 0.0402\n",
      "step 250: loss = 0.0408\n",
      "step 300: loss = 0.0411\n",
      "step 350: loss = 0.0395\n",
      "step 400: loss = 0.0408\n",
      "step 450: loss = 0.0416\n",
      "test_loss: 0.041044268608093265\n",
      "step 0: loss = 0.0408\n",
      "step 50: loss = 0.0411\n",
      "step 100: loss = 0.0388\n",
      "step 150: loss = 0.0401\n",
      "step 200: loss = 0.0414\n",
      "step 250: loss = 0.0410\n",
      "step 300: loss = 0.0458\n",
      "step 350: loss = 0.0398\n",
      "step 400: loss = 0.0434\n",
      "step 450: loss = 0.0418\n",
      "test_loss: 0.04113027572631836\n",
      "step 0: loss = 0.0397\n",
      "step 50: loss = 0.0421\n",
      "step 100: loss = 0.0419\n",
      "step 150: loss = 0.0420\n",
      "step 200: loss = 0.0425\n",
      "step 250: loss = 0.0407\n",
      "step 300: loss = 0.0403\n",
      "step 350: loss = 0.0393\n",
      "step 400: loss = 0.0404\n",
      "step 450: loss = 0.0416\n",
      "test_loss: 0.04103744506835937\n",
      "step 0: loss = 0.0386\n",
      "step 50: loss = 0.0440\n",
      "step 100: loss = 0.0426\n",
      "step 150: loss = 0.0431\n",
      "step 200: loss = 0.0404\n",
      "step 250: loss = 0.0403\n",
      "step 300: loss = 0.0404\n",
      "step 350: loss = 0.0426\n",
      "step 400: loss = 0.0401\n",
      "step 450: loss = 0.0414\n",
      "test_loss: 0.04123705387115478\n",
      "step 0: loss = 0.0413\n",
      "step 50: loss = 0.0404\n",
      "step 100: loss = 0.0417\n",
      "step 150: loss = 0.0398\n",
      "step 200: loss = 0.0413\n",
      "step 250: loss = 0.0414\n",
      "step 300: loss = 0.0403\n",
      "step 350: loss = 0.0427\n",
      "step 400: loss = 0.0410\n",
      "step 450: loss = 0.0402\n",
      "test_loss: 0.04117995262145996\n",
      "step 0: loss = 0.0414\n",
      "step 50: loss = 0.0409\n",
      "step 100: loss = 0.0412\n",
      "step 150: loss = 0.0416\n",
      "step 200: loss = 0.0414\n",
      "step 250: loss = 0.0409\n",
      "step 300: loss = 0.0438\n",
      "step 350: loss = 0.0399\n",
      "step 400: loss = 0.0409\n",
      "step 450: loss = 0.0405\n",
      "test_loss: 0.04104075908660889\n",
      "step 0: loss = 0.0397\n",
      "step 50: loss = 0.0425\n",
      "step 100: loss = 0.0398\n",
      "step 150: loss = 0.0406\n",
      "step 200: loss = 0.0420\n",
      "step 250: loss = 0.0415\n",
      "step 300: loss = 0.0400\n",
      "step 350: loss = 0.0419\n",
      "step 400: loss = 0.0414\n",
      "step 450: loss = 0.0406\n",
      "test_loss: 0.04126222610473633\n",
      "step 0: loss = 0.0411\n",
      "step 50: loss = 0.0416\n",
      "step 100: loss = 0.0398\n",
      "step 150: loss = 0.0417\n",
      "step 200: loss = 0.0417\n",
      "step 250: loss = 0.0420\n",
      "step 300: loss = 0.0416\n",
      "step 350: loss = 0.0423\n",
      "step 400: loss = 0.0414\n",
      "step 450: loss = 0.0417\n",
      "test_loss: 0.04128049373626709\n",
      "step 0: loss = 0.0414\n",
      "step 50: loss = 0.0396\n",
      "step 100: loss = 0.0418\n",
      "step 150: loss = 0.0386\n",
      "step 200: loss = 0.0452\n",
      "step 250: loss = 0.0427\n",
      "step 300: loss = 0.0408\n",
      "step 350: loss = 0.0399\n",
      "step 400: loss = 0.0413\n",
      "step 450: loss = 0.0400\n",
      "test_loss: 0.0414362096786499\n",
      "step 0: loss = 0.0432\n",
      "step 50: loss = 0.0395\n",
      "step 100: loss = 0.0426\n",
      "step 150: loss = 0.0439\n",
      "step 200: loss = 0.0401\n",
      "step 250: loss = 0.0402\n",
      "step 300: loss = 0.0408\n",
      "step 350: loss = 0.0409\n",
      "step 400: loss = 0.0415\n",
      "step 450: loss = 0.0398\n",
      "test_loss: 0.04109013557434082\n",
      "step 0: loss = 0.0425\n",
      "step 50: loss = 0.0400\n",
      "step 100: loss = 0.0413\n",
      "step 150: loss = 0.0420\n",
      "step 200: loss = 0.0426\n",
      "step 250: loss = 0.0388\n",
      "step 300: loss = 0.0397\n",
      "step 350: loss = 0.0414\n",
      "step 400: loss = 0.0413\n",
      "step 450: loss = 0.0408\n",
      "test_loss: 0.04120383739471436\n",
      "step 0: loss = 0.0405\n",
      "step 50: loss = 0.0423\n",
      "step 100: loss = 0.0402\n",
      "step 150: loss = 0.0410\n",
      "step 200: loss = 0.0406\n",
      "step 250: loss = 0.0400\n",
      "step 300: loss = 0.0389\n",
      "step 350: loss = 0.0409\n",
      "step 400: loss = 0.0420\n",
      "step 450: loss = 0.0408\n",
      "test_loss: 0.04121279239654541\n",
      "step 0: loss = 0.0402\n",
      "step 50: loss = 0.0409\n",
      "step 100: loss = 0.0401\n",
      "step 150: loss = 0.0396\n",
      "step 200: loss = 0.0398\n",
      "step 250: loss = 0.0417\n",
      "step 300: loss = 0.0388\n",
      "step 350: loss = 0.0397\n",
      "step 400: loss = 0.0417\n",
      "step 450: loss = 0.0391\n",
      "test_loss: 0.04116344928741455\n",
      "step 0: loss = 0.0386\n",
      "step 50: loss = 0.0416\n",
      "step 100: loss = 0.0412\n",
      "step 150: loss = 0.0396\n",
      "step 200: loss = 0.0412\n",
      "step 250: loss = 0.0441\n",
      "step 300: loss = 0.0394\n",
      "step 350: loss = 0.0403\n",
      "step 400: loss = 0.0407\n",
      "step 450: loss = 0.0400\n",
      "test_loss: 0.041073851585388184\n",
      "step 0: loss = 0.0406\n",
      "step 50: loss = 0.0428\n",
      "step 100: loss = 0.0424\n",
      "step 150: loss = 0.0406\n",
      "step 200: loss = 0.0423\n",
      "step 250: loss = 0.0409\n",
      "step 300: loss = 0.0394\n",
      "step 350: loss = 0.0402\n",
      "step 400: loss = 0.0395\n",
      "step 450: loss = 0.0405\n",
      "test_loss: 0.04148199081420898\n",
      "step 0: loss = 0.0407\n",
      "step 50: loss = 0.0404\n",
      "step 100: loss = 0.0409\n",
      "step 150: loss = 0.0419\n",
      "step 200: loss = 0.0428\n",
      "step 250: loss = 0.0411\n",
      "step 300: loss = 0.0399\n",
      "step 350: loss = 0.0394\n",
      "step 400: loss = 0.0428\n",
      "step 450: loss = 0.0414\n",
      "test_loss: 0.041089987754821776\n",
      "step 0: loss = 0.0425\n",
      "step 50: loss = 0.0429\n",
      "step 100: loss = 0.0426\n",
      "step 150: loss = 0.0432\n",
      "step 200: loss = 0.0396\n",
      "step 250: loss = 0.0407\n",
      "step 300: loss = 0.0415\n",
      "step 350: loss = 0.0402\n",
      "step 400: loss = 0.0401\n",
      "step 450: loss = 0.0376\n",
      "test_loss: 0.04090136051177978\n",
      "step 0: loss = 0.0418\n",
      "step 50: loss = 0.0418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 100: loss = 0.0401\n",
      "step 150: loss = 0.0420\n",
      "step 200: loss = 0.0422\n",
      "step 250: loss = 0.0408\n",
      "step 300: loss = 0.0402\n",
      "step 350: loss = 0.0402\n",
      "step 400: loss = 0.0424\n",
      "step 450: loss = 0.0393\n",
      "test_loss: 0.041292128562927244\n",
      "step 0: loss = 0.0406\n",
      "step 50: loss = 0.0427\n",
      "step 100: loss = 0.0400\n",
      "step 150: loss = 0.0392\n",
      "step 200: loss = 0.0401\n",
      "step 250: loss = 0.0408\n",
      "step 300: loss = 0.0397\n",
      "step 350: loss = 0.0408\n",
      "step 400: loss = 0.0399\n",
      "step 450: loss = 0.0416\n",
      "test_loss: 0.04085536956787109\n",
      "step 0: loss = 0.0394\n",
      "step 50: loss = 0.0426\n",
      "step 100: loss = 0.0416\n",
      "step 150: loss = 0.0397\n",
      "step 200: loss = 0.0430\n",
      "step 250: loss = 0.0430\n",
      "step 300: loss = 0.0380\n",
      "step 350: loss = 0.0403\n",
      "step 400: loss = 0.0392\n",
      "step 450: loss = 0.0407\n",
      "test_loss: 0.040982961654663086\n",
      "step 0: loss = 0.0430\n",
      "step 50: loss = 0.0409\n",
      "step 100: loss = 0.0410\n",
      "step 150: loss = 0.0412\n",
      "step 200: loss = 0.0443\n",
      "step 250: loss = 0.0406\n",
      "step 300: loss = 0.0408\n",
      "step 350: loss = 0.0400\n",
      "step 400: loss = 0.0434\n",
      "step 450: loss = 0.0429\n",
      "test_loss: 0.04134223461151123\n",
      "step 0: loss = 0.0395\n",
      "step 50: loss = 0.0401\n",
      "step 100: loss = 0.0418\n",
      "step 150: loss = 0.0411\n",
      "step 200: loss = 0.0421\n",
      "step 250: loss = 0.0424\n",
      "step 300: loss = 0.0385\n",
      "step 350: loss = 0.0399\n",
      "step 400: loss = 0.0403\n",
      "step 450: loss = 0.0416\n",
      "test_loss: 0.04102261543273926\n",
      "step 0: loss = 0.0416\n",
      "step 50: loss = 0.0413\n",
      "step 100: loss = 0.0409\n",
      "step 150: loss = 0.0440\n",
      "step 200: loss = 0.0395\n",
      "step 250: loss = 0.0415\n",
      "step 300: loss = 0.0410\n",
      "step 350: loss = 0.0400\n",
      "step 400: loss = 0.0414\n",
      "step 450: loss = 0.0416\n",
      "test_loss: 0.041487622261047366\n",
      "step 0: loss = 0.0375\n",
      "step 50: loss = 0.0396\n",
      "step 100: loss = 0.0403\n",
      "step 150: loss = 0.0412\n",
      "step 200: loss = 0.0402\n",
      "step 250: loss = 0.0411\n",
      "step 300: loss = 0.0411\n",
      "step 350: loss = 0.0406\n",
      "step 400: loss = 0.0416\n",
      "step 450: loss = 0.0416\n",
      "test_loss: 0.041032958030700686\n",
      "step 0: loss = 0.0409\n",
      "step 50: loss = 0.0411\n",
      "step 100: loss = 0.0422\n",
      "step 150: loss = 0.0391\n",
      "step 200: loss = 0.0406\n",
      "step 250: loss = 0.0406\n",
      "step 300: loss = 0.0407\n",
      "step 350: loss = 0.0409\n",
      "step 400: loss = 0.0426\n",
      "step 450: loss = 0.0405\n",
      "test_loss: 0.04110929489135742\n",
      "step 0: loss = 0.0414\n",
      "step 50: loss = 0.0405\n",
      "step 100: loss = 0.0388\n",
      "step 150: loss = 0.0398\n",
      "step 200: loss = 0.0431\n",
      "step 250: loss = 0.0404\n",
      "step 300: loss = 0.0423\n",
      "step 350: loss = 0.0387\n",
      "step 400: loss = 0.0420\n",
      "step 450: loss = 0.0414\n",
      "test_loss: 0.04127798080444336\n",
      "step 0: loss = 0.0410\n",
      "step 50: loss = 0.0397\n",
      "step 100: loss = 0.0403\n",
      "step 150: loss = 0.0412\n",
      "step 200: loss = 0.0408\n",
      "step 250: loss = 0.0411\n",
      "step 300: loss = 0.0414\n",
      "step 350: loss = 0.0402\n",
      "step 400: loss = 0.0416\n",
      "step 450: loss = 0.0418\n",
      "test_loss: 0.0410678768157959\n",
      "step 0: loss = 0.0405\n",
      "step 50: loss = 0.0392\n",
      "step 100: loss = 0.0405\n",
      "step 150: loss = 0.0419\n",
      "step 200: loss = 0.0421\n",
      "step 250: loss = 0.0432\n",
      "step 300: loss = 0.0412\n",
      "step 350: loss = 0.0409\n",
      "step 400: loss = 0.0411\n",
      "step 450: loss = 0.0424\n",
      "test_loss: 0.041275224685668944\n",
      "step 0: loss = 0.0401\n",
      "step 50: loss = 0.0412\n",
      "step 100: loss = 0.0411\n",
      "step 150: loss = 0.0430\n",
      "step 200: loss = 0.0404\n",
      "step 250: loss = 0.0395\n",
      "step 300: loss = 0.0642\n",
      "step 350: loss = 0.0413\n",
      "step 400: loss = 0.0410\n",
      "step 450: loss = 0.0424\n",
      "test_loss: 0.04115745067596435\n",
      "step 0: loss = 0.0386\n",
      "step 50: loss = 0.0418\n",
      "step 100: loss = 0.0410\n",
      "step 150: loss = 0.0404\n",
      "step 200: loss = 0.0421\n",
      "step 250: loss = 0.0402\n",
      "step 300: loss = 0.0405\n",
      "step 350: loss = 0.0403\n",
      "step 400: loss = 0.0416\n",
      "step 450: loss = 0.0409\n",
      "test_loss: 0.04131765842437744\n",
      "step 0: loss = 0.0406\n",
      "step 50: loss = 0.0406\n",
      "step 100: loss = 0.0413\n",
      "step 150: loss = 0.0415\n",
      "step 200: loss = 0.0396\n",
      "step 250: loss = 0.0391\n",
      "step 300: loss = 0.0421\n",
      "step 350: loss = 0.0401\n",
      "step 400: loss = 0.0403\n",
      "step 450: loss = 0.0422\n",
      "test_loss: 0.04111125469207764\n",
      "step 0: loss = 0.0403\n",
      "step 50: loss = 0.0398\n",
      "step 100: loss = 0.0407\n",
      "step 150: loss = 0.0404\n",
      "step 200: loss = 0.0417\n",
      "step 250: loss = 0.0396\n",
      "step 300: loss = 0.0400\n",
      "step 350: loss = 0.0414\n",
      "step 400: loss = 0.0419\n",
      "step 450: loss = 0.0398\n",
      "test_loss: 0.04108987331390381\n",
      "step 0: loss = 0.0413\n",
      "step 50: loss = 0.0405\n",
      "step 100: loss = 0.0410\n",
      "step 150: loss = 0.0428\n",
      "step 200: loss = 0.0427\n",
      "step 250: loss = 0.0410\n",
      "step 300: loss = 0.0397\n",
      "step 350: loss = 0.0422\n",
      "step 400: loss = 0.0401\n",
      "step 450: loss = 0.0420\n",
      "test_loss: 0.041108322143554685\n",
      "step 0: loss = 0.0398\n",
      "step 50: loss = 0.0396\n",
      "step 100: loss = 0.0408\n",
      "step 150: loss = 0.0419\n",
      "step 200: loss = 0.0408\n",
      "step 250: loss = 0.0410\n",
      "step 300: loss = 0.0386\n",
      "step 350: loss = 0.0387\n",
      "step 400: loss = 0.0400\n",
      "step 450: loss = 0.0410\n",
      "test_loss: 0.04107382297515869\n",
      "step 0: loss = 0.0407\n",
      "step 50: loss = 0.0389\n",
      "step 100: loss = 0.0397\n",
      "step 150: loss = 0.0412\n",
      "step 200: loss = 0.0433\n",
      "step 250: loss = 0.0397\n",
      "step 300: loss = 0.0411\n",
      "step 350: loss = 0.0418\n",
      "step 400: loss = 0.0400\n",
      "step 450: loss = 0.0410\n",
      "test_loss: 0.04117302417755127\n",
      "step 0: loss = 0.0411\n",
      "step 50: loss = 0.0411\n",
      "step 100: loss = 0.0405\n",
      "step 150: loss = 0.0427\n",
      "step 200: loss = 0.0394\n",
      "step 250: loss = 0.0398\n",
      "step 300: loss = 0.0420\n",
      "step 350: loss = 0.0424\n",
      "step 400: loss = 0.0412\n",
      "step 450: loss = 0.0412\n",
      "test_loss: 0.04104718208312988\n",
      "step 0: loss = 0.0403\n",
      "step 50: loss = 0.0403\n",
      "step 100: loss = 0.0398\n",
      "step 150: loss = 0.0421\n",
      "step 200: loss = 0.0422\n",
      "step 250: loss = 0.0402\n",
      "step 300: loss = 0.0407\n",
      "step 350: loss = 0.0391\n",
      "step 400: loss = 0.0413\n",
      "step 450: loss = 0.0399\n",
      "test_loss: 0.04107464790344238\n",
      "step 0: loss = 0.0394\n",
      "step 50: loss = 0.0407\n",
      "step 100: loss = 0.0387\n",
      "step 150: loss = 0.0414\n",
      "step 200: loss = 0.0406\n",
      "step 250: loss = 0.0413\n",
      "step 300: loss = 0.0427\n",
      "step 350: loss = 0.0396\n",
      "step 400: loss = 0.0424\n",
      "step 450: loss = 0.0417\n",
      "test_loss: 0.04111924171447754\n",
      "step 0: loss = 0.0423\n",
      "step 50: loss = 0.0397\n",
      "step 100: loss = 0.0424\n",
      "step 150: loss = 0.0416\n",
      "step 200: loss = 0.0412\n",
      "step 250: loss = 0.0435\n",
      "step 300: loss = 0.0421\n",
      "step 350: loss = 0.0398\n",
      "step 400: loss = 0.0397\n",
      "step 450: loss = 0.0394\n",
      "test_loss: 0.040956306457519534\n",
      "step 0: loss = 0.0413\n",
      "step 50: loss = 0.0395\n",
      "step 100: loss = 0.0417\n",
      "step 150: loss = 0.0424\n",
      "step 200: loss = 0.0429\n",
      "step 250: loss = 0.0391\n",
      "step 300: loss = 0.0398\n",
      "step 350: loss = 0.0390\n",
      "step 400: loss = 0.0417\n",
      "step 450: loss = 0.0410\n",
      "test_loss: 0.041194496154785154\n",
      "step 0: loss = 0.0399\n",
      "step 50: loss = 0.0381\n",
      "step 100: loss = 0.0435\n",
      "step 150: loss = 0.0408\n",
      "step 200: loss = 0.0399\n",
      "step 250: loss = 0.0430\n",
      "step 300: loss = 0.0406\n",
      "step 350: loss = 0.0403\n",
      "step 400: loss = 0.0417\n",
      "step 450: loss = 0.0404\n",
      "test_loss: 0.041113090515136716\n",
      "step 0: loss = 0.0388\n",
      "step 50: loss = 0.0381\n",
      "step 100: loss = 0.0410\n",
      "step 150: loss = 0.0407\n",
      "step 200: loss = 0.0408\n",
      "step 250: loss = 0.0404\n",
      "step 300: loss = 0.0412\n",
      "step 350: loss = 0.0397\n",
      "step 400: loss = 0.0416\n",
      "step 450: loss = 0.0391\n",
      "test_loss: 0.04115729331970215\n",
      "step 0: loss = 0.0405\n",
      "step 50: loss = 0.0419\n",
      "step 100: loss = 0.0408\n",
      "step 150: loss = 0.0414\n",
      "step 200: loss = 0.0406\n",
      "step 250: loss = 0.0432\n",
      "step 300: loss = 0.0419\n",
      "step 350: loss = 0.0406\n",
      "step 400: loss = 0.0390\n",
      "step 450: loss = 0.0398\n",
      "test_loss: 0.0410369348526001\n",
      "step 0: loss = 0.0405\n",
      "step 50: loss = 0.0428\n",
      "step 100: loss = 0.0419\n",
      "step 150: loss = 0.0419\n",
      "step 200: loss = 0.0414\n",
      "step 250: loss = 0.0422\n",
      "step 300: loss = 0.0404\n",
      "step 350: loss = 0.0392\n",
      "step 400: loss = 0.0408\n",
      "step 450: loss = 0.0441\n",
      "test_loss: 0.04104422092437744\n",
      "step 0: loss = 0.0392\n",
      "step 50: loss = 0.0427\n",
      "step 100: loss = 0.0400\n",
      "step 150: loss = 0.0411\n",
      "step 200: loss = 0.0392\n",
      "step 250: loss = 0.0402\n",
      "step 300: loss = 0.0395\n",
      "step 350: loss = 0.0394\n",
      "step 400: loss = 0.0421\n",
      "step 450: loss = 0.0410\n",
      "test_loss: 0.041066651344299314\n",
      "step 0: loss = 0.0375\n",
      "step 50: loss = 0.0418\n",
      "step 100: loss = 0.0414\n",
      "step 150: loss = 0.0409\n",
      "step 200: loss = 0.0409\n",
      "step 250: loss = 0.0424\n",
      "step 300: loss = 0.0400\n",
      "step 350: loss = 0.0416\n",
      "step 400: loss = 0.0396\n",
      "step 450: loss = 0.0405\n",
      "test_loss: 0.041435437202453615\n",
      "step 0: loss = 0.0397\n",
      "step 50: loss = 0.0413\n",
      "step 100: loss = 0.0396\n",
      "step 150: loss = 0.0430\n",
      "step 200: loss = 0.0413\n",
      "step 250: loss = 0.0407\n",
      "step 300: loss = 0.0424\n",
      "step 350: loss = 0.0404\n",
      "step 400: loss = 0.0402\n",
      "step 450: loss = 0.0417\n",
      "test_loss: 0.04129787921905517\n",
      "step 0: loss = 0.0403\n",
      "step 50: loss = 0.0419\n",
      "step 100: loss = 0.0415\n",
      "step 150: loss = 0.0436\n",
      "step 200: loss = 0.0392\n",
      "step 250: loss = 0.0406\n",
      "step 300: loss = 0.0428\n",
      "step 350: loss = 0.0402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 400: loss = 0.0405\n",
      "step 450: loss = 0.0407\n",
      "test_loss: 0.04108291625976562\n",
      "step 0: loss = 0.0400\n",
      "step 50: loss = 0.0414\n",
      "step 100: loss = 0.0404\n",
      "step 150: loss = 0.0387\n",
      "step 200: loss = 0.0413\n",
      "step 250: loss = 0.0405\n",
      "step 300: loss = 0.0398\n",
      "step 350: loss = 0.0414\n",
      "step 400: loss = 0.0415\n",
      "step 450: loss = 0.0393\n",
      "test_loss: 0.04134847164154053\n",
      "step 0: loss = 0.0427\n",
      "step 50: loss = 0.0385\n",
      "step 100: loss = 0.0396\n",
      "step 150: loss = 0.0395\n",
      "step 200: loss = 0.0410\n",
      "step 250: loss = 0.0392\n",
      "step 300: loss = 0.0412\n",
      "step 350: loss = 0.0400\n",
      "step 400: loss = 0.0421\n",
      "step 450: loss = 0.0437\n",
      "test_loss: 0.04109816074371338\n",
      "step 0: loss = 0.0405\n",
      "step 50: loss = 0.0396\n",
      "step 100: loss = 0.0393\n",
      "step 150: loss = 0.0400\n",
      "step 200: loss = 0.0426\n",
      "step 250: loss = 0.0428\n",
      "step 300: loss = 0.0401\n",
      "step 350: loss = 0.0421\n",
      "step 400: loss = 0.0413\n",
      "step 450: loss = 0.0417\n",
      "test_loss: 0.04082503795623779\n",
      "step 0: loss = 0.0415\n",
      "step 50: loss = 0.0402\n",
      "step 100: loss = 0.0418\n",
      "step 150: loss = 0.0430\n",
      "step 200: loss = 0.0404\n",
      "step 250: loss = 0.0424\n",
      "step 300: loss = 0.0416\n",
      "step 350: loss = 0.0408\n",
      "step 400: loss = 0.0417\n",
      "step 450: loss = 0.0409\n",
      "test_loss: 0.04110675811767578\n",
      "step 0: loss = 0.0445\n",
      "step 50: loss = 0.0407\n",
      "step 100: loss = 0.0410\n",
      "step 150: loss = 0.0404\n",
      "step 200: loss = 0.0408\n",
      "step 250: loss = 0.0423\n",
      "step 300: loss = 0.0413\n",
      "step 350: loss = 0.0419\n",
      "step 400: loss = 0.0391\n",
      "step 450: loss = 0.0398\n",
      "test_loss: 0.04098692893981934\n",
      "step 0: loss = 0.0382\n",
      "step 50: loss = 0.0423\n",
      "step 100: loss = 0.0415\n",
      "step 150: loss = 0.0402\n",
      "step 200: loss = 0.0400\n",
      "step 250: loss = 0.0426\n",
      "step 300: loss = 0.0401\n",
      "step 350: loss = 0.0435\n",
      "step 400: loss = 0.0426\n",
      "step 450: loss = 0.0396\n",
      "test_loss: 0.040999803543090824\n",
      "step 0: loss = 0.0425\n",
      "step 50: loss = 0.0429\n",
      "step 100: loss = 0.0419\n",
      "step 150: loss = 0.0413\n",
      "step 200: loss = 0.0393\n",
      "step 250: loss = 0.0440\n",
      "step 300: loss = 0.0421\n",
      "step 350: loss = 0.0416\n",
      "step 400: loss = 0.0383\n",
      "step 450: loss = 0.0425\n",
      "test_loss: 0.04112608432769775\n",
      "step 0: loss = 0.0388\n",
      "step 50: loss = 0.0408\n",
      "step 100: loss = 0.0407\n",
      "step 150: loss = 0.0415\n",
      "step 200: loss = 0.0389\n",
      "step 250: loss = 0.0400\n",
      "step 300: loss = 0.0387\n",
      "step 350: loss = 0.0393\n",
      "step 400: loss = 0.0420\n",
      "step 450: loss = 0.0410\n",
      "test_loss: 0.04115626811981201\n",
      "step 0: loss = 0.0396\n",
      "step 50: loss = 0.0411\n",
      "step 100: loss = 0.0409\n",
      "step 150: loss = 0.0389\n",
      "step 200: loss = 0.0415\n",
      "step 250: loss = 0.0386\n",
      "step 300: loss = 0.0393\n",
      "step 350: loss = 0.0402\n",
      "step 400: loss = 0.0413\n",
      "step 450: loss = 0.0414\n",
      "test_loss: 0.041186351776123044\n",
      "step 0: loss = 0.0422\n",
      "step 50: loss = 0.0434\n",
      "step 100: loss = 0.0396\n",
      "step 150: loss = 0.0409\n",
      "step 200: loss = 0.0403\n",
      "step 250: loss = 0.0409\n",
      "step 300: loss = 0.0405\n",
      "step 350: loss = 0.0397\n",
      "step 400: loss = 0.0400\n",
      "step 450: loss = 0.0410\n",
      "test_loss: 0.041166224479675294\n",
      "step 0: loss = 0.0407\n",
      "step 50: loss = 0.0396\n",
      "step 100: loss = 0.0409\n",
      "step 150: loss = 0.0393\n",
      "step 200: loss = 0.0420\n",
      "step 250: loss = 0.0401\n",
      "step 300: loss = 0.0400\n",
      "step 350: loss = 0.0411\n",
      "step 400: loss = 0.0394\n",
      "step 450: loss = 0.0418\n",
      "test_loss: 0.04110546588897705\n",
      "step 0: loss = 0.0414\n",
      "step 50: loss = 0.0387\n",
      "step 100: loss = 0.0417\n",
      "step 150: loss = 0.0397\n",
      "step 200: loss = 0.0384\n",
      "step 250: loss = 0.0404\n",
      "step 300: loss = 0.0415\n",
      "step 350: loss = 0.0403\n",
      "step 400: loss = 0.0414\n",
      "step 450: loss = 0.0423\n",
      "test_loss: 0.04108651161193848\n",
      "step 0: loss = 0.0386\n",
      "step 50: loss = 0.0403\n",
      "step 100: loss = 0.0421\n",
      "step 150: loss = 0.0397\n",
      "step 200: loss = 0.0389\n",
      "step 250: loss = 0.0402\n",
      "step 300: loss = 0.0420\n",
      "step 350: loss = 0.0428\n",
      "step 400: loss = 0.0407\n",
      "step 450: loss = 0.0410\n",
      "test_loss: 0.040927186012268066\n",
      "step 0: loss = 0.0410\n",
      "step 50: loss = 0.0409\n",
      "step 100: loss = 0.0406\n",
      "step 150: loss = 0.0415\n",
      "step 200: loss = 0.0416\n",
      "step 250: loss = 0.0403\n",
      "step 300: loss = 0.0392\n",
      "step 350: loss = 0.0399\n",
      "step 400: loss = 0.0423\n",
      "step 450: loss = 0.0406\n",
      "test_loss: 0.04109515190124512\n",
      "step 0: loss = 0.0392\n",
      "step 50: loss = 0.0397\n",
      "step 100: loss = 0.0420\n",
      "step 150: loss = 0.0401\n",
      "step 200: loss = 0.0401\n",
      "step 250: loss = 0.0417\n",
      "step 300: loss = 0.0437\n",
      "step 350: loss = 0.0440\n",
      "step 400: loss = 0.0404\n",
      "step 450: loss = 0.0413\n",
      "test_loss: 0.041106958389282224\n",
      "step 0: loss = 0.0408\n",
      "step 50: loss = 0.0437\n",
      "step 100: loss = 0.0420\n",
      "step 150: loss = 0.0418\n",
      "step 200: loss = 0.0393\n",
      "step 250: loss = 0.0426\n",
      "step 300: loss = 0.0389\n",
      "step 350: loss = 0.0399\n",
      "step 400: loss = 0.0395\n",
      "step 450: loss = 0.0398\n",
      "test_loss: 0.04099938869476318\n",
      "step 0: loss = 0.0379\n",
      "step 50: loss = 0.0420\n",
      "step 100: loss = 0.0411\n",
      "step 150: loss = 0.0396\n",
      "step 200: loss = 0.0433\n",
      "step 250: loss = 0.0418\n",
      "step 300: loss = 0.0430\n",
      "step 350: loss = 0.0405\n",
      "step 400: loss = 0.0413\n",
      "step 450: loss = 0.0415\n",
      "test_loss: 0.04103939533233643\n",
      "step 0: loss = 0.0420\n",
      "step 50: loss = 0.0397\n",
      "step 100: loss = 0.0397\n",
      "step 150: loss = 0.0429\n",
      "step 200: loss = 0.0402\n",
      "step 250: loss = 0.0402\n",
      "step 300: loss = 0.0409\n",
      "step 350: loss = 0.0424\n",
      "step 400: loss = 0.0398\n",
      "step 450: loss = 0.0412\n",
      "test_loss: 0.04109328269958496\n",
      "step 0: loss = 0.0396\n",
      "step 50: loss = 0.0409\n",
      "step 100: loss = 0.0397\n",
      "step 150: loss = 0.0425\n",
      "step 200: loss = 0.0424\n",
      "step 250: loss = 0.0422\n",
      "step 300: loss = 0.0434\n",
      "step 350: loss = 0.0386\n",
      "step 400: loss = 0.0421\n",
      "step 450: loss = 0.0415\n",
      "test_loss: 0.040929274559021\n",
      "step 0: loss = 0.0396\n",
      "step 50: loss = 0.0419\n",
      "step 100: loss = 0.0414\n",
      "step 150: loss = 0.0420\n",
      "step 200: loss = 0.0409\n",
      "step 250: loss = 0.0426\n",
      "step 300: loss = 0.0417\n",
      "step 350: loss = 0.0393\n",
      "step 400: loss = 0.0401\n",
      "step 450: loss = 0.0402\n",
      "test_loss: 0.04117982387542725\n",
      "step 0: loss = 0.0406\n",
      "step 50: loss = 0.0403\n",
      "step 100: loss = 0.0413\n",
      "step 150: loss = 0.0433\n",
      "step 200: loss = 0.0412\n",
      "step 250: loss = 0.0386\n",
      "step 300: loss = 0.0399\n",
      "step 350: loss = 0.0415\n",
      "step 400: loss = 0.0422\n",
      "step 450: loss = 0.0429\n",
      "test_loss: 0.04113988399505615\n",
      "step 0: loss = 0.0395\n",
      "step 50: loss = 0.0425\n",
      "step 100: loss = 0.0392\n",
      "step 150: loss = 0.0418\n",
      "step 200: loss = 0.0681\n",
      "step 250: loss = 0.0394\n",
      "step 300: loss = 0.0418\n",
      "step 350: loss = 0.0416\n",
      "step 400: loss = 0.0407\n",
      "step 450: loss = 0.0427\n",
      "test_loss: 0.041141505241394045\n",
      "step 0: loss = 0.0395\n",
      "step 50: loss = 0.0413\n",
      "step 100: loss = 0.0409\n",
      "step 150: loss = 0.0399\n",
      "step 200: loss = 0.0401\n",
      "step 250: loss = 0.0425\n",
      "step 300: loss = 0.0415\n",
      "step 350: loss = 0.0397\n",
      "step 400: loss = 0.0411\n",
      "step 450: loss = 0.0423\n",
      "test_loss: 0.04123024463653564\n",
      "step 0: loss = 0.0421\n",
      "step 50: loss = 0.0417\n",
      "step 100: loss = 0.0434\n",
      "step 150: loss = 0.0422\n",
      "step 200: loss = 0.0408\n",
      "step 250: loss = 0.0434\n",
      "step 300: loss = 0.0417\n",
      "step 350: loss = 0.0390\n",
      "step 400: loss = 0.0402\n",
      "step 450: loss = 0.0413\n",
      "test_loss: 0.04105423927307129\n",
      "step 0: loss = 0.0404\n",
      "step 50: loss = 0.0414\n",
      "step 100: loss = 0.0392\n",
      "step 150: loss = 0.0409\n",
      "step 200: loss = 0.0401\n",
      "step 250: loss = 0.0411\n",
      "step 300: loss = 0.0445\n",
      "step 350: loss = 0.0377\n",
      "step 400: loss = 0.0406\n",
      "step 450: loss = 0.0426\n",
      "test_loss: 0.04106966495513916\n",
      "step 0: loss = 0.0402\n",
      "step 50: loss = 0.0412\n",
      "step 100: loss = 0.0378\n",
      "step 150: loss = 0.0403\n",
      "step 200: loss = 0.0401\n",
      "step 250: loss = 0.0395\n",
      "step 300: loss = 0.0426\n",
      "step 350: loss = 0.0440\n",
      "step 400: loss = 0.0409\n",
      "step 450: loss = 0.0421\n",
      "test_loss: 0.04078588008880615\n",
      "step 0: loss = 0.0413\n",
      "step 50: loss = 0.0403\n",
      "step 100: loss = 0.0393\n",
      "step 150: loss = 0.0426\n",
      "step 200: loss = 0.0409\n",
      "step 250: loss = 0.0417\n",
      "step 300: loss = 0.0417\n",
      "step 350: loss = 0.0408\n",
      "step 400: loss = 0.0406\n",
      "step 450: loss = 0.0388\n",
      "test_loss: 0.04104271411895752\n",
      "step 0: loss = 0.0412\n",
      "step 50: loss = 0.0394\n",
      "step 100: loss = 0.0405\n",
      "step 150: loss = 0.0431\n",
      "step 200: loss = 0.0411\n",
      "step 250: loss = 0.0389\n",
      "step 300: loss = 0.0408\n",
      "step 350: loss = 0.0416\n",
      "step 400: loss = 0.0402\n",
      "step 450: loss = 0.0413\n",
      "test_loss: 0.041117773056030274\n",
      "step 0: loss = 0.0409\n",
      "step 50: loss = 0.0407\n",
      "step 100: loss = 0.0408\n",
      "step 150: loss = 0.0421\n",
      "step 200: loss = 0.0405\n",
      "step 250: loss = 0.0415\n",
      "step 300: loss = 0.0396\n",
      "step 350: loss = 0.0401\n",
      "step 400: loss = 0.0420\n",
      "step 450: loss = 0.0424\n",
      "test_loss: 0.041074695587158205\n",
      "step 0: loss = 0.0396\n",
      "step 50: loss = 0.0428\n",
      "step 100: loss = 0.0419\n",
      "step 150: loss = 0.0396\n",
      "step 200: loss = 0.0399\n",
      "step 250: loss = 0.0424\n",
      "step 300: loss = 0.0413\n",
      "step 350: loss = 0.0412\n",
      "step 400: loss = 0.0409\n",
      "step 450: loss = 0.0410\n",
      "test_loss: 0.04134700298309326\n",
      "step 0: loss = 0.0405\n",
      "step 50: loss = 0.0410\n",
      "step 100: loss = 0.0411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 150: loss = 0.0398\n",
      "step 200: loss = 0.0409\n",
      "step 250: loss = 0.0400\n",
      "step 300: loss = 0.0421\n",
      "step 350: loss = 0.0425\n",
      "step 400: loss = 0.0420\n",
      "step 450: loss = 0.0400\n",
      "test_loss: 0.04142539501190186\n",
      "step 0: loss = 0.0404\n",
      "step 50: loss = 0.0420\n",
      "step 100: loss = 0.0409\n",
      "step 150: loss = 0.0412\n",
      "step 200: loss = 0.0424\n",
      "step 250: loss = 0.0418\n",
      "step 300: loss = 0.0415\n",
      "step 350: loss = 0.0411\n",
      "step 400: loss = 0.0421\n",
      "step 450: loss = 0.0415\n",
      "test_loss: 0.040949230194091794\n",
      "step 0: loss = 0.0399\n",
      "step 50: loss = 0.0381\n",
      "step 100: loss = 0.0430\n",
      "step 150: loss = 0.0423\n",
      "step 200: loss = 0.0421\n",
      "step 250: loss = 0.0405\n",
      "step 300: loss = 0.0423\n",
      "step 350: loss = 0.0437\n",
      "step 400: loss = 0.0380\n",
      "step 450: loss = 0.0396\n",
      "test_loss: 0.04104677200317383\n",
      "step 0: loss = 0.0406\n",
      "step 50: loss = 0.0410\n",
      "step 100: loss = 0.0394\n",
      "step 150: loss = 0.0417\n",
      "step 200: loss = 0.0417\n",
      "step 250: loss = 0.0406\n",
      "step 300: loss = 0.0442\n",
      "step 350: loss = 0.0393\n"
     ]
    }
   ],
   "source": [
    "#optimizer = tf.keras.optimizers.SGD(learning_rate=1e-1)\n",
    "learning_rate = 10\n",
    "epochs = 20000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for step, (points, Omega_Omegabar, mass, restriction) in enumerate(train_set):\n",
    "        with tf.GradientTape() as tape:\n",
    "            \n",
    "            omega = volume_form(points, Omega_Omegabar, mass, restriction)\n",
    "            loss = weighted_MAPE(Omega_Omegabar, omega, mass)  \n",
    "            grads = tape.gradient(loss, model.trainable_weights)\n",
    "            #print(grads)\n",
    "            \n",
    "        with tf.device('/cpu:0'):\n",
    "            for weight, grad in zip(model.trainable_weights, grads):\n",
    "                weight_cpu = tf.Variable(weight)\n",
    "                weight_cpu.assign_sub(learning_rate*grad)\n",
    "                weight.assign(weight_cpu)\n",
    "        \n",
    "        if step % 50 == 0:\n",
    "            print(\"step %d: loss = %.4f\" % (step, loss))\n",
    "    \n",
    "    test_loss = 0\n",
    "    test_loss_old = 100\n",
    "    \n",
    "    for step, (points, Omega_Omegabar, mass, restriction) in enumerate(test_set):\n",
    "        omega = volume_form(points, Omega_Omegabar, mass, restriction)\n",
    "        test_loss += weighted_MAPE(Omega_Omegabar, omega, mass)\n",
    "   \n",
    "    test_loss = tf.math.real(test_loss).numpy()/(step+1)\n",
    "    print(\"test_loss:\", test_loss)\n",
    "    \n",
    "    # This part doesn't work right now\n",
    "    if test_loss > test_loss_old:\n",
    "        break\n",
    "    test_loss_old = test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some debugging tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def volume_form(x, Omega_Omegabar, mass, restriction):\n",
    "\n",
    "    kahler_metric = complex_hessian(tf.math.real(model(x)), x)\n",
    "    volume_form = tf.linalg.det(tf.matmul(restriction, tf.matmul(kahler_metric, restriction, adjoint_b=True)))\n",
    "    weights = mass / tf.reduce_sum(mass)\n",
    "    factor = tf.reduce_sum(weights * volume_form / Omega_Omegabar)\n",
    "    #factor = tf.constant(4.380538, dtype=tf.complex64)\n",
    "    return  volume_form/factor\n",
    "    #return factor\n",
    "for step, (points, Omega_Omegabar, mass, restriction) in enumerate(dataset):\n",
    "    omega = volume_form(points, Omega_Omegabar, mass, restriction)\n",
    "    \n",
    "    weights = mass / tf.reduce_sum(mass)\n",
    "    print('omega', omega)\n",
    "    print('OO',Omega_Omegabar)\n",
    "    print(tf.cast(tf.abs(Omega_Omegabar -  omega), dtype=tf.complex64) / Omega_Omegabar)\n",
    "   # print(mass/tf.reduce_sum(mass))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(5, 15) dtype=complex64, numpy=\n",
      "array([[ 2.2768168e+00+1.6352530e+00j,  5.8658237e+00-4.2718673e-01j,\n",
      "         5.3873414e-01-1.7244281e-01j, -3.9534414e-01+7.2521999e-02j,\n",
      "        -1.8923687e-02+1.1802919e-02j, -3.8347977e-01-2.2627316e-01j,\n",
      "        -7.6662725e-01+8.0790210e-01j,  2.6793966e-01+6.7697294e-02j,\n",
      "        -4.4106621e-01-1.5242220e-01j, -1.6198709e+00-5.7326740e-01j,\n",
      "         2.8095651e+00+8.1460869e-01j, -1.1343197e+00-2.5883400e+00j,\n",
      "        -4.6649609e+00+1.1073344e+00j,  1.7060937e-01-2.2714563e-02j,\n",
      "         8.5827917e-01-7.9530555e-01j],\n",
      "       [ 1.9180773e+00+4.4432992e-01j, -1.7982172e+00+1.0105304e+00j,\n",
      "        -6.0589733e+00-8.5629034e-01j,  2.9838247e+00-3.7910095e-01j,\n",
      "         2.8432586e+00+9.5829725e-01j, -4.8352519e-01-4.6606249e-01j,\n",
      "        -1.7036586e+00-9.6777737e-01j, -3.2158126e-03+1.6477600e-03j,\n",
      "         1.7850419e+00+6.1775154e-01j,  4.3784446e-01-8.8729739e-02j,\n",
      "         2.8238079e-01+1.9228293e-01j, -2.9208440e-01+1.6115788e-01j,\n",
      "        -2.7304552e+00+1.0982864e+00j, -1.3929390e+00+3.4327587e-01j,\n",
      "        -3.5853200e+00-1.3666987e-01j],\n",
      "       [-1.4581397e+00+1.9110115e+00j, -1.7909920e-01+3.6252013e-01j,\n",
      "        -1.1931094e+00+3.0919951e-01j,  7.2101420e-01-1.2520562e+00j,\n",
      "        -5.5755311e-01-2.0128660e-02j,  3.1300583e+00-2.7365923e-01j,\n",
      "         4.9949604e-01-5.2422490e+00j, -1.7338538e-01+2.4592508e-02j,\n",
      "         1.9228923e+00-6.3471007e-01j,  3.0070311e-01+5.4850805e-01j,\n",
      "         1.8207195e+00-1.2229290e+00j, -4.6268797e+00-1.4987248e+00j,\n",
      "         1.1876729e+00+3.0295810e-01j, -7.5662631e-01+1.2398404e-01j,\n",
      "         3.7306695e+00+1.6974617e+00j],\n",
      "       [ 1.5903249e-01-2.4123290e-01j,  4.2902447e-02+8.1711315e-02j,\n",
      "        -3.3539832e-01+3.9116329e-01j, -2.4176884e+00+1.5029418e-02j,\n",
      "        -4.5817409e+00+8.9052248e-01j, -4.5735011e+00+3.7343332e-01j,\n",
      "         2.3762546e-02-4.5621830e-01j, -1.9385403e-01+8.9566067e-02j,\n",
      "         4.0899768e+00-1.5305756e-02j, -2.5236410e-01+9.8926350e-02j,\n",
      "        -2.1313027e-01-2.0266116e-02j,  2.7595261e-02+4.9135927e-03j,\n",
      "         4.4409629e-02+1.6594049e-01j, -5.0119634e+00+7.1290761e-02j,\n",
      "        -3.5859435e-03+2.7132857e-01j],\n",
      "       [ 2.8225083e+00-1.9472898e+00j, -1.9300312e-01+1.2453795e-01j,\n",
      "        -7.0934814e-01-1.3123615e-01j, -3.2763820e+00+1.2321837e+00j,\n",
      "         1.0924923e+00-7.1551239e-01j,  6.0893261e-01+3.7527627e-01j,\n",
      "         1.0850860e+00-8.6598074e-01j,  4.2221852e-02+8.6574353e-02j,\n",
      "        -2.4428689e+00-5.7877851e-01j,  5.8101020e+00-3.2419884e-01j,\n",
      "         4.1471138e+00+9.2794955e-02j,  1.2292573e+00+7.8488910e-01j,\n",
      "        -1.2955481e-01-9.0879291e-01j, -2.4329317e+00-7.5963151e-01j,\n",
      "         8.8978034e-01+5.8874261e-01j]], dtype=complex64)>\n",
      "<tf.Variable 'Variable:0' shape=(15, 15) dtype=complex64, numpy=\n",
      "array([[ 4.48537874e+00-5.30717336e-02j,  3.97880413e-02-8.86093974e-02j,\n",
      "        -5.31146526e-02+2.45492142e-02j,  8.67075175e-02-1.01624019e-01j,\n",
      "        -1.85670733e-01-6.91963136e-02j, -1.48487300e-01-6.05651662e-02j,\n",
      "         1.20190650e-01-1.69870649e-02j, -5.21362424e-02+1.12177646e-02j,\n",
      "        -9.27942619e-03-8.68658349e-02j,  1.54285967e-01-1.91998314e-02j,\n",
      "         5.47923846e-03+3.80867757e-02j,  3.08177918e-01+2.94221759e-01j,\n",
      "        -1.75426140e-01+1.19707145e-01j, -1.02277972e-01-1.52171299e-01j,\n",
      "        -1.69872731e-01-1.57393202e-01j],\n",
      "       [-5.20558469e-02+1.35510653e-01j,  4.76845026e+00-1.03557847e-01j,\n",
      "        -3.10402393e-01-2.63872325e-01j, -3.83945145e-02-8.85578468e-02j,\n",
      "        -6.10975474e-02-5.13151139e-02j,  3.30757163e-02-6.16653822e-02j,\n",
      "        -6.88735908e-03+3.06749791e-01j,  5.53110726e-02+3.24442722e-02j,\n",
      "         5.61661236e-02-9.50987190e-02j,  3.09128780e-02-1.52821139e-01j,\n",
      "        -5.20106591e-02-6.28411546e-02j,  7.88327605e-02+3.26288608e-03j,\n",
      "         2.27459460e-01+5.63018508e-02j,  8.88381824e-02-2.21358798e-02j,\n",
      "        -2.27249622e-01-5.98294660e-02j],\n",
      "       [-8.53944421e-02-1.59421653e-01j, -4.13616151e-01+3.89929742e-01j,\n",
      "         4.94551754e+00+4.43636924e-02j,  1.50215358e-01+4.44892310e-02j,\n",
      "        -1.41069680e-01+1.37919381e-01j,  4.76978607e-02+3.27756219e-02j,\n",
      "         3.87099892e-01-3.87359917e-01j,  2.56092846e-03+2.12033968e-02j,\n",
      "        -3.92926149e-02-7.29754344e-02j, -1.58888400e-01+7.86960423e-02j,\n",
      "        -1.02544896e-01+3.43110152e-02j, -1.23513997e-01+1.75399467e-01j,\n",
      "         8.80063027e-02+7.74727017e-02j, -4.35849488e-01-7.28005618e-02j,\n",
      "         2.04764962e-01+3.01671684e-01j],\n",
      "       [ 1.15745574e-01+1.57252774e-01j, -1.34361401e-01+5.97085431e-03j,\n",
      "        -1.57307282e-01-2.38582008e-02j,  4.51907349e+00+9.55896154e-02j,\n",
      "        -2.30961278e-01-1.77684844e-01j, -4.40963022e-02+1.08772136e-01j,\n",
      "         1.72351792e-01-3.85349840e-01j,  1.27901202e-02-2.35390174e-03j,\n",
      "         4.82990772e-01-4.60580401e-02j, -7.62113333e-02+4.24391218e-02j,\n",
      "         4.44995686e-02+1.28467217e-01j,  4.94761541e-02-1.91863418e-01j,\n",
      "        -6.11173883e-02+1.82351992e-01j, -1.56536192e-01-3.34206402e-01j,\n",
      "        -2.64782637e-01+9.08263773e-02j],\n",
      "       [-3.49517226e-01+4.60335091e-02j,  1.31304011e-01+1.01816170e-02j,\n",
      "        -3.40697199e-01-1.68635607e-01j, -5.71821369e-02+1.61503091e-01j,\n",
      "         4.63839626e+00+1.15938382e-02j,  6.46486878e-02-9.14678499e-02j,\n",
      "        -3.66927534e-02+1.30854309e-01j,  3.99125144e-02-9.48923826e-03j,\n",
      "         1.12672292e-01-1.14765264e-01j, -1.40039295e-01-4.60513979e-02j,\n",
      "         1.43637145e-02-1.35932982e-01j, -1.13129027e-01-5.67088183e-03j,\n",
      "        -5.44985309e-02-1.37464151e-01j,  2.69388884e-01-2.30145697e-02j,\n",
      "        -5.95672801e-02+1.25840053e-01j],\n",
      "       [-4.16479334e-02+4.21791933e-02j,  2.05890760e-02+2.97590904e-02j,\n",
      "         1.83426924e-02+1.32056922e-01j, -1.38517901e-01+1.10309437e-01j,\n",
      "         6.92991614e-02+5.72190918e-02j,  4.63341808e+00-9.73349288e-02j,\n",
      "         2.43234590e-01+1.41646847e-01j,  7.56485062e-03-4.06649448e-02j,\n",
      "         2.42830552e-02-5.73686250e-02j, -8.83598477e-02-5.31893317e-03j,\n",
      "        -1.07734889e-01+3.14638652e-02j, -1.21181324e-01-1.96566097e-02j,\n",
      "        -4.81016561e-02+2.78094318e-04j,  1.52766019e-01+1.74600426e-02j,\n",
      "        -3.01439971e-01-3.85408252e-02j],\n",
      "       [ 3.00037742e-01-6.98629916e-02j, -4.49373275e-02-1.61733106e-01j,\n",
      "         2.56243914e-01+6.09023511e-01j,  1.36562020e-01+2.60854691e-01j,\n",
      "        -2.74651754e-03-1.08257674e-01j,  1.03100389e-01-2.59782612e-01j,\n",
      "         4.71440935e+00+3.64487455e-03j,  3.80301401e-02-3.25144790e-02j,\n",
      "         9.35852975e-02+5.02375886e-02j,  2.00922623e-01-1.79778919e-01j,\n",
      "        -1.75713956e-01-1.36633992e-01j, -2.38222659e-01-1.92321986e-01j,\n",
      "        -1.23847418e-01-2.11267114e-01j,  3.34210247e-01-1.15863480e-01j,\n",
      "         2.37649888e-01-4.49349672e-01j],\n",
      "       [-1.66156478e-02-7.40435254e-03j,  4.84454557e-02-4.18997854e-02j,\n",
      "        -2.20417902e-02-1.80865303e-02j,  2.84999199e-02+1.54592039e-03j,\n",
      "         1.95790012e-03+9.60797537e-03j, -1.78892480e-03+2.34980136e-02j,\n",
      "         5.60312420e-02+6.79078884e-03j,  1.02035797e+00-1.41587004e-03j,\n",
      "         5.71653470e-02+1.35919440e-03j, -4.11586696e-03+1.70466807e-02j,\n",
      "        -1.22735696e-02+5.34778112e-04j,  2.58589257e-03-8.94873589e-03j,\n",
      "         3.30339633e-02-1.97864827e-02j, -9.78538394e-03+3.22149433e-02j,\n",
      "        -3.68325524e-02+5.54051772e-02j],\n",
      "       [-2.33105617e-04+1.45302713e-01j, -1.40367433e-01-7.84626901e-02j,\n",
      "        -2.98394740e-01+2.03740776e-01j,  4.79328364e-01+1.82237089e-01j,\n",
      "         8.17002635e-03+2.04623297e-01j,  3.38333040e-01+8.51498097e-02j,\n",
      "         1.94009811e-01-3.49350661e-01j,  1.14320517e-01-2.17770189e-02j,\n",
      "         4.51932383e+00+1.78256646e-01j,  1.14981748e-01+1.48429707e-01j,\n",
      "         5.64091764e-02-1.75696075e-01j, -3.98885965e-01-1.18323140e-01j,\n",
      "         3.33802626e-02-4.43213582e-02j,  6.17789589e-02+1.38245270e-01j,\n",
      "         2.13064000e-01+1.79522529e-01j],\n",
      "       [-7.09886604e-04-1.26861371e-02j, -2.62292355e-01+2.72542089e-01j,\n",
      "        -5.57135046e-02+1.39568806e-01j, -4.03439905e-03+4.15739752e-02j,\n",
      "        -2.80338436e-01+3.21313441e-02j, -1.63199157e-01-8.84823427e-02j,\n",
      "         2.22131297e-01+8.47790092e-02j, -4.71316651e-02-2.76802052e-02j,\n",
      "        -2.04155609e-01-9.45463106e-02j,  4.82785606e+00-9.14236009e-02j,\n",
      "        -6.13764375e-02-3.94683145e-02j,  3.66575152e-01-3.63307834e-01j,\n",
      "         1.35576446e-02+3.57938886e-01j, -2.09227741e-01-5.84533885e-02j,\n",
      "         9.33421105e-02-2.38120303e-01j],\n",
      "       [-4.62606281e-01-2.92114038e-02j, -1.05735824e-01-3.64217255e-03j,\n",
      "        -2.66819566e-01-5.29242568e-02j,  8.31092298e-02+5.24944253e-03j,\n",
      "        -1.63037237e-02+1.26137689e-01j,  5.59939817e-03-1.25258237e-01j,\n",
      "         1.06825724e-01+2.39404127e-01j, -1.50322216e-02-3.76014039e-03j,\n",
      "         7.54148513e-02+2.03093141e-02j,  2.86383659e-01-2.77430087e-01j,\n",
      "         4.48912239e+00-4.42797877e-03j,  4.68228877e-01+1.18662231e-02j,\n",
      "        -1.73990220e-01+1.60518482e-01j, -2.17481717e-01-1.80866912e-01j,\n",
      "        -9.43631157e-02-3.28368872e-01j],\n",
      "       [ 1.39473125e-01-3.72130185e-01j,  5.04988022e-02-1.66483656e-01j,\n",
      "        -4.57128733e-02-1.31300867e-01j,  2.80909566e-03+1.68979898e-01j,\n",
      "         8.12469050e-02+2.91961357e-02j, -4.57474701e-02-3.06981169e-02j,\n",
      "         3.91981453e-02-1.05741225e-01j,  2.10939124e-02-2.54678586e-03j,\n",
      "         6.12901598e-02+2.84539282e-01j,  1.91698879e-01+2.75423944e-01j,\n",
      "        -5.37234508e-02+4.94065508e-02j,  4.68861437e+00-6.97277561e-02j,\n",
      "         1.88950926e-01-3.14086676e-01j, -2.56929696e-02-9.12510138e-03j,\n",
      "        -7.24915490e-02+2.37303257e-01j],\n",
      "       [-5.19249380e-01-8.25218931e-02j,  4.53692585e-01-6.55159131e-02j,\n",
      "         1.70892984e-01-2.90004879e-01j,  1.45027399e-01-1.58730075e-01j,\n",
      "        -1.79294348e-01+1.14760011e-01j, -3.70380431e-02-7.66561106e-02j,\n",
      "        -1.95335552e-01+1.75296322e-01j,  9.01950300e-02+2.10906975e-02j,\n",
      "         9.28890929e-02+1.45167699e-02j,  3.77069503e-01-2.98564043e-02j,\n",
      "         7.31683522e-02+8.59315693e-02j,  1.53700531e-01+3.36694151e-01j,\n",
      "         4.57327032e+00+1.19299643e-01j, -1.22024298e-01+1.21267781e-01j,\n",
      "         8.43100473e-02-1.91762120e-01j],\n",
      "       [-5.13286963e-02+1.19752854e-01j, -4.29707654e-02-1.45511866e-01j,\n",
      "        -4.25873190e-01+3.33982483e-02j, -5.83635420e-02+2.12984264e-01j,\n",
      "         3.29427958e-01-1.09983608e-01j,  7.29476437e-02-1.16220005e-02j,\n",
      "         3.54535788e-01+2.12048709e-01j, -2.34260764e-02-6.92817569e-02j,\n",
      "         1.44652680e-01-1.01296343e-01j, -1.73711032e-01+5.02539426e-02j,\n",
      "        -2.53031403e-02-4.64386865e-02j,  6.10113889e-02+8.44140574e-02j,\n",
      "        -7.20824208e-03-5.15585840e-02j,  4.63824749e+00-2.41747703e-02j,\n",
      "         1.78209066e-01-2.16658525e-02j],\n",
      "       [-9.55702737e-02+2.83877194e-01j, -2.95165628e-01-2.67776310e-01j,\n",
      "         3.08804125e-01-5.71958303e-01j, -2.65512139e-01-2.44255111e-01j,\n",
      "        -2.29474574e-01+4.49274331e-02j, -2.87705183e-01-1.03206253e-02j,\n",
      "         1.73533857e-01+5.55275679e-01j, -7.68620819e-02-1.07333794e-01j,\n",
      "         2.17798963e-01-2.64715731e-01j,  1.73822373e-01+1.34617284e-01j,\n",
      "        -1.17425717e-01+1.60592675e-01j,  1.03107719e-02-3.18308085e-01j,\n",
      "         2.97945172e-01+3.53375167e-01j,  6.63591251e-02+9.30496529e-02j,\n",
      "         4.46437311e+00+1.52958790e-02j]], dtype=complex64)>\n"
     ]
    }
   ],
   "source": [
    "for weight in model.trainable_weights:\n",
    "    print(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
